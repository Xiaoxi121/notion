# MySQL（关系型数据库）

## 基础篇

- SQL 是一种结构化查询语言(Structured Query Language)，专门用来与数据库打交道，目的是提供一种从数据库中读写数据的简单有效的方法。

- 关系型数据库（RDB，Relational Database）就是一种建立在关系模型的基础上的数据库。关系模型表明了数据库中所存储的数据之间的联系（一对一、一对多、多对多）。

- 常见的关系型数据库：
  - MySQL
  - PostgreSQL
  - Oracle
  - SQL Server
  - SQLite（微信本地的聊天记录的存储就是）
  
- MySQL 是一种关系型数据库，主要用于持久化存储我们的系统中的一些数据比如用户信息。默认端口是3306

- MySQL**字段类型**
  - 数值类型：
    - 整型（TINYINT、SMALLINT、MEDIUMINT、INT 和 BIGINT）、
    - 浮点型（FLOAT 和 DOUBLE）
    - 定点型（DECIMAL）
      - DECIMAL 是定点数，FLOAT/DOUBLE 是浮点数。DECIMAL 可以存储精确的小数值，FLOAT/DOUBLE 只能存储近似的小数值。
  - 字符串类型**：CHAR、VARCHAR**、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT、TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB 等，最常用的是 CHAR 和 VARCHAR。
  - 日期时间类型：**YEAR、TIME、DATE**、DATETIME 和 TIMESTAMP 等。
    - 不要用字符串储存日期
  
- MySQL 中的整数类型可以使用可选的 UNSIGNED 属性来表示不允许负值的无符号整数。使用 UNSIGNED 属性可以将正整数的上限提高一倍，因为它不需要存储负数值。

- CHAR 和 VARCHAR 是最常用到的字符串类型，两者的主要区别在于：CHAR 是定长字符串，VARCHAR 是变长字符串。

- NULL 跟 ''(空字符串)是两个完全不一样的值
  - ,就算是两个 NULL,它俩也不一定相等
  - ''的长度是 0，是不占用空间的，而NULL 是需要占用空间的。
  - NULL 会影响聚合函数的结果。而且如果参数是 \*(COUNT(*))，则会统计所有的记录数，包括 NULL 值；如果参数是某个字段名(COUNT(列名))，则会忽略 NULL 值，只统计非空值的个数
  - 必须使用 IS NULL 或 IS NOT NULLl来判断
  
- 除非有特别的原因使用 NULL 值，应该总是让字段保持 NOT NULL。
  - 索引 NULL 列需要额外的空间来保存，所以要占用更多的空间；
  - 进行比较和计算时要对 NULL 值做特别的处理
  
- MySQL 中没有专门的布尔类型，而是用 TINYINT(1) 类型来表示布尔值。TINYINT(1) 类型可以存储 0 或 1，分别对应 false 或 true。

- 如何分析SQL的性能
  
  我们可以使用 EXPLAIN 命令来分析 SQL 的 执行计划 。执行计划是指一条 SQL 语句在经过 MySQL 查询优化器的优化会后，具体的执行方式。
  
  - 对于执行计划，参数有：
    - **possible_keys** 字段表示可能用到的索引；
    - **key** 字段表示实际用的索引，如果这一项为 NULL，说明没有使用索引；
    - **key_len** 表示索引的长度；
    - **rows** 表示扫描的数据行数。
    - **type** 表示数据扫描类型，我们需要重点看这个。
      type 字段就是描述了找到所需数据时使用的扫描方式是什么，常见扫描类型的执行效率从低到高的顺序为：
      - **All（全表扫描）：**在这些情况里，all 是最坏的情况，因为采用了全表扫描的方式。
      - **index（全索引扫描）：**index 和 all 差不多，只不过 index 对索引表进行全扫描，这样做的好处是不再需要对数据进行排序，但是开销依然很大。所以，要尽量避免全表扫描和全索引扫描。
      - **range（索引范围扫描）**：range 表示采用了索引范围扫描，一般在 where 子句中使用 < 、>、in、between 等关键词，只检索给定范围的行，属于范围查找。从这一级别开始，索引的作用会越来越明显，因此我们需要尽量让 SQL 查询可以使用到 range 这一级别及以上的 type 访问方式。
      - **ref（非唯一索引扫描）：**ref 类型表示采用了非唯一索引，或者是唯一索引的非唯一性前缀，返回数据返回可能是多条。因为虽然使用了索引，但该索引列的值并不唯一，有重复。这样即使使用索引快速查找到了第一条数据，仍然不能停止，要进行目标值附近的小范围扫描。但它的好处是它并不需要扫全表，因为索引是有序的，即便有重复值，也是在一个非常小的范围内扫描。
      - **eq_ref（唯一索引扫描）**：eq_ref 类型是使用主键或唯一索引时产生的访问方式，通常使用在多表联查中。比如，对两张表进行联查，关联条件是两张表的 user_id 相等，且 user_id 是唯一索引，那么使用 EXPLAIN 进行执行计划查看的时候，type 就会显示 eq_ref。
      - **const（结果只有一条的主键或唯一索引扫描）：**const 类型表示使用了主键或者唯一索引与常量值进行比较，比如 select name from product where id=1。需要说明的是 const 类型和 eq_ref 都使用了主键或唯一索引，**不过这两个类型有所区别，const 是与常量进行比较，查询效率会更快，而 eq_ref 通常用于多表联查中。**
  
    - **extra 显示的结果，**这里说几个重要的参考指标：
      - **Using filesort ：**当查询语句中包含 group by 操作，而且无法利用索引完成排序操作的时候， 这时不得不选择相应的**排序算法**进行，甚至可能会通过文件排序，效率是很低的，所以要避免这种问题的出现。
      - **Using temporary：**使了**用临时表保存中间结果**，MySQL 在对查询结果排序时使用临时表，常见于排序 order by 和分组查询 group by。效率低，要避免这种问题的出现。
      - **Using index：**所需数据**只需在索引即可全部获得，不须要再到表中取数据，**也就是使**用了覆盖索引，避免了回表操作**，效率不错。
  

### 2.1 执行一条select语句期间发生了什么

- Server 层负责**建立连接、分析和执行 SQL**。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。

- 存储引擎层负责数据的存储和提取。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。

  - MySQL 存储引擎采用的是 **插件式架构** ，支持多种存储引擎，我们甚至可以**为不同的数据库表设置不同的存储引擎以适应不同场景的需要。****存储引擎是基于表的，而不是数据库。**

  - InnoDB和MyISAM区别

    | 区别                       | InnoDB                                                       |                            Memory                            | MyISAM                                                       |
    | -------------------------- | ------------------------------------------------------------ | :----------------------------------------------------------: | ------------------------------------------------------------ |
    | **行级锁**                 | **支持**                                                     |                          **不支持**                          | **不支持，只支持表级别**                                     |
    | **事务**                   | **支持，提供四个隔离级别**                                   |                          **不支持**                          | **不支持**                                                   |
    | 外键                       | 支持                                                         |                            不支持                            | 不支持                                                       |
    | 数据库异常崩溃后的安全恢复 | 支持                                                         | 将数据存储在内存中，适用于对性能要求较高的读操作，但是在服务器重启或崩溃时数据会丢失 | 不支持                                                       |
    | MVCC                       | 支持                                                         |                                                              | 不支持                                                       |
    | 索**引实现**               | **InnoDB 引擎中，其数据文件本身就是索引文件。相比 MyISAM，索引文件和数据文件是分离的，其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。**<br /><br /><br />Innodb是聚簇索引，聚簇索引的文件存放在主键索引的叶子节点上，因此 InnoDB 必须要有主键，通过主键索引效率很高。但是辅助索引需要两次查询，先查询到主键，然后再通过主键查询到数据。因此，主键不应该过大，因为主键太大，其他索引也都会很大 |                                                              | **MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是两者的实现方式不太一样。**<br /><br /><br />myisam是非聚簇为索引，数据文件是分离的，**索引保存的是数据文件的指针。主键索引和辅助索引是独立的。** |
    | count的效率****            | **InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描** |                                                              | **MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快** |
    | 性能差别                   | 更强大                                                       |                                                              |                                                              |
    | 数据缓存策略和机制         | 使用缓冲池（Buffer Pool）缓存数据页和索引页                  |                                                              | 只缓存索引页不缓存数据页                                     |
    


#### 第一步：连接器

1. 连接Mysql
2. 三次握手
3. 验证用户和密码
4. 连接器获得权限

如果一个用户已经建立了连接，即使管理员中途修改了该用户的权限，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设。

- 通过`show processlist`查看mysql被多少个服务端连接
- 空闲连接超过`wait_timeout`参数就会自动断开
  - 可以手动关闭
- MySQL 服务支持的最大连接数由 `max_connections` 参数控制
- 长连接（推荐长连接）、短链接
  - 长连接：内存占用过多
    - 定期断开长连接
    - 客户端主动重置链接

#### 第二步：查询缓存

客户端发送sql语句，如果是查询语句，查询缓存，未命中继续向下。Mysql8.0之后删除server层中的查询缓存，**因为一更新查询缓存就删除了，很容易命中失败。**

#### 第三步：解析SQL

查询sql之前，由解析器对sql语句进行解析

- 词法分析：识别关键字
- 语法分析：构建sql语法树

#### 第四步：执行SQL

sql查询语句流程：

- prepare 预处理阶段 **预处理器**
  - 检查 SQL 查询语句中的**表或者字段**是否存在；(MySql 8.0之后)
  - 将 select * 中的 *** 符号，扩展为表上的所有列**
- optimize：优化阶段 **优化器**
  - 优化器：确定SQL语句的执行方案
  - 通过explain看选择的哪个执行方案
- execute：执行阶段 **执行器**
  - 开始执行语句，执行过程中，和存储引擎发生交互
  - 三种执行过程
    - 主键索引查询
      - 执行器调用read_first_record函数指针指向的函数，让存储引擎定位第一条符合条件的记录，返回给执行器，发给客户端，while循环再查一次，直到找不到返回-1结束循环
    - 全表扫描
      - 让存储引擎读取表中的第一条记录；**Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录，直到存储引擎把表中的所有记录读完，然后向执行器（Server层） 返回了读取完毕的信息；**
    - 索引下推
      - 索引下推能够**减少二级索引**在**查询时**的**回表**操作，提高查询的效率，因为它**将 Server 层部分负责的事情，交给存储引擎层去处理了。**
      - 联合索引当遇到范围查询 (>、<) 就会停止匹配，也就是 age 字段能用到联合索引，但是 reward 字段则无法利用到索引。
      - 不用索引下推：先返回age满足的给server，server对reward进行处理
      - 用:返回age和reward都满足的给server

### update语句的具体执行过程是怎样的？

具体更新一条记录 UPDATE t_user SET name = 'xiaolin' WHERE id = 1; 的流程如下:

1. 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：
   1. 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；
   2. 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。
2. 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：
   1. 如果一样的话就不进行后续更新流程；
   2. 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；
3. 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。
4. InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 WAL 技术，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。
5. 至此，一条记录更新完了。
6. 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。
7. 事务提交（为了方便说明，这里不说组提交的过程，只说两阶段提交）：
   1. prepare 阶段：将 redo log 对应的事务状态设置为 prepare，然后将 redo log 刷新到硬盘；
   2. commit 阶段：将 binlog 刷新到磁盘，接着调用引擎的提交事务接口，将 redo log 状态设置为 commit（将事务设置为 commit 状态后，刷入到磁盘 redo log 文件）；

至此，一条更新语句执行完成

### 2.2 Mysql 一行记录是怎么存储的

#### MySQL的数据存放在哪个文件？

- MySQl的数据保存在磁盘

- MySQL存储的行为是由存储引擎实现的，不同的存储引擎保存的文件不同

- InnoDB是MySQL默认的存储引擎，以此为例

  - /var/lib/mysql/目录里面有一个以database为名的目录，保存表结构和表数据的文件存放在这里

    - **db.opt存放当前数据库默认字符集和字符校验规则**

    - **数据库表.frm  保存表结构：表的元数据，主要包括表结构定义**

    - **数据库表.ibd（这个文件也被称为独占表空间文件） 保存表数据**

      表空间文件结构是什么样的？

      - 段
        - 数据段
          - 存放b+树非叶子节点区集合
        - 索引段
          - 存放b+树叶子节点区集合
        - 回滚段
          - 存放的是回滚数据的区的集合
      - 区
        - InnoDB存储引擎以b+树来组织数据，每一次通过链表连接，如果连接页，因为页之间物理位置不连续，会造成大量随机io
        - 所以要让链表中相邻的页物理位置相邻
        - 所以表中数据量大的时候，为索引分配空间时，按照区来分配，每个区1m，可以分配64个页，使得链表中相邻的页物理位置相邻
      - 页
        - InnoDB是以页为单位来读写的，默认每个页的大小为 16KB，也就是最多能保证 16KB 的连续存储空间。
        - 数据表中的行以**数据页**来管理
      - 行

#### InnoDB行格式有哪些？

- Redundant
- Compact
- Dynamic
- Compressed

#### COMPACT行格式长什么样

#### 记录的额外信息

##### 1. 变长字段长度列表

没有变长字段的时候，没有这一个列表

- 数据占用的大小，按照列的顺序逆序存放
  - 记录头信息指向下一个记录记录头信息和真实数据之间的位置
  - 是因为这样可以使得位置靠前的记录的真实数据和数据对应的字段长度信息可以同时在一个 CPU Cache Line 中，这样就可以提高 CPU Cache 的命中率。
  - NULL 值列表的信息也需要逆序存放。
- NULL 是不会存放在行格式中记录的真实数据部分里的，所以「变长字段长度列表」里不需要保存值为 NULL 的变长字段的长度。

##### 2. NULL值列表

当数据表的字段都定义成 NOT NULL 的时候，这时候表里的行格式就不会有 NULL 值列表了。

Compact 行格式把这些值为 NULL 的列存储到 NULL值列表中。

1. 如果存在允许 NULL 值的列，则每个列对应一个二进制位（bit），二进制位按照列的顺序逆序排列。
2. NULL 值列表必须用整数个字节的位表示（1字节8位），如果使用的二进制位个数不足整数个字节，则在字节的高位补 0

##### 3. 记录头信息

1.  delete_mask 不会真正删除记录，逻辑删除
2.  next_record 记录之间通过链表组织
3.  record_type 普通记录、非叶子节点记录、最小记录、最大记录

#### 记录的真实数据

记录真实数据部分除了我们定义的字段，**还有三个隐藏字段，分别为：row_id、trx_id、roll_pointer**

- row_id
  - 如果既没有指定主键，又没有唯一约束，那么 InnoDB 就会为记录添加 row_id 隐藏字段。row_id不是必需的，占用 6 个字节。
- trx_id
  - 事务id，表示这个数据是由哪个事务生成的。 trx_id是必需的，占用 6 个字节。
- roll_pointer
  - 记录上一个版本的指针。roll_pointer 是必需的，占用 7 个字

#### varchar(n)中n最大取值为多少

- MySQL 规定除了 TEXT、BLOBs 这种大对象类型之外，其他所有的列（不包括隐藏列和记录头信息）占用的字节长度加起来不能超过 65535 个字节。（**一行总长度不能超过**）
  - 一行数据的最大字节数 65535，其实是包含「变长字段长度列表」和 「NULL 值列表」所占用字节数的
- n是指最多存储的字符数量
  - 所以要看数据库表的字符集，因为字符集代表着，1个字符要占用多少字节

##### 单字段情况

- 在数据库表只有一个 varchar(n) 字段且字符集是 ascii 的情况下，varchar(n) 中 n 最大值 = 65535 - 2 - 1 = 65532
- 在 UTF-8 字符集下，一个字符最多需要三个字节，varchar(n) 的 n 最大取值就是 65532/3 = 21844。

##### 多字段情况

如果有多个字段的话，要保证所有字段的长度 + 变长字段字节数列表所占用的字节数 + NULL值列表所占用的字节数 <= 65535。

#### 行溢出后，MySQl怎么处理的

- 如果一个数据页存不了一条记录，**InnoDB 存储引擎会自动将溢出的数据存放到「溢出页」中。**
- Compact 行格式针对行溢出的处理是这样的：当发生行溢出时，在记录的真实数据处只会保存该列的一部分数据，而把剩余的数据放在「溢出页」中，**然后真实数据处用 20 字节存储指向溢出页的地址，从而可以找到剩余数据所在的页。**
- Compressed 和 Dynamic 这两种格式采用完全的行溢出方式，记录的真实数据处不会存储该列的一部分数据，只存储 20 个字节的指针来指向溢出页。而实际的数据都存储在溢出页中。

### 2.3 NOSQL和SQL区别

- SQL数据库，指关系型数据库 - 主要代表：SQL Server，Oracle，MySQL(开源)，PostgreSQL(开源)。
  关系型数据库存储结构化数据。**这些数据逻辑上以行列二维表的形式存在，每一列代表数据的一种属性，每一行代表一个数据实体**
- NoSQL指非关系型数据库 ，主要代表：MongoDB，Redis。NoSQL 数据库逻辑上提供了不同于二维表的存储方式，存储方式可以是JSON文档、哈希表或者其他方式。
- sql or nosql
  - 关系型数据库支持 ACID 即原子性，一致性，隔离性和持续性。相对而言，NoSQL 采用更宽松的模型 BASE ， 即基本可用，软状态和最终一致性。
  - NoSQL数据之间无关系，这样就非常容易扩展，也无形之间，在架构的层面上带来了可扩展的能力。比如 redis 自带主从复制模式、哨兵模式、切片集群模式。

### 2.4 数据库三大范式

1. 第一范式（1NF）：要求数据库表的**每一列都是不可分割的原子数据项**

2. 第二范式（2NF）：在1NF的基础上，非码属性必须完全依赖于候选码（在1NF基础上消除非主属性对主码的部分函数依赖）

   第二范式需要确保数据库表中的**每一列都和主键相关，而不能只与主键的某一部分相关（**主要针对联合主键而言）。

3. 第三范式（3NF）：在2NF基础上，任何非主属性 (opens new window)不依赖于其它非主属性（在2NF基础上消除传递依赖）
   第三范式需要确保数据表中**的每一列数据都和主键直接相关，而不能间接相关。**

### 2.5 MySQL怎么连表查询

1. **内连接 (INNER JOIN)**

   内连接**返回两个表中有匹配关系的行。**

2. **左外连接 (LEFT JOIN)**

   左外连接**返回左表中的所有行，即使在右表中没有匹配的行。未匹配的右表列会包含NULL**

3. **右外连接 (RIGHT JOIN)**

   右外连接返回右表中的所有行，**即使左表中没有匹配的行。未匹配的左表列会包含NULL**

4. **全外连接 (FULL JOIN**)

   全外连接返回两个表中所有行，包括非匹配行，**在MySQL中，FULL JOIN 需要使用 UNION 来实现，因为 MySQL 不直接支持 FULL JOIN**

### 2.6 MySQL如何避免重复插入数据

1. **方式一：使用UNIQUE约束**

   在表的相关列上添加UNIQUE约束，确保每个值在该列中唯一

2. **方式二：使用INSERT ... ON DUPLICATE KEY UPDATE**
   这种语句允许在插入记录时处理重复键的情况。如果插入的记录与现有记录冲突，可以选择更新现有记录：

3. **方式三：使用INSERT IGNORE**： 

   该语句会在插入记录时忽略那些因重复键而导致的插入错误。

- 如果需要保证全局唯一性，使用UNIQUE约束是最佳做法。
- 如果需要插入和更新结合可以使用ON DUPLICATE KEY UPDATE。
- 对于快速忽略重复插入，INSERT IGNORE是合适的选择

### 2.7 CHAR和VARCHAR有什么区别

1. **CHAR是固定长度的字符串类型**，定义时需要**指定固定长度**，存储时会在末尾补足空格。CHAR适合存储长度固定的数据，如固定长度的代码、状态等，存储空间固定，对于短字符串效率较高。
2. **VARCHAR是可变长度的字符串类型，**定义时需要**指定最大长度**，实际存储时根据实际长度占用存储空间。VARCHAR适合存储长度可变的数据，如用户输入的文本、备注等，节约存储空间

### 2.8 Text数据类型可以无限大吗

MySQL 3 种text类型的最大长度如下：

1. TEXT：65,535 bytes ~64kb
2. MEDIUMTEXT：16,777,215 bytes ~16Mb
3. LONGTEXT：4,294,967,295 bytes ~4Gb

### 2.9 外键约束

外键约束的作用是维护表与表之间的关系，确保数据的完整性和一致性

### 2.10 MySQL的关键字in和exist

在MySQL中，IN 和 EXISTS 都是用来处理子查询的关键词，

1. IN 用于**检查左边的表达式是否存在于右边的列表或子查询的结果集中**。如果存在，则IN 返回TRUE，否则返回FALSE。
2. EXISTS **用于判断子查询是否至少能返回一行数据**。它不关心子查询返回什么数据，只关心是否有结果。如果子查询有结果，则EXISTS 返回TRUE，否则返回FALSE。

1. 性能差异：在很多情况下，**EXISTS 的性能优于 IN**，特别是当子查询的表很大时。这是因为EXISTS 一旦找到匹配项就会立即停止查询，而IN可能会扫描整个子查询结果集。
2. 使用场景：**如果子查询结果集较小且不频繁变动，IN 可能更直观易懂**。而**当子查询涉及外部查询的每一行判断，并且子查询的效率较高时，EXISTS 更为合适。**
3. NULL值处理**：IN 能够正确处理子查询中包含NULL值的情况，而EXISTS 不受子查询结果中NULL值的影响，**因为它关注的是行的存在性，而不是具体值

### 2.11 mysql中的一些基本函数

一、字符串函数

1. CONCAT(str1, str2, ...)：连接多个字符串，返回一个合并后的字符串。
2. SELECT CONCAT('Hello', ' ', 'World') AS Greeting;
3. LENGTH(str)：返回字符串的长度（字符数）。
4. SELECT LENGTH('Hello') AS StringLength;
5. SUBSTRING(str, pos, len)：从指定位置开始，截取指定长度的子字符串。
6. SELECT SUBSTRING('Hello World', 1, 5) AS SubStr;
7. REPLACE(str, from_str, to_str)：将字符串中的某部分替换为另一个字符串。
8. SELECT REPLACE('Hello World', 'World', 'MySQL') AS ReplacedStr;

二、数值函数

1. ABS(num)：返回数字的绝对值。
2. SELECT ABS(-10) AS AbsoluteValue;
3. POWER(num, exponent)：返回指定数字的指定幂次方。
4. SELECT POWER(2, 3) AS PowerValue;

三、日期和时间函数

1. NOW()：返回当前日期和时间。
2. SELECT NOW() AS CurrentDateTime;
3. CURDATE()：返回当前日期。
4. SELECT CURDATE() AS CurrentDate;

四、聚合函数

1. COUNT(column)：计算指定列中的非NULL值的个数。
2. SELECT COUNT(*) AS RowCount FROM my_table;
3. SUM(column)：计算指定列的总和。
4. SELECT SUM(price) AS TotalPrice FROM orders;
5. AVG(column)：计算指定列的平均值。
6. SELECT AVG(price) AS AveragePrice FROM orders;
7. MAX(column)：返回指定列的最大值。
8. SELECT MAX(price) AS MaxPrice FROM orders;
9. MIN(column)：返回指定列的最小值。
10. SELECT MIN(price) AS MinPrice FROM orders

### 2.12 SQL的查询语句的执行顺序是怎么样的

![image-20241007214806656](D:\2024\Notes\Typora\八股\MySQL.assets\image-20241007214806656.png)

## 索引篇

### 3.1 索引面试题

#### 什么是索引？

- 索引是数据的目录
- 所谓的存储引擎，就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。
- MySQL 存储引擎有 MyISAM 、InnoDB、Memory，其中 InnoDB 是在 MySQL 5.5 之后成为默认的存储引擎。

#### 索引的分类？

- **按「数据结构」分类：B+tree索引、Hash索引、Full-text索引。**
- **按「物理存储」分类：聚簇索引（主键索引）、二级索引（辅助索引）（唯一索引、普通索引、前缀索引都属于二级索引）。**
- **按「字段特性」分类：主键索引、唯一索引、普通索引、前缀索引。**
- **按「字段个数」分类：单列索引、联合索引。**

##### 按数据结构分类

![image-20241007220425387](D:\2024\Notes\Typora\八股\MySQL.assets\image-20241007220425387.png)

- MySQL 常见索引有 B+Tree 索引、HASH 索引、Full-Text 索引。
- InnoDB 是在 MySQL 5.5 之后成为默认的 MySQL 存储引擎，B+Tree 索引类型也是 MySQL 存储引擎采用最多的索引类型。
  - **如果有主键，默认会使用主键作为聚簇索引的索引键（key）；**
    如果没有主键，就选择**第一个不包含 NULL 值的唯一列**作为聚簇索引的索引键（key）；
    在上面两个都没有的情况下，InnoDB **将自动生成一个隐式自增 id 列作**为聚簇索引的索引键（key）；
    - 总结下自增值不连续的 4 个场景：
      **自增初始值和自增步长设置不为 1**
      **唯一键冲突**
      **事务回滚**
      **批量插入（如 insert...select 语句）**
  - 其它索引都属于辅助索引（Secondary Index），也被称为二级索引或非聚簇索引。
  - 创建的主键索引和二级索引默认使用的是 B+Tree 索引。
  - **B+Tree 是一种多叉树，叶子节点才存放数据，非叶子节点只存放索引，而且每个节点里的数据是按主键顺序存放的。每一层父节点的索引值都会出现在下层子节点的索引值中，因此在叶子节点中，包括了所有的索引值信息，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表。**
- 为什么InnoDB选择B+tree作为索引的数据结构？
  - B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。
  - 对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。**数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操。**而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。
  - **Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。但是 Hash 表不适合做范围查询，它更适合做等值的查**

##### 按物理存储分类

- 聚簇索引

  主键索引的 B+Tree  **的叶子节点存放的是实际数据，所有完整的用户记录都存放在主键索引的 B+Tree 的叶子节点里；**

- 二级索引

  二级索引的 B+Tree  的**叶子节点存放的是主键值，而不是实际数据。**

- 聚簇索引和非聚簇索引的区别
  - 数据存储
    - 聚簇索引：索引的叶子节点包含了实际的数据行
    - 非聚簇索引：不包含完整的数据行，而是包含指向数据行的指针或者主键值
  - 索引与数据的关系
    - 通过聚簇索引查找数据时，可以直接从索引中获得数据行，而不需要额外的步骤去查找数据所在的位置。
    - **当通过非聚簇索引查找数据时，首先在非聚簇索引中找到对应的主键值，然后通过这个主键值回溯到聚簇索引中查找实际的数据行，这个过程称为“回表”**
  - 唯一性
    - 此每个表只能有一个聚簇索引，因为数据只能有一种物理排序方式。
    - 一个表可以有多个非聚簇索引，因为它们不直接影响数据的物理存储位置。
  - 效率
    - **对于范围查询和排序查询**，聚簇索引通常更有效率，因为它避免了额外的寻址开销
    - 非聚簇索引**在使用覆盖索引进行查询时效率更高**，因为它不需要读取完整的数据行。但是需要进行回表的操作，使用非聚簇索引效率比较低，因为需要进行额外的回表操作

##### 按字段特性分类

从字段特性的角度来看，索引分为主键索引、唯一索引、普通索引、前缀索引。

1. 主键索引

   一张表最多只有一个主键索引，索引列的值不允许有空值。

2. 唯一索引

   唯一索引建立在 UNIQUE 字段上的索引，一张表可以有多个唯一索引，索引列的值必须唯一，但是允许有空值。

3. 普通索引

   普通索引就是建立在普通字段上的索引，既不要求字段为主键，也不要求字段为 UNIQUE。

4. 前缀索引

   前缀索引是指对字符类型字段的前几个字符建立的索引，而不是在整个字段上建立的索引，前缀索引可以建立在字段类型为 char、 varchar、binary、varbinary 的列上。

   **使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小**
   
   使用前缀索引的目的是为了减少索引占用的存储空间，提升查询效率。

##### 按字段个数分类

从字段个数的角度来看，索引分为单列索引、联合索引（复合索引）。

- 建立在单列上的索引称为单列索引，比如主键索引；

- 建立在多列上的索引称为联合索引；

  - 可以看到，联合索引的非叶子节点用两个字段的值作为  B+Tree 的 key 值。当在联合索引查询数据时，先按 product_no 字段比较，在 product_no 相同的情况下再按 name 字段比较。
  - 需要注意的是，因为有查询优化器，所以 a 字段在 where 子句的顺序并不重要。
  - 联合索引的最左匹配原则，在遇到范围查询（如 >、<）的时候，就会停止匹配，也就是范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引。注意，对于 >=、<=、BETWEEN、like 前缀匹配的范围查询，并不会停止匹配，前面我也用了四个例子说明了。
  - **如果一个列即使单列索引，又是联合索引，单独查它的话先走哪个？**
    **mysql 优化器会分析每个索引的查询成本，然后选择成本最低的方案来执行 sql。**

- 索引下推

  而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。

- 索引区分度

  - 另外，建立联合索引时的字段顺序，对索引效率也有很大影响。越靠前的字段被用于索引过滤的概率越高，实际开发工作中建立联合索引时，**要把区分度大的字段排在前面**，这样区分度大的字段越有可能被更多的 SQL 使用到。
  - 因为 MySQL 还有一个查询优化器，查询优化器发现某个值出现在表的数据行中的百分比（惯用的百分比界线是"30%"）很高的时候，它一般会忽略索引，进行全表扫描

- 联合索引进行排序

  要利用索引的有序性，**在 status 和 create_time 列建立联合索引，这样根据 status 筛选后的数据就是按照 create_time 排好序的，避免在文件排序，提高了查询效**

#### 如果聚簇索引的数据更新，它的存储要不要变化？

- 如果更新的数据是非索引数据，也就是普通的用户记录，那么存储结构是不会发生变化
- 如果更新的数据是索引数据，那么存储结构是有变化的，因为要维护 b+树的有序性

#### 索引已经建好了，那我再插入一条数据，索引会有哪些变化？

插入新数据可能**导致B+树结构的调整和索引信息的更新，**以保持B+树的平衡性和正确性，这些变化通常由数据库系统自动处理，确保数据的一致性和索引的有效性。
**如果插入的数据导致叶子节点已满，可能会触发叶子节点的分裂操作，以保持B+树的平衡性。**

#### 什么时候需要/不需要创建索引？

索引优点：

**提高查询速度**

索引缺点：

1. **占用物理空间**
2. **创建索引和维护索引需要时间**
3. 需要**进行动态维护，降低增删改查表的效率**

索引适用：

1. 字段有**唯一性限制**的，比如商品编码；
2. 经常**用于 WHERE 查询条件的字段**，这样能够提高整个表的查询速度，如果查询条件不是一个字段，可以建立联合索引。
3. **经常用于 GROUP BY 和 ORDER BY 的字段，这**样在查询的时候就不需要再去做一次排序了，因为我们都已经知道了建立索引之后在 B+Tree 中的记录都是排序好的。

不适用：

1. **WHERE 条件，GROUP BY，ORDER BY 里用不到的字段**，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的，因为索引是会占用物理空间的。
2. **字段中存在大量重复数据**，不需要创建索引
3. **表数据太少**
4. **经常更新的**字段不用创建索引

#### 索引字段是不是建的越多越好？

不是，**建的的越多会占用越多的空间，而且在写入频繁的场景下，对于B+树的维护所付出的性能消耗也会越大**

#### 什么字段适合当主键？

1. 字段具有唯一性且不为空
2. 字段最好有递增趋势
3. 不建议用业务数据作为主键
4. 对于多台服务器，考虑分布式id

#### 表中十个字段，你主键用自增ID还是UUID，为什么？

**自增 id。**

- 因为 **uuid 相对顺序的自增 id 来说是毫无规律**可言的，**新行的值不一定要比之前的主键的值要大**，所以 innodb **无法做到总是把新行插入到索引的最后，而是需要为新行寻找新的合适的位置从而来分配新的空间。**

- 这个过程需要做很多额外的操作，数据的毫无顺序会导致数据分布散乱，将会导致以下的问题：

  - **写入的目标页很可能已经刷新到磁盘上并且从缓存上移除**，或者还没有被加载到缓存中，innodb 在插入之前**不得不先找到并从磁盘读取目标页到内存中，这将导致大量的随机 IO。**

  - 因为写入是乱序的**，innodb 不得不频繁的做页分裂操作，以便为新的行分配空间，页分裂导致移动大量的数据，影响性能。**

  - 由于**频繁的页分裂，页会变得稀疏并被不规则的填充，最终会导致数据会有碎片。**

  - **UUID 太占用内存。**每个 UUID 由 36 个字符组成，在字符串进行比较时，**需要从前往后比较，字符串越长，性能越差**。另外字符串越长，**占用的内存越大，**由于页的大小是固定的，这样一个页上能存放的**关键字数量就会越少，**这样最终就会导致**索引树的高度越大，**在索引搜索的时候，发生的磁盘 IO 次数越多，性能越差

    结论：使用 InnoDB 应该尽可能的按主键的自增顺序插入，并且尽可能使用单调的增加的聚簇键的值来插入新行

#### 性别字段能加索引么？为啥？

不建议针对性别字段加索引。
实际上与索引创建规则之一区分度有关，性别字段假设有100w数据，50w男、50w女，区别度几乎等于 0 。
区分度的计算方式 ：select count(DISTINCT sex)/count(*) from sys_user
实际上对于性别字段不适合创建索引，是因为select * 操作，还得进行50w次回表操作，根据主键从聚簇索引中找到其他字段 ，这一部分开销从上面的测试来说还是比较大的，所以从性能角度来看不建议性别字段加索引，加上索引并不是索引失效，而是回表操作使得变慢的。
既然走索引的查询的成本比全表扫描高，优化器就会选择全表扫描的方向进行查询，这时候建立的性别字段索引就没有启到加快查询的作用，反而还因为创建了索引占用了空间

#### 有什么优化索引的方法？

- **前缀索引优化**

  - orderby无法使用
  - 不能把前缀索引用作覆盖索引

- **覆盖索引优化**

- **主键索引最好是自增的**

  - 如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置
  - 非自增索引，需要移动其他数据来满足新数据的插入，可能会造成页分裂，从而造成大量的内存碎片，导致索引结构不连贯，影响查询效率。
  - 另外，主键字段的长度不要太大，因为主键字段长度越小，意味着二级索引的叶子节点越小（二级索引的叶子节点存放的数据是主键值），这样二级索引占用的空间也就越小。

- 索引最好设置为Not Null

  - 索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如进行索引统计时，count 会省略值为NULL 的行。
  - NULL 值是一个没意义的值，但是它会占用物理空间，所以会带来的存储空间的问题

- **防止索引失效**

详细讲讲

- 前缀索引优化：使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。
- 覆盖索引优化：覆盖索引是指 SQL 中 query 的所有字段，在索引 B+Tree 的叶子节点上都能找得到的那些索引，从二级索引中查询得到记录，而不需要通过聚簇索引查询获得，可以避免回表的操作。
- 主键索引最好是自增的：
  - 如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。**因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。**
  - 如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，**因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。**
- 防止索引失效：
  - 当我们使**用左或者左右模糊匹配**的时候，也就是 like %xx 或者 like %xx%这两种方式都会造成索引失效；
  - 当我们在查询条件中**对索引列做了计算、函数、类型转换操作，**这些情况下都会造成索引失效；
  - **联合索引要能正确使用需要遵循最左匹配原则**，也就是按照最左优先的方式进行索引的匹配，否则就会导致索引失效。
  - **在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，**那么索引会失效

### 如果Explain用到的索引不正确的话，有什么办法干预吗？

可以使用 force index，强制走索引。

### 3.2 从数据页的角度看b+树

#### InnoDB是如何存储数据的

InnoDB 的数据是按「数据页」为单位来读写的

数据页包含7部分：

1. 文件头

   ​	在 File Header 中有两个指针，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向链表。采用链表的结构是让数据页之间不需要是物理上的连续的，而是逻辑上的连续。

2. 页头

3. 最小和最大记录

4. 用户记录

   ​	数据页中的记录按照「主键」顺序组成单向链表，单向链表的特点就是插入、删除非常方便，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索。

5. 空闲空间

6. 页目录

7. 文件尾

#### B+树是如何进行查询的

​	在定位记录所在哪一个页时，也是通过二分法快速定位到包含该记录的页。定位到该页后，又会在该页内进行二分法快速定位记录所在的分组（槽号），最后在分组内进行遍历查找。

### 3.3 为什么MySQL采用b+树作为索引

#### 怎样的索引的数据结构是好的

- 能在尽可能少的磁盘的 I/O 操作中完成查询工作；
- 要能高效地查询某一个记录，也要能高效地执行范围查找；

#### 二分查找、二分查找树、自平衡二叉树、什么是b树

树是存储在磁盘中的，访问每个节点，都对应一次磁盘 I/O 操作（假设一个节点的大小「小于」操作系统的最小读写单位块的大小），也就是说树的高度就等于每次查询数据时磁盘 IO 操作的次数，所以树的高度越高，就会影响查询性能。

而如果同样的节点数量在平衡二叉树的场景下，树的高度就会很高，意味着磁盘 I/O 操作会更多。所以，B 树在数据查询中比平衡二叉树效率要高。但是 B 树的每个节点都包含数据（索引+记录），而用户的记录数据的大小很有可能远远超过了索引数据，这就需要花费更多的磁盘 I/O 操作次数来读到「有用的索引数据」。 

#### 什么是b+树

b+树和b树的差异：

1. 叶子节点（最底部的节点）才会存放实际数据（索引+记录），非叶子节点只会存放索引；
2. 所有索引都会在叶子节点出现，叶子节点之间构成一个有序链表；非叶子节点的索引也会同时存在在子节点中，并且是在子节点中所有索引的最大（或最小）。
3. 非叶子节点中有多少个子节点，就有多少个索引；（MySQL中的b+树，b+树对于非叶子节点的子节点和索引的个数，定义的方式可能有所不同）

b+树特点

1. **所有叶子节点都在同一层：**这是B+树的一个重要特性，确保了所有数据项的检索都具有相同的I/O延迟，提高了搜索效率。每个叶子节点都包含指向相邻叶子节点的指针，形成一个链表，由于叶子节点之间的链接，B+树非常适合进行范围查询和排序扫描。可以沿着叶子节点的链表顺序访问数据，而无需进行多次随机访问。
2. **非叶子节点存储键值：**非叶子节点仅存储键值和指向子节点的指针，不包含数据记录。这些键值用于指导搜索路径，帮助快速定位到正确的叶子节点。并且，由于非叶子节点只存放键值，当数据量比较大时，相对于B树，B+树的层高更少，查找效率也就更高。
3. **叶子节点存储数据记录**：与B树不同，B+树的叶子节点存储实际的数据记录或指向数据记录的指针。这意味着每次搜索都会到达叶子节点，才能找到所需数据。
4. **自平衡：**B+树**在插入、删除和更新操作后会自动重新平衡**，确保树的高度保持相对稳定，从而保持良好的搜索性能。每个节点最多可以有M个子节点，最少可以有ceil(M/2)个子节点（除了根节点），这里的M是树的阶数

##### 单点查询

**B+ 树的非叶子节点不存放实际的记录数据，仅存放索引，**因此数据量相同的情况下，相比存储即存索引又存记录的 B 树，B+树的非叶子节点可以存放更多的索引，因此 B+ 树可以比 B 树更「矮胖」，查询底层节点的磁盘 I/O次数会更少。

##### 插入和删除效率

**B+ 树有大量的冗余节点（所有非叶子节点都是冗余索引），**这些冗余索引让 B+ 树在插入、删除的效率都更高，**比如删除根节点的时候，不会像 B 树那样会发生复杂的树的变化；**

##### 范围查询

**B+ 树叶子节点之间用链表连接了起来，有利于范围查询，**而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。

##### MySQL中的B+树

InnoDB实现

- B+ 树的叶子节点之间是用「双向链表」进行连接，这样的好处是既能向右遍历，也能向左遍历。为了实现倒序遍历或者排序。
- B+ 树点节点内容是数据页，数据页里存放了用户的记录以及各种信息，每个数据页默认大小是 16 KB。

#### 为什么不用跳表

B+树的高度在3层时存储的数据可能已达千万级别**，但对于跳表而言同样去维护千万的数据量那么所造成的跳表层数过高而导致的磁盘io次数增多，也就是使用B+树在存储同样的数据下磁盘io次数更少。**

### 3.4 MySQL单表不要超过2000w行，靠谱吗

#### 单表数量限制

id 是主键，本身就是唯一的，也就是说主键的大小可以限制表的上限：

- 如果主键声明 int 类型，也就是 32 位，那么支持 2^32-1 ~~21 亿；
- 如果主键声明 bigint 类型，那就是 2^62-1 （36893488147419103232），难以想象这个的多大了，一般还没有到这个限制之前，可能数据库已经爆满了

#### 页的数据结构

- 在页的 7 个组成部分中，我们自己存储的记录会按照我们指定的行格式存储到 User Records 部分。
- 但是在一开始生成页的时候，**其实并没有 User Records 这个部分，每当我们插入一条记录，都会从 Free Space 部分，也就是尚未使用的存储空间中申请一个记录大小的空间划分到 User Records 部分。**
- 当 Free Space 部分的空间全部被 User Records 部分替代掉之后，也就意味着这个页使用完了，如果还有新的记录插入的话，就需要去申请新的页了。

#### 索引的数据结构

- 在 MySQL 中索引的数据结构和刚刚描述的页几乎是一模一样的，而且大小也是 16K,。
- 但是在索引页中记录的是页 (数据页，索引页) 的**最小主键 id 和页号**，以及在索引页中增加了层级的信息，从 0 开始往上算，所以页与页之间就有了上下层级的概念。
  - id:对应页中记录的最小记录
  - 页号：地址是指向对应页的指针

而数据页与此几乎大同小异，区别在于数据页记录的是真实的行数据而不是页地址，而且 id 的也是顺序的。

#### 单表建议值

假设

- 非叶子节点内指向其他页的数量为 x
- 叶子节点内能容纳的数据行数为 y
- B+ 数的层数为 z

总数会等于 x 的 z-1 次方 与 Y 的乘积。页的其他结构1k，整个页16k，剩下15k用于存储数据。

- x
  - 索引页中：主键bigint 8byte 页号固定4byte 索引页中一条数据12byte
  - 15*1024/12 ~= 1280行
- y
  - 但是叶子节点中存放的是真正的行数据，这个影响的因素就会多很多，比如，字段的类型，字段的数量。每行数据占用空间越大，页中所放的行数量就会越少。
  - 这边我们暂时按一条行数据 1k 来算，那一页就能存下 15 条，Y = 15*1024/1000 ≈15。

假设b+树三层，2.45kw，所以b+树的最大行数建议值2000w，4层的时候，300多亿，不合理。

### 3.5 索引失效有哪些？

#### 索引存储结构长什么样？

- InnoDB 存储引擎：B+ 树索引的叶子节点保存数据本身；

  - 对主键字段建立的索引叫做聚簇索引，对普通字段建立的索引叫做二级索引。那么多个普通字段组合在一起创建的索引就叫做联合索引，也叫组合索引。

    - 聚簇索引
      - 聚簇索引的叶子节点存放的是实际数据，所有完整的用户数据都存放在聚簇索引的叶子节点

    - 二级索引
      - 二级索引的叶子节点存放的是主键值，而不是实际数据。

  - 回表：
    - 先在「二级索引」的 B+ 树找到对应的叶子节点，获取主键值；
    - 然后用上一步获取的主键值，在「聚簇索引」中的 B+ 树检索到对应的叶子节点，然后获取要查询的数据。
  - 索引下推  
    - 在存储引擎层进行索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，再返还给 Server 层，从而减少回表次数。
  - 覆盖索引
    - 如果要查询的数据在「二级索引」的叶子节点，那么只需要在「二级索引」的 B+ 树找到对应的叶子节点，然后读取要查询的数据，这个过程叫做覆盖索引

- MyISAM 存储引擎：B+ 树索引的叶子节点保存数据的物理地址；

如果用了索引列，索引过程不一定用到索引，某些情况会导致索引失效，发生全表扫描

-  对索**引使用左或者左右模糊匹配**
   - 也就是 like %xx 或者 like %xx% 这两种方式都会造成索引失效。
-  对索引使用**函数**
   - 索引保存的是索引字段的原值，不是经过函数计算以后的，所以没法走索引
   - 但是8.0以后增加函数索引，可以针对函数计算后的值增加一个索引，再次查询的时候可以走索引
-  对索引进行**表达式**计算
   - 索引保存原始值，而不是 id + 1 表达式计算后的值，所以无法走索引，只能通过把索引字段的取值都取出来，然后依次进行表达式的计算来进行条件判断，因此采用的就是全表扫描的方式。
   - 但是可以索引扫描 id =  10 - 1
-  对**索引隐式类型转换**
   - MySQL 在遇到字符串和数字比较的时候，会自动把字符串转为数字，然后再进行比较。
   - 所以如果索引是整型，查询是字符串 不会导致索引失效
   - 如果索引是字符串，查询是数字，会走全表扫描
-  联合索引**非最左匹配**
   - 在联合索引的情况下，数据是按照索引第一列排序，第一列数据相同时才会按照第二列排序。
   - 如果我们想使用联合索引中尽可能多的列，查询条件中的各个列必须是联合索引中从最左边开始连续的列。如果我们仅仅按照第二列搜索，肯定无法走索引
-  **WHERE子句中的or**
   - 在 WHERE 子句中，如果在 OR 前的条件列是索引列，而在 OR 后的条件列不是索引列，那么索引会失效。因为 OR 的含义就是两个只要满足一个即可，因此只有一个条件列是索引列是没有意义的，只要有条件列不是索引列，就会进行全表扫描。

### 3.6 MySQL 使用 like “%x“，索引一定会失效吗？

使用左模糊匹配（like "%xx"）**并不一定会走全表扫描，关键还是看数据表中的字段。**

- 如果数据库表中的字段只有主键+二级索引，那么即使使用了左模糊匹配，也不会走全表扫描（type=all），而是走全扫描二级索引树(type=index)。
- 我们都知道联合索引要遵循最左匹配才能走索引，但是如果数据库表中的字段都是索引的话，即使查询过程中，没有遵循最左匹配原则，也是走全扫描二级索引树(type=index）

### 3.7 count(*) 和 count(1) 有什么区别？哪个性能最好？

#### 哪种count性能最好？

**count(*) = count(1)>count(主键字段)>count(字段)**

count() 是一个聚合函数，函数的参数不仅可以是字段名，也可以是其他任意表达式，该函数作用是统计符合查询条件的记录中，函数指定的参数不为 NULL 的记录有多少个。

- 在通过 count 函数统计有多少个记录时，MySQL 的 server 层会维护一个名叫 count 的变量。
- server 层会循环向 InnoDB 读取一条记录，如果 count 函数指定的参数不为 NULL，那么就会将变量 count 加 1，直到符合查询的全部记录被读完，就退出循环
- 最后将 count 变量的值发送给客户端。

1. 如果表里只有主键索引，没有二级索引时，那么，InnoDB 循环遍历聚簇索引。如果表里有二级索引时，InnoDB 循环遍历的对象就不是聚簇索引，而是二级索引。
2. count(1)、 count(*)、 count(主键字段)在执行的时候，如果表里存在二级索引，优化器就会选择二级索引进行扫描。
   所以，如果要执行 count(1)、 count(*)、 count(主键字段) 时，尽量在数据表上建立二级索引，这样优化器会自动采用 key_len 最小的二级索引进行扫描，相比于扫描主键索引效率会高一些。
   再来，就是不要使用 count(字段) 来统计记录个数，因为它的效率是最差的，会采用全表扫描的方式来统计。如果你非要统计表中该字段不为 NULL 的记录个数，建议给这个字段建立一个二级索引。

##### Count（1）

- InnoDB 循环遍历聚簇索引（主键索引），将读取到的记录返回给 server 层，但是不会读取记录中的任何字段的值，因为 count 函数的参数是 1，不是字段，所以不需要读取记录中的字段值。参数 1 很明显并不是 NULL，因此 server 层每从 InnoDB 读取到一条记录，就将 count 变量加 1。
- 可以看到，count(1) 相比 count(主键字段) 少一个步骤，就是不需要读取记录中的字段值，所以通常会说 count(1) 执行效率会比 count(主键字段) 高一点。

##### Count(*)

- count(\*) 其实等于 count(0)，也就是说，当你使用 count(*) 时，MySQL 会将 * 参数转化为参数 0 来处理。count(*) 执行过程跟 count(1) 执行过程基本一样的，性能没有什么差异。
- 而且 MySQL 会对 count(*) 和 count(1) 有个优化，如果有多个二级索引的时候，优化器会使用key_len 最小的二级索引进行扫描。只有当没有二级索引的时候，才会采用主键索引来进行统计

##### Count(字段)

全表扫描

#### 为什么要通过遍历的方式来计数？

- 在 MyISAM 存储引擎里，执行 count 函数的方式是不一样的，通常在没有任何查询条件下的 count(*)，MyISAM 的查询速度要明显快于 InnoDB。
  - 使用 MyISAM 引擎时，执行 count 函数只需要 O(1 )复杂度，这是因为每张 MyISAM 的数据表都有一个 meta 信息有存储了row_count值，由表级锁保证一致性，所以直接读取 row_count 值就是 count 函数的执行结果。
- 而 InnoDB 存储引擎是支持事务的，同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB 表“应该返回多少行”也是不确定的，所以无法像 MyISAM一样，只维护一个 row_count 变量。
- 而当带上 where 条件语句之后，MyISAM 跟 InnoDB 就没有区别了，它们都需要扫描表来进行记录个数的统计。

#### 如何优化count(*)

1. 近似值 explain
2. 额外表保存计数值

## 事务篇

### 4.1 事务隔离级别是怎么实现的

数据库中的「事务（Transaction）」就能达到这样的效果。
我们在转账操作前先开启事务，等所有数据库操作执行完成后，才提交事务，对于已经提交的事务来说，该事务对数据库所做的修改将永久生效，如果中途发生发生中断或错误，那么该事务期间对数据库所做的修改将会被回滚到没执行该事务之前的状态。**事务是逻辑上的一组操作，要么都执行，要么都不执行。**

#### 事务有哪些特性？

- 事务是由 MySQL 的引擎来实现的，我们常见的 InnoDB 引擎它是支持事务的。
- 比如 MySQL 原生的 MyISAM 引擎就不支持事务，也正是这样，所以大多数 MySQL 的引擎都是用 InnoDB。

1. 原子性 Atomicity

2. 一致性 Consistency

3. 隔离性 Isolation

   防止多个事务并发执行时由于交叉执行而导致数据的不一致

4. 持久性 Durability

InnoDB引擎通过什么来保证事务这四个特性的呢？

1. 原子性是通过 **undo log（回滚日志）** 来保证的；
2. 隔离性是通过 **MVCC（多版本并发控制） 或锁机制**来保证的；
3. 持久性是通过 **redo log （重做日志**）来保证的；
4. 一致性则是通过持久性+原子性+隔离性来保证；

**即，AID是手段，C是目的**

#### 并行事务会引起什么问题

Mysql允许多个客户端连接，即可能会出现同时处理多个事务的情况。在同时处理多个事务的时候，就可能出现**脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题。**

##### 脏读

- 如果一个事务「**读到」了另一个「未提交事务修改过的数据」，**就意味着发生了「脏读」现象。
- 即a修改了没提交被b读到

##### 不可重复读

- 在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。
  - 描述的是在一个事务内，对某条数据的两次**读取**之间，由于其他事务对该数据进**行了修改或删除，导致第一次读取和第二次读取的结果不同。**

- b先读，a提交b再读，前后读取数据不一样

##### 幻读

- 在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。
  - 描述的是在一个事务内，对一组数据（如通过范围查询得到的数据集合）的两次读取之间，**由于其他事务插入了新的数据，**导致第一次**读取**和第二次读取的**结果集数量不同。**

- 前后读取的记录数量不一致

**幻读其实可以看作是不可重复读的一种特殊情况，单独把幻读区分出来的原因主要是解决幻读和不可重复读的方案不一样。**执行 delete 和 update 操作的时候，可以直接对记录加锁，保证事务安全。而执行 insert 操作的时候，由于记录锁（Record Lock）只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁（Gap Lock）。也就是说执行 insert 操作的时候需要依赖 Next-Key Lock（Record Lock+Gap Lock） 进行加锁来保证不出现幻读。

#### 事务的隔离级别有哪些

 SQL 标准提出了四种隔离级别来规避这些现象，**隔离级别越高，性能效率就越低**，这四个隔离级别如下：

- 读未提交（read uncommitted），指一个事务**还没提交时，它做的变更就能被其他事务看到**；
- 读提交（read committed），指一个事务提**交之后，它做的变更才能被其他事务看到**；
- 可重复读（repeatable read），指一个事务**执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致**的，**MySQL InnoDB 引擎的默认隔离级别；**
- 串行化（serializable ）；会对**记录加上读写锁**，在**多个事务对这条记录进行读写操作时，如果发生了读写冲突的时候**，后访问的事**务必须等前一个事务执行完成**，才能继续执行
  - 实现：**是通过行级锁来实现的，序列化隔离级别下，普通的 select 查询是会对记录加 S 型的 next-key 锁，**其他事务就没没办法对这些已经加锁的记录进行增删改操作了，从而避免了脏读、不可重复读和幻读现象


我们讨论的 MySQL 虽然支持 4 种隔离级别，但是与SQL 标准中规定的各级隔离级别允许发生的现象却有些出入。

MySQL 在「可重复读」隔离级别下，可以很大程度上避免幻读现象的发生（注意是很大程度避免，并不是彻底避免），所以 **MySQL 并不会使用「串行化」隔离级别来避免幻读现象的发生**，因为使用「串行化」隔离级别会影响性能。

#### 并发事务的控制方式有哪些？

- 锁
- MVCC
  - undolog
    - readview+隐藏字段
- 事务隔离级别

#### MySQL的隔离级别是怎么实现的？

- 锁
- MVCC

串行化是通过锁来实现的，读提交和可重复读是通过MVCC实现的。不过，除了串行化之外的其他隔离级别可能也需要用锁机制，比如可重复读在当前读情况下就需要锁来保证不会出现幻读。

#### ReadView在MVCC里如何工作的？

##### ReadView的四个字段

1. m_ids ：指的是在创建 Read View 时，当前数据库中「活跃事务」的事务 id 列表，注意是一个列表，“活跃事务”指的就是，启动了但还没提交的事务。
2. min_trx_id ：指的是在创建 Read View 时，当前数据库中「活跃事务」中事务 id 最小的事务，也就是 m_ids 的最小值。
3. max_trx_id ：这个并不是 m_ids 的最大值，而是创建 Read View 时当前数据库中应该给下一个事务的 id 值，也就是全局事务中最大的事务 id 值 + 1；
4. creator_trx_id ：指的是创建该 Read View 的事务的事务 id

简单来说就是

​	**当前活跃事务的事务id列表，当前活跃事务id最小的事务，创建readview时数据库给下一个事务的id，该事务的id**

##### 聚簇索引记录中两个跟事务有关的隐藏列

对于使用InnoDB存储引擎的数据库表，聚簇索引记录中包含以下两个隐藏列：

1. trx_id，当一个事务对某条聚簇索引记录进行改动时，就会把该事务的事务 id 记录在 trx_id 隐藏列里；
2. roll_pointer，每次对某条聚簇索引记录进行改动时，都会把旧版本的记录写入到 undo 日志中，然后这个隐藏列是个指针，指向每一个旧版本记录，于是就可以通过它找到修改前的记录。

简单来说就是

​	**改动聚簇索引记录的事务id，以及一个指向旧版本记录的指针**



**一个事务去访问记录的时候，除了自己的更新记录总是可见之外，还有这几种情况：**

1. **如果记录的 trx_id 值小于 Read View 中的 min_trx_id 值，表示这个版本的记录是在创建 Read View 前已经提交的事务生成的，所以该版本的记录对当前事务可见。**

2. **如果记录的 trx_id 值大于等于 Read View 中的 max_trx_id 值，表示这个版本的记录是在创建 Read View 后才启动的事务生成的，所以该版本的记录对当前事务不可见。**

3. **如果记录的 trx_id 值在 Read View 的 min_trx_id 和 max_trx_id 之间，需要判断 trx_id 是否在 m_ids 列表中：**

   1. **如果记录的 trx_id 在 m_ids 列表中，表示生成该版本记录的活跃事务依然活跃着（还没提交事务），所以该版本的记录对当前事务不可见。**
   2. **如果记录的 trx_id 不在 m_ids列表中，表示生成该版本记录的活跃事务已经被提交，所以该版本的记录对当前事务可见。**

   **这种通过「版本链」来控制并发事务访问同一个记录时的行为就叫 MVCC（多版本并发控制）。**

#### 可重复读是如何工作的

'可重复读隔离级别是**启动事务时生成一个 Read View，然后整个事务期间都在用这个 Read View。**

- a启动以后b启动
- 事务 B 读取小林的账户余额记录，读到余额是 100 万；
  事务 A 将小林的账户余额记录修改成 200 万，并没有提交事务；
  事务 B 读取小林的账户余额记录，读到余额还是 100 万；
  事务 A 提交事务；
  事务 B 读取小林的账户余额记录，读到余额依然还是 100 万；
- 然后事务 B 第二次去读取该记录，发现这条记录的 trx_id 值为 51，在事务 B 的 Read View 的 min_trx_id 和 max_trx_id 之间，则需要判断 trx_id 值是否在  m_ids 范围内，判断的结果是在的，那么说明这条记录是被还未提交的事务修改的，这时事务 B 并不会读取这个版本的记录。而是沿着 undo log 链条往下找旧版本的记录，直到找到 trx_id 「小于」事务 B 的 Read View 中的 min_trx_id 值的第一条记录，所以事务 B 能读取到的是 trx_id 为 50 的记录，也就是小林余额是 100 万的这条记录。
- 最后，当事物 A 提交事务后，由于隔离级别时「可重复读」，所以事务 B 再次读取记录时，还是**基于启动事务时创建的 Read View 来判断**当前版本的记录是否可见。所以，即使事物 A 将小林余额修改为 200 万并提交了事务， **事务 B 第三次读取记录时，读到的记录都是小林余额是 100 万的这条记录。**

#### 读提交是如何工作的

读提交隔离级别是在**每次读取数据时，都会生成一个新的 Read View。**

- 在事务 A 提交后，由于隔离级别是「读提交」，所以事务 B 在每次读数据的时候，会重新创建  Read View。
- 事务 B 在找到小林这条记录时，会发现这条记录的 trx_id 是 51，比事务 B 的 Read View 中的 min_trx_id 值（52）还小，这意味着修改这条记录的事务早就在创建  Read View 前提交过了，所以该版本的记录对事务 B 是可见的。

### 4.2 MySQL可重复读隔离级别，完全解决幻读了吗？

MySQL InnoDB 引擎的默认隔离级别虽然是「可重复读」，但是它很大程度上避免幻读现象（并不是完全解决了），解决的方案有两种：

- 针对快照读（**普通 select 语句**），**是通过 MVCC 方式解决了幻读，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。**
- 针对当前读（**select ... for update 等语句**），是**通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题。**

#### **什么是幻读**

只要 T1 和 T2 时刻执行产生的结果集是不相同的，那就发生了幻读的问题

#### 快照读是如何避免幻读的？

可重复读隔离级是由 MVCC（多版本并发控制）实现的，实现的方式是开始事务后（执行 begin 语句后），在执行第一个查询语句后，会创建一个 Read View，后续的查询语句利用这个 Read View，通过这个  Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的，即使中途有其他事务插入了新纪录，是查询不出来这条数据的，所以就很好了避免幻读问题。

#### 当前读是如何避免幻读的？

**MySQL 里除了普通查询是快照读，其他都是当前读，**比如 **update、insert、delete，这些语句执行前都会查询最新版本的数据，然后再做进一步的操作。另外，select ... for update 这种查询语句是当前读，每次执行的时候都是读取最新的数据。**

Innodb 引擎为了解决「可重复读」隔离级别使用「当前读」而造成的幻读问题，就引出了**间隙锁。**

- 事务 A 执行了这面这条**锁定读语句后**，就在对表中的记录加上 id 范围为 (2, +∞] 的 next-key lock（next-key lock 是间隙锁+记录锁的组合）。
- 然后，事务 B 在执行插入语句的时候，判断到插入的位置被事务 A 加了 next-key lock，于是事物 B 会**生成一个插入意向锁，同时进入等待状态，直到事务 A 提交了事务。这就避免了由于事务 B 插入新记录而导致事务 A 发生幻读的现象**

#### 一条update是不是原子性的？为什么

是原子性，主要通过锁+undolog 日志保证原子性的

- **执行 update 的时候，会加行级别锁，保证了一个事务更新一条记录的时候，不会被其他事务干扰。**
- **事务执行过程中，会生成 undolog，如果事务执行失败，就可以通过 undolog 日志进行回滚。**

#### 幻读被完全解决了吗？

**可重复读隔离级别下虽然很大程度上避免了幻读，但是还是没有能完全解决幻读。**

- 第一个例子：**对于快照读**， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。
  - 在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。                                                                                                                                                                
  - **即a一开始查id为为5的，b插入一个id为5的，由于mvcc现在看不到，然后a对id为5的进行更新，这个时候b插入的记录隐藏列的值为a的id，对a可见，a再次查询，就和第一次查的不一样。**
- 第二个例子：**对于当前读**，如果事务开启后，并没有执行当前读，而是先快照读，然后这期间如果其他事务插入了一条记录，那么事务后续使用当前读进行查询的时候，就会发现两次查询的记录条目就不一样了，所以就发生幻读。
  - **即一开始select，后来b插入，这时selectforupdate**
- 要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

### 滥用事务，或者一个事务里面有特别 多sql的弊端？

1. 如果一个事务特别多 sql，**锁定的数据太多，容易造成大量的死锁和锁超时。**
2. **回滚记录会占用大量存储空间，事务回滚时间长。**在MySQL (opens new window)中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值，sql 越多，所需要保存的回滚数据就越多。
3. **执行时间长，容易造成主从延迟，主库上必须等事务执行完成才会写入binlog，再传给备库。**所以，如果一个主库上的语句执行10分钟，那这个事务很可能就会导致从库延迟10分钟

## 锁篇

### MySQL有哪些锁？

#### 全局锁

- 执行后，整个数据库就处于只读状态了，这时其他线程执行以下操作，都会被阻塞：
  - 对数据的增删改操作，比如 insert、delete、update等语句；
  - 对表结构的更改操作，比如 alter table、drop table 等语句。
- 会话断开，全局锁自动被释放
- 应用场景
  - 全库逻辑备份
- 加全局锁缺点
  - 数据多-》备份时间长-》只读状态-》业务不能更新数据-》业务停滞
  - 解决办法
    - 如果数据库的引擎支持的事务支持可重复读的隔离级别，那么在备份数据库之前先开启事务，会先创建 Read View，然后整个事务执行期间都在用这个 Read View，而且由于 MVCC 的支持，备份期间业务依然可以对数据进行更新操作。
    - 但是，对于 MyISAM 这种不支持事务的引擎，在备份数据库时就要使用全局锁的方法。

#### 表级锁

##### 表锁

- 表锁限制别的线程和自己线程的读写
- 会话退出，释放所有锁
- 表锁颗粒度太大，影响并发性能，因此使用颗粒度更细的行级锁

##### 元数据锁（MDL）

- 不需要显式使用，对数据库表进行操作时，会自动给加上MDL
  - CRUD操作加MDL读锁
  - 对表进行结构操作的时候加MDL写锁
- MDL 是为了保证当用户对表执行 CRUD 操作时，防止其他线程对这个表结构做了变更。
- 在事务执行期间，MDL一直持有，事务提交后才会释放
- 一个线程因为写锁申请不到的时候，后续的申请读锁的查询操作也会被阻塞
  - 申请MDL锁形成队列，写锁优先级高。
  - 所以为了能安全的对表结构进行变更，在对表结构变更前，先要看看数据库中的长事务，是否有事务已经对表加上了 MDL 读锁，如果可以考虑 kill 掉这个长事务，然后再做表结构的变更。

##### 意向锁

- 在遍历对某些记录加上共享锁||独占锁之前，需要先在表级别上加上一个意向共享锁||意向独占锁
- **意向共享锁和意向独占锁是表级锁，不会和行级的共享锁和独占锁发生冲突**，而且意向锁之间也不会发生冲突，只会和共享表锁（lock tables ... read）和独占表锁（lock tables ... write）发生冲突。
- 意向锁的目的是为了快速判断表里是否有记录被加锁
- 意向锁是由数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享/排他锁之前，InnoDB 会先获取该数据行所在在数据表的对应意向锁。

##### AUTO-INC锁

- Auto-Inc是特殊的表锁价值，锁不是再一个事务提交后才释放，而是再执行完插入语句后就会立即释放。

- 在插入数据的时候，可以不指定主键的值，数据库会自动给主键赋值递增的值

- 但是， AUTO-INC 锁再对大量数据进行插入的时候，会影响插入性能，因为另一个事务中的插入会被阻塞。

  - MySQL5.1.22后，提供了一种轻量级的锁来实现自增
  - 一样也是在插入数据的时候，会为被 AUTO_INCREMENT 修饰的字段加上轻量级锁，然后给该字段赋值一个自增的值，**就把这个轻量级锁释放了，而不需要等待整个插入语句执行完后才释放锁。**

- 通过 innodb_autoinc_lock_mode控制使用什么锁

  - 0

    auto-inc，语句结束后释放锁

  - 1

    - 普通insert语句，使用轻量级锁，申请主键后马上释放
    - insert-select批量插入数据，语句结束后释放

  - 2

    - 轻量级，申请主键后释放
    - 效率最高，但是当搭配 binlog 的日志格式是 statement 一起使用的时候，**在「主从复制的场景」中会发生数据不一致的问题。**
    - 当 innodb_autoinc_lock_mode = 2 时，并且 binlog_format = row，既能提升并发性，又不会出现数据一致性问题。

#### 行级锁

InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁。

- 普通的 select 语句是不会对记录加锁的，因为它属于**快照读。**

- 查询会加锁的语句称为**锁定读。**

  ```mysql
  //对读取的记录加共享锁
  select ... lock in share mode;
  //对读取的记录加独占锁
  select ... for update;
  上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin、start transaction 或者 set autocommit = 0。
  ```

- 共享锁（S锁,Shared lock）满足读读共享，读写互斥。独占锁（X锁,exclusive lock）满足写写互斥、读写互斥。

  - sx不兼容
  - ss兼容
  - xx不兼容

1. Record Lock，记录锁，也就是仅仅把一条记录锁上；
2. Gap Lock，间隙锁，锁定一个范围，但是不包含记录本身；
3. Next-Key Lock：临键锁，Record Lock + Gap Lock 的组合，锁定一个范围，并且锁定记录本身

##### Record Lock

当事务执行 commit 后，事务过程中生成的锁都会被释放。

##### Gap Lock

- Gap Lock 称为间隙锁，只存在于可重复读隔离级别，目的是为了解决可重复读隔离级别下幻读的现象。
- **间隙锁虽然存在 X 型间隙锁和 S 型间隙锁，但是并没有什么区别，间隙锁之间是兼容的**，即两个事务可以同时持有包含共同间隙范围的间隙锁，并不存在互斥关系，因为间隙锁的目的是防止插入幻影记录而提出的。

##### Next-Key Lock

- next-key lock 即能保护该记录，又能阻止其他事务将新纪录插入到被保护记录前面的间隙中
- **next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的。**

##### 插入意向锁

- 一个事务在插入一条记录的时候，需要判断插入位置是否已被其他事务加了间隙锁（next-key lock 也包含间隙锁）。
- 如果有的话，插入操作就会发生阻塞，直到拥有间隙锁的那个事务提交为止（释放间隙锁的时刻），在此期间会生成一个插入意向锁，表明有事务想在某个区间插入新记录，但是现在处于等待状态。
- 会判断到插入的位置已经被事务 A 加了间隙锁，于是事物 B 会生成一个插入意向锁，然后将锁的状态设置为等待状态（PS：MySQL 加锁时，是先生成锁结构，然后设置锁的状态，如果锁状态是等待状态，并不是意味着事务成功获取到了锁，只有当锁状态为正常状态时，才代表事务成功获取到了锁）。此时事务 B 就会发生阻塞，直到事务 A 提交了事务。
- 插入意向锁名字虽然有意向锁，但是它并不是意向锁，它是一种特殊的间隙锁，属于行级别锁。

#### 表级锁和行级锁了解吗？有什么区别

表级锁和行级锁对比：
表级锁： MySQL 中**锁定粒度最大的**一种锁（全局锁除外），是针对非索引字段加的锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。不过，触发锁冲突的概率最高，高并发下效率极低。表级锁和存储引擎无关，MyISAM 和 InnoDB 引擎都支持表级锁。
行级锁： MySQL 中锁定**粒度最小**的一种锁，是 针对索引字段加的锁 ，只针对当前操作的行记录进行加锁。 行级锁能大大**减少数据库操作的冲突**。其加锁粒度最小**，并发度高，**但加锁的开销也最大，加锁慢，会出现死锁。行级锁和存储引擎有关，是在存储引擎层面实现的

**锁定粒度、针对索引or非索引加锁，对表or行加锁，并发度，加锁开销，和存储引擎是否有关。**

### MySQL是怎么加锁的

#### 什么SQL语句会加行级锁？

1. InnoDB 引擎是支持行级锁的，而 MyISAM 引擎并不支持行级锁，所以后面的内容都是基于 InnoDB 引擎的。

2. 普通的 select 语句是不会对记录加锁的（除了串行化隔离级别），因为它属于快照读，是通过  MVCC（多版本并发控制）实现的。

3. 如果要在查询时对记录加行级锁，可以使用下面这两个方式，这两种查询会加锁的语句称为锁定读。

   ```mysql
   //对读取的记录加共享锁(S型锁)
   select ... lock in share mode;
   //对读取的记录加独占锁(X型锁)
   select ... for update;
   
   //上面这两条语句必须在一个事务中，因为当事务提交了，锁就会被释放，所以在使用这两条语句的时候，要加上 begin 或者 start transaction 开启事务的语句。
   ```

4. 除了上面这两条锁定读语句会加行级锁之外，update 和 delete 操作都会加行级锁，且锁的类型都是独占锁(X型锁)。

#### 行级锁有哪些种类

- 在读已提交隔离级别下，行级锁的种类只有记录锁，也就是仅仅把一条记录锁上。
- 在可重复读隔离级别下，行级锁的种类除了有记录锁，还有间隙锁（目的是为了避免幻读），所以行级锁的种类主要有三类

#### MySQL是怎么加行级锁的？

- **加锁的对象是索引，加锁的基本单位是 next-key lock**，它是由记录锁和间隙锁组合而成的，next-key lock 是前开后闭区间，而间隙锁是前开后开区间。
- **在能使用记录锁或者间隙锁就能避免幻读现象的场景下（如果操作的索引是唯一索引或主键，InnoDB 会对 Next-Key Lock 进行优化，将其降级为 Record Lock，**即仅锁住索引本身，而不是范围）next-key lock  就会退化成记录锁或间隙锁。

##### 唯一索引等值查询

- 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成「记录锁」。

- 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会退化成「间隙锁」。

  

- **对二级索引进行锁定读查询的时候，因为存在两个索引（二级索引和主键索引），所以两个索引都会加锁。**

##### 唯一索引范围查询

- 唯一索引在满足一些条件的时候，索引的 next-key lock 退化为间隙锁或者记录锁。

##### 非唯一索引等值查询

- 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁。
- 当查询的记录「不存在」时，扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。

##### 非唯一索引范围查询

- 非唯一索引范围查询，索引的 next-key lock 不会退化为间隙锁和记录锁。

##### 没有加索引的查询

因此，在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，**如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了，这是挺严重的问题。**

### update没加索引会锁全表?

InnoDB 的行锁是针对索引字段加的锁，表级锁是针对非索引字段加的锁。当我们执行 UPDATE、DELETE 语句时，如果 WHERE条件中字段没有命中唯一索引或者索引失效的话，就会导致扫描全表对表中的所有行记录进行加锁

- 执行update语句，对记录加x锁，其他事务被阻塞，只有等事务结束才能释放。
- 在 InnoDB 事务中，对记录加锁带基本单位是 next-key 锁，但是会因为一些条件会退化成间隙锁或记录锁。锁是加在索引上的而非行上。
- update 语句的 where 条件没有使用索引，就会全表扫描，于是就会对所有记录加上 next-key 锁（记录锁 + 间隙锁），相当于把整个表锁住了。
- 即使使用索引，也要看看这条语句在执行过程种，优化器最终选择的是索引扫描，还是全表扫描，如果走了全表扫描，就会对全表的记录加锁了。
- **Innodb 源码里面在扫描记录的时候，都是针对索引项这个单位去加锁的， update 不带索引就是全表扫描，也就是表里的索引项都加锁，相当于锁了整张表，所以大家误以为加了表锁。**

- 将 MySQL 里的 sql_safe_updates 参数设置为 1，开启安全更新模式。

### MySQL 记录锁+间隙锁可以防止删除操作而导致的幻读吗？

可以

- 在 MySQL 的可重复读隔离级别下，针对当前读的语句会对索引加记录锁+间隙锁，这样可以避免其他事务执行增、删、改时导致幻读的问题。
- 有一点要注意的是，在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了，这是挺严重的问题。

### MySQL死锁了，怎么办

#### 为什么会产生死锁

- 行锁的释放时机是**在事务提交（commit）后，**锁就会被释放，并不是一条语句执行完就释放行锁。

- 当我们执行插入语句时，**会在插入间隙上获取插入意向锁，而插入意向锁与间隙锁是冲突的，**所以**当其它事务持有该间隙的间隙锁时，需要等待其它事务释放间隙锁之后，才能获取到插入意向锁。**而间隙锁与间隙锁之间是兼容的，所以所以两个事务中 select ... for update 语句并不会相互影响。

  ![image-20241008145025065](D:\2024\Notes\Typora\八股\MySQL.assets\image-20241008145025065.png)

- 对于范围为 (1006, +∞] 的 next-key lock，两个事务是可以同时持有的，不会冲突。因为 +∞ 并不是一个真实的记录，自然就不需要考虑 X 型与 S 型关系。

- 插入意向锁名字虽然有意向锁，但是它并不是意向锁，它是一种特殊的间隙锁。不同于间隙锁的是，该锁只用于并发插入操作。

#### Insert语句是怎么加行级锁的？

Insert 语句在**正常执行时是不会生成锁结构的**，它是**靠聚簇索引记录自带的 trx_id 隐藏列来作为隐式锁来保护记录的。**

隐式锁：

- 事务需要加锁的时候，如果这个锁不会发生冲突，就跳过加锁环节，这个过程称为隐式锁。
- 只有在特殊情况下，才会将隐式锁转换为显示锁。
- 隐式锁是 InnoDB 实现的一种延迟加锁机制，其特点是只有在可能发生冲突时才加锁，从而减少了锁的数量，提高了系统整体性能。

##### 1. 记录之间有间隙锁

每插入一条新记录，都需要看一下待插入记录的下一条记录上是否已经被加了间隙锁，**如果已加间隙锁，此时会生成一个插入意向锁，**然后锁的状态设置为等待状态

##### 2. 遇到唯一键冲突

如果在插入新记录时，插入了一个与「已有的记录的主键或者唯一二级索引列值相同」的记录（不过可以有多条记录的唯一二级索引列的值同时为NULL，这里不考虑这种情况），此时插入就会失败，然后对于这条记录加上了 S 型的锁。

- 如果主键索引重复，插入新记录的事务会给已存在的主键值重复的聚簇索引记录添加 **S 型记录锁。**

- 如果唯一二级索引重复，插入新记录的事务都会给已存在的二级索引列值重复的二级索引记录添加 **S 型 next-key 锁**

  并发多个事务的时候，第一个事务插入的记录，并不会加锁，而是会用隐式锁保护唯一二级索引的记录。
  但是当第一个事务还未提交的时候，有其他事务插入了与第一个事务相同的记录，第二个事务就会被阻塞，因为此时第一事务插入的记录中的隐式锁会变为显示锁且类型是 X 型的记录锁，而第二个事务是想对该记录加上 S 型的 next-key 锁，X 型与 S 型的锁是冲突的，所以导致第二个事务会等待，直到第一个事务提交后，释放了锁。

#### 如何避免死锁？

在数据库层面，有两种策略通过「打破循环等待条件」来解除死锁状态：

- 设置**事务等待锁的超时时间**。当一个事务的等待时间超过该值后，就对这个事务进行回滚
- 开启**主动死锁检测**。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行

## 日志篇

### MySQL 日志：undo log回滚日志、redo log重做日志、binlog归档日志 有什么用？

- undo log（回滚日志）：是 Innodb 存储引擎层生成的日志，**实现了事务中的原子性，主要用于事务回滚和 MVCC。**
- redo log（重做日志）：是 Innodb 存储引擎层生成的日志，实现了事务中的**持久性**，**主要用于掉电等故障恢复；**
- binlog （归档日志）：是 **Server 层生成的日志，主要用于数据备份和主从复制；**
- relay log 中继日志，用于主从复制场景下，slave通过io线程拷贝master的bin log后本地生成的日志
- 慢查询日志，用于记录执行时间过长的sql，需要设置阈值后手动开启

#### 为什么需要undo log

1. MySQL 会隐式开启事务来执行“增删改”语句的，执行完就自动提交事务的。执行一条语句是否自动提交事务，是由 autocommit 参数决定的，默认是开启。所以，执行一条 update 语句也是会使用事务的。
2. 如果 MySQL 发生了崩溃，要怎么回滚到事务之前的数据呢？实现这一机制就是 undo log（回滚日志），它保证了事务的 ACID 特性 (opens new window)中的原子性（Atomicity）。
3. **在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面**，当事务回滚时，可以利用 undo log 来进行回滚。

在发生回滚时，就读取 undo log 里的数据，然后做原先相反操作。比如当 delete 一条记录时，undo log 中会把记录中的内容都记下来，然后执行回滚操作的时候，就读取 undo log 里的数据，然后进行 insert 操作

1. 在插入一条记录时，要把这条**记录的主键值记下来**，这样之后回滚时只需要把这个主键值对应的记录删掉就好了；
2. 在删除一条记录时，要把**这条记录中的内容都记下来**，这样之后回滚时再把由这些内容组成的记录插入到表中就好了；
3. 在更新一条记录时，要把**被更新的列的旧值记下**，这样之后回滚时再把这些列更新为旧值就好了

undolog的两大作用

- **实现事务回滚，保障事务的原子性。**事务处理过程中，如果出现了错误或者用户执行了 ROLLBACK 语句，MySQL 可以利用 undo log 中的历史数据将数据恢复到事务开始之前的状态。
- **实现 MVCC（多版本并发控制）关键因素之一**。MVCC 是通过 ReadView + undo log 实现的。undo log 为每条记录保存多份历史数据，MySQL 在执行快照读（普通 select 语句）的时候，会根据事务的 Read View 里的信息，顺着 undo log 的版本链找到满足其可见性的记录。

undo log 是如何刷盘（持久化到磁盘）的？

- **undo log 和数据页的刷盘策略是一样的，都需要通过 redo log 保证持久化。**
- **buffer pool 中有 undo 页，对 undo 页的修改也都会记录到 redo log。redo log 会每秒刷盘，提交事务时也会刷盘，数据页和 undo 页都是靠这个机制保证持久化的。**

#### 为什么需要buffer pool

- 当读取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接读取 Buffer Pool 中的数据，否则再去磁盘中读取。
- 当修改数据时，如果数据存在于 Buffer Pool 中，那直接修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘

- **Buffer Pool 除了缓存「索引页」和「数据页」，还包括了 Undo 页，插入缓存、自适应哈希索引、锁信息**

### redo log怎么保证持久性的？

Redo log是MySQL中用于保证持久性的重要机制之一。它通过以下方式来保证持久性：

1. Write-ahead logging（WAL）：**在事务提交之前，将事务所做的修改操作记录到redo log中，然后再将数据写入磁盘。这样即使在数据写入磁盘之前发生了宕机，系统可以通过redo log中的记录来恢复数据。**
2. Redo log的顺序写入：**redo log采用追加写入的方式，将redo日志记录追加到文件末尾，而不是随机写入。**这样可以减少磁盘的随机I/O操作，提高写入性能。
3. Checkpoint机制：**MySQL会定期将内存中的数据刷新到磁盘，同时将最新的LSN（Log Sequence Number）记录到磁盘中，这个LSN可以确保redo log中的操作是按顺序执行的。**在恢复数据时，系统会根据LSN来确定从哪个位置开始应用redo lo

#### 有了undolog为什么需要redo log

- Buffer Pool 是基于内存的，而内存总是不可靠，**万一断电重启，还没来得及落盘的脏页数据就会丢失。**

- 当有一条记录**需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，**这个时候更新就算完成了。
  - **redolog是物理日志，每执行一个事务就会产生这样的一条或多条物理日志。在事务提交时，先将 redo log 持久化到磁盘即可**，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。
  
- **InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 WAL （Write-Ahead Logging）技术。**WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上。

- **开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。在内存修改该 Undo 页面后，需要记录对应的 redo log。**

- redo log 记录了此次事务「完成后」的数据状态，记录的是更新之后的值；
  undo log 记录了此次事务「开始前」的数据状态，记录的是更新之前的值；

  事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务。

##### 为什么不直接数据写磁盘

- 写入 redo log 的方式**使用了追加操作， 所以磁盘操作是顺序写；写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是随机写。磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小**。

##### redolog作用

- **实现事务的持久性，让 MySQL 有 crash-safe 的能力，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；**
- **将写操作从「随机写」变成了「顺序写」，提升 MySQL 写入磁盘的性能。**

##### redolog也不是直接写入磁盘

​	redo log 也有自己的缓存—— redo log buffer，每当产生一条 redo log 时，先写入到 redo log buffer，后续持久化到磁盘。

##### redolog什么时候刷盘

- **MySQL正常关闭**

- 记录**写入量大于**redologbuffer的一半，触发落盘

- **后台线程每隔一秒**进行持久化

- **每次事务提交时**都将缓存在 redo log buffer 里的 redo log **直接持久化到磁盘**（这个策略由 innodb_flush_log_at_trx_commit 参数控制）

  - 0 将 redo log 留在 redo log buffer 中 ，该模式下在事务提交时不会主动触发写入磁盘的操作。

    后台线程每隔一秒：会把缓存在 redo log buffer 中的 redo log ，通过调用 write() 写到操作系统的 Page Cache，然后调用 fsync() 持久化到磁盘。所以参数为 0 的策略，MySQL 进程的崩溃会导致上一秒钟所有事务数据的丢失;

  - 1 每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘，这样可以保证 MySQL 异常重启之后数据不会丢失

  - 2 每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，即只写到了操作系统的文件缓存

    后台线程每隔一秒：调用 fsync，将缓存在操作系统中 Page Cache 里的 redo log 持久化到磁盘。所以参数为 2 的策略，较取值为 0 情况下更安全，因为 MySQL 进程的崩溃并不会丢失数据，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。

  - 数据安全性：参数 1 > 参数 2 > 参数 0
    写入性能：参数 0 > 参数 2> 参数 1

##### redolog写满了怎么办

- InnoDB 存储引擎有 1 个**重做日志文件组( redo log Group）**，「重做日志文件组」由有 2 个 redo log 文件组成，这两个 redo 日志的文件名叫 ：ib_logfile0 和 ib_logfile1。**重做日志文件组是以循环写的方式工作的，从头开始写，写到末尾就又回到开头，相当于一个环形。**
- 如果 write pos 追上了 checkpoint，就意味着 redo log 文件满了，**这时 MySQL 不能再执行新的更新操作，也就是说 MySQL 会被阻塞（**因此所以针对并发量大的系统，适当设置 redo log 的文件大小非常重要），此时会停下来**将 Buffer Pool 中的脏页刷新到磁盘中，然后标记 redo log 哪些记录可以被擦除，接着对旧的 redo log 记录进行擦除，等擦除完旧记录腾出了空间，checkpoint 就会往后移动（图中顺时针），**然后 MySQL 恢复正常运行，继续执行新的更新操作。
- **一次 checkpoint 的过程就是脏页刷新到磁盘中变成干净页，然后标记 redo log 哪些记录可以被覆盖的过程**

#### 为什么需要bin log

- MySQL 在**完成一条更新操作后，Server 层还会生成一条 binlog，等之后事务提交的时候，会将该事物执行过程中产生的所有 binlog 统一写 入 binlog 文件。**
- binlog 文件是**记录了所有数据库表结构变更和表数据修改的日志**，不会记录查询类的操作，比如 SELECT 和 SHOW 操作。

##### redolog和binlog有什么区别

​	最开始 MySQL 里并没有 InnoDB 引擎，MySQL 自带的引擎是 MyISAM，但是 MyISAM 没有 crash-safe 的能力，binlog 日志只能用于归档。而 InnoDB 是另一个公司以插件形式引入 MySQL 的，既然只依靠 binlog 是没有 crash-safe 能力的，所以 InnoDB 使用 redo log 来实现 crash-safe 能力

1. 使用对象不同

   1. binlog——MySQL的server层实现的日志，所有的存储引擎都可以使用
   2. redolog——InnoDB存储引擎实现的日志

2. 文件格式不同

   1. binlog

      1. STATEMENT

         **每一条修改数据的 SQL 都会被记录到 binlog 中**（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。

         但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致；

      2. ROW

         **记录行数据最终被修改成什么样了**（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。

         但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已；

      3. **MIXED**

         **包含statement和row，自动选择使用哪种**

   2. redolog 物理日志

3. 写入方式不同

   1. binlog——**追加写，写满一个文件创建一个新的，不会覆盖之前的**
   2. redolog——循环写，日志空间大小固定，全部写满从头开始，保存未被刷入磁盘的脏页日志

4. 用途不同

   1. binlog**——备份复制，主从复制**
   2. redolog——**用于掉电等故障恢复**

   将数据库的数据删除了，不可以用redolog恢复，只能用binlog，因为redolog记录的是未被写入磁盘的数据的物理日志，边写边擦除。

##### 主从复制是怎么实现的

- MySQL 的主从复制依赖于 binlog ，记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。
- **这个过程一般是异步的，**也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。
- MySQL主从复制过程三个阶段
  - 写入 Binlog：**主库写 binlog 日志，提交事务，并更新本地存储数据。**
  - 同步 Binlog：**把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。**
  - 回放 Binlog：**回放 binlog，并更新存储引擎中的数据。**
- 可以在写数据时只写主库，读数据时只读从库，即使写请求会锁表或者锁记录，也不会影响读请求的执行。
- 具体详细过程如下：
  - MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。
  - 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。
  - 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。
    在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行


从库是不是越多越好？

- **库数量增加-》从库连接的io线程增多-》主库也要创建同样多的logdump来处理复制的要求-》对主库消耗资源多，同时受到主库网络带宽限制**
- **一主二从一备主，**这就是一主多从的MySQL集群结构

MySQL主从复制模型

1. 同步复制
2. 异步复制（默认）：这种**模式一旦主库宕机，数据就会发生丢失。**
3. 半同步复制：**只要一部分复制成功响应回来就行**，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端

##### binlog什么时候刷盘

- 事务执行过程中，先把日志写到 binlog cache（Server 层的 cache），**事务提交的时候，**再把 binlog cache 写到 binlog 文件中。

- **binlog 是不能被拆开的，因此无论这个事务有多大（比如有很多条语句），也要保证一次性写入**。这是因为**有一个线程只能同时有一个事务在执行的设定，所以每当执行一个 begin/start transaction 的时候，就会默认提交上一个事务，这样如果一个事务的 binlog 被拆开的时候，在备库执行就会被当做多个事务分段自**行，这样破坏了原子性，是有问题的。

- 什么时候binlogcache写入binlog文件?

  **在事务提交的时候**。虽然每个线程有自己 binlog cache，但是最终都写到同一个 binlog 文件

  1. write，将binlogcache写入**binlog文件，存在文件系统的pagecache中**
  2. fsync**，数据持久化到磁盘**

  MySQL提供一个 sync_binlog 参数来控制数据库的 binlog 刷到磁盘上的频率，大小是write几个进行fsync的个数

#### 为什么需要两阶段提交

事务提交，剩下的就是「两阶段提交」的事情了

- **事务提交后，redo log 和 binlog 都要持久化到磁盘**，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致，造成主从环境的数据不一致性。redo log 影响主库的数据，binlog 影响从库的数据，所以 **redo log 和 binlog 必须保持一致才能保证主从数据一致。**
- MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决，两阶段提交其实是**分布式事务一致性协议，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态。**
- 两阶段提交把单个事务的提交拆分成了 2 个阶段
  - 分别是「准备（Prepare）阶段」和「提交（Commit）阶段」
  - 每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。
  - 注意，不要把提交（Commit）阶段和 commit 语句混淆了，**commit 语句执行的时候，会包含提交（Commit）阶段。**

##### 两阶段提交的过程是什么样的

- 为了保证这两个日志的一致性，MySQL 使用了内部 XA 事务。内部 XA 事务由 binlog 作为协调者，存储引擎是参与者。
- 事务的提交过程有两个阶段，就是将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog，具体如下：
  - prepare 阶段：**将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）**
  - commit 阶段**：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用）**，接着调用引擎的提交事务接口，**将 redo log 状态设置为 commit，**此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功；

##### 异常重启会出现什么现象

- 在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID
  - 如果 binlog 中没有当前内部 XA 事务的 XID，**说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务。对应时刻 A 崩溃恢复的情况。**
  - 如果 binlog 中有当前内部 XA 事务的 XID，**说明 redolog 和 binlog 都已经完成了刷盘，则提交事务。对应时刻 B 崩溃恢复的情况。**

##### 两阶段提交有什么问题

- 磁盘 I/O 次数高：对于“双1”配置，**每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。**

  - 当 sync_binlog = 1 的时候，表示每次提交事务都会将 binlog cache 里的 binlog 直接持久到磁盘；
  - 当 innodb_flush_log_at_trx_commit = 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘；

- 锁竞争激烈**：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。**

  - 早期：通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。

  - 组提交：MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数。

    - prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：

      - flush 阶段：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；

        flush 阶段队列的作用是用于支撑 redo log 的组提交。

      - sync 阶段：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；

        sync 阶段队列的作用是用于支持 binlog 的组提交。

      - commit 阶段：各个事务按顺序做 InnoDB commit 操作；

        commit 阶段队列的作用是承接 sync 阶段的事务，完成最后的引擎提交

      上面的每个阶段都有一个队列，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。

  - MySQL 5.6 没有 redo log 组提交，MySQL 5.7 有 redo log 组提交。

    - 在 prepare 阶段不再让事务各自执行 redo log 刷盘操作，而是推迟到组提交的 flush 阶段，也就是说 prepare 阶段融合在了 flush 阶段。
    - 这个优化是将 redo log 的刷盘延迟到了 flush 阶段之中，sync 阶段之前。通过延迟写 redo log 的方式，为 redolog 做了一次组写入，这样 binlog 和 redo log 都进行了优化。

#### MySQL磁盘IO很高，有什么优化的办法？

如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redolog 刷盘的时机，从而降低磁盘 I/O 的频率：

- 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数**，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数**

  这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。

- 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000）

  表示每次提交事务都 write，**但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。**

- 将 innodb_flush_log_at_trx_commit 设置为 2**。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件**

  注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据

## 内存篇

### 揭开 Buffer Pool 的面纱

#### 为什么要有BufferPool

- Innodb 存储引擎设计了一个缓冲池（Buffer Pool），来提高数据库的读写性能。
- - 当读取数据时，**如果数据存在于  Buffer Pool 中，客户端就会直接读取  Buffer Pool 中的数据，否则再去磁盘中读取。**
  - 当修改数据时，**首先是修改  Buffer Pool  中数据所在的页，然后将其页设置为脏页，最后由后台线程将脏页写入到磁盘。**
- Buffer Pool 是在 MySQL 启动的时候，向操作系统申请的一片连续的内存空间，默认配置下 Buffer Pool 只有 128MB 。
- 在 MySQL 启动的时候，InnoDB 会为 Buffer Pool 申请一片连续的内存空间，然后按照默认的16KB的大小划分出一个个的页， Buffer Pool 中的页就叫做缓存页。此时这些缓存页都是空闲的，之后随着程序的运行，才会有磁盘上的页被缓存到 Buffer Pool 中。
- BufferPool除了缓存索引页和数据页，还包括了undo页、插入缓存、自适应哈希索引、锁信息等等。
- 为了更好的管理这些在 Buffer Pool 中的缓存页，I**nnoDB 为每一个缓存页都创建了一个控制块，控制块信息包括「缓存页的表空间、页号、缓存页地址、链表节点」等等。**

#### 如何管理BufferPool

如何管理空闲页

- free链表，把空闲缓存页的控制块作为链表的节点，这个链表称为free链表。**Free 链表上除了有控制块，还有一个头节点，该头节点包含链表的头节点地址，尾节点地址，以及当前链表中节点的数量等信息。**

如何管理脏页

- 设计 Buffer Pool 除了能提高读性能，还能提高写性能**，也就是更新数据的时候，不需要每次都要写入磁盘，而是将 Buffer Pool 对应的缓存页标记为脏页，然后再由后台线程将脏页写入到磁盘。**
- 链表的结点也都是控制块，区别在于都是脏页

如何提高缓存命中率

LRU算法

- 预读失效
  - **MySQL 是这样做的，它改进了 LRU 算法，将 LRU 划分了 2 个区域：old 区域 和 young 区域。**
- BufferPool污染
  - MySQL 是这样做的，**进入到 young 区域条件增加了一个停留在 old 区域的时间判断。**
  - 另外，MySQL 针对 young 区域其实做了一个优化**，为了防止 young 区域节点频繁移动到头部。young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会。**

脏页什么时候会被刷新到磁盘

- InnoDB 的更新操作采用的是 Write Ahead Log 策略，即先写日志，再写入磁盘，通过 redo log 日志让 MySQL 拥有了崩溃恢复能力。
- 触发脏页刷新
  - redo log**日志满了**
  - bufferPoo**l空间不足**
  - MySQL空闲，**后台线程定期**将适量脏页刷新到磁盘
  - MySQL**正常关闭**之前，把所有的脏页刷入到磁盘
- 在我们开启了慢 SQL 监控后，如果你发现**「偶尔」会出现一些用时稍长的 SQL**，这可能是因为脏页在刷新到磁盘时可能会给数据库带来性能开销，导致数据库操作抖动。
  - 如果间断出现这种现象**，就需要调大 Buffer Pool 空间或 redo log 日志的大小** 

## 数据库优化

1. 读写分离解决数据库读并发问题
2. 分库分表解决数据库存储问题

### 读写分离

#### 什么是读写分离

- 读写分离主要是为了将对数据库的读写操作分散到不同的数据库节点上。
- 一般情况下，我们都会选择一主多从，也就是一台主数据库负责写，其他的从数据库负责读。主库和从库之间会进行数据同步，以保证从库中数据的准确性。这样的架构实现起来比较简单，并且也符合系统的写少读多的特点

#### 如何实现读写分离

- 部署多台数据库，选择其中的一台作为主数据库，其他的一台或者多台作为从数据库。
- 保证主数据库和从数据库之间的数据是实时同步的，这个过程也就是我们常说的主从复制。
- 系统将写请求交给主数据库处理，读请求交给从数据库处理。

常用方式：

1. 代理方式

   我们可以在应用和数据中间加了一个代理层。应用程序所有的数据请求都交给代理层处理，代理层负责分离读写请求，将它们路由到对应的数据库中。

   提供类似功能的中间件有 MySQL Router（官方， MySQL Proxy 的替代方案）、Atlas（基于 MySQL Proxy）、MaxScale、MyCat。

2. 组件方式

   在这种方式中，我们可以通过引入第三方组件来帮助我们读写请求。

   如果你要采用这种方式的话，推荐使用 sharding-jdbc ，直接引入 jar 包即可使用，

#### 主从复制原理是什么

- **MySQL binlog(binary log 即二进制日志文件) 主要记录了 MySQL 数据库中数据的所有变化(数据库执行的所有 DDL 和 DML 语句)**。因此，我们根据主库的 MySQL binlog 日志就能够将主库的数据同步到从库中。
- 过程
  - **主库将数据库中数据的变化写入到 binlog**
  - **从库连接主库**
  - **从库会创建一个 I/O 线程向主库请求更新的 binlog**
  - **主库会创建一个 binlog dump 线程来发送 binlog ，从库中的 I/O 线程负责接收**
  - **从库的 I/O 线程将接收的 binlog 写入到 relay log 中。**
  - **从库的 SQL 线程读取 relay log 同步数据到本地（也就是再执行一遍 SQL ）**。

#### 如何避免主从延迟

1. **强制将读请求路由到主库处理**
2. **延迟读取**

#### 什么情况下会出现主从延迟？如何尽量减少主从延迟

MySQL 主从同步延时是指从库的数据落后于主库的数据，这种情况可能由以下两个原因造成：

- **从库 I/O 线程接收 binlog 的速度跟不上主库写入 binlog 的速度**
- **从库 SQL 线程执行 relay log 的速度跟不上从库 I/O 线程接收 binlog 的速度**

与主从同步有关的时间点主要有 3 个：

1. 主库执行完一个事务，写入 binlog，将这个时刻记为 T1；
2. 从库 I/O 线程接收到 binlog 并写入 relay log 的时刻记为 T2；
3. 从库 SQL 线程读取 relay log 同步数据本地的时刻记为 T3。

- T2 和 T1 的差值反映了从库 I/O 线程的性能和网络传输的效率，这个差值越小说明从库 I/O 线程的性能和网络传输效率越高。
- T3 和 T2 的差值反映了从库 SQL 线程执行的速度，这个差值越小，说明从库 SQL 线程执行速度越快

出现主从延迟的情况：

1. **从库机器性能比主库差**
2. **从库处理的读请求过多**
3. **大事务**
4. **从库太多**
5. **网络延迟**
6. **单线程复制**
7. **复制模式**

### 分库分表

#### 什么是分库

分库 就是将数据库中的数据分散到不同的数据库上，可以垂直分库，也可以水平分库。

- 垂直分库 就是把单一数据库按照业务进行划分，不同的业务使用不同的数据库，进而将一个数据库的压力分担到多个数据库。
- 水平分库 是把同一个表按一定规则拆分到不同的数据库中，每个库可以位于不同的服务器上，这样就实现了水平扩展，解决了单表的存储和性能瓶颈的问题。

#### 什么是分表

分表 就是对单表的数据进行拆分，可以是垂直拆分，也可以是水平拆分。

- 垂直分表 是对数据表列的拆分，把一张列比较多的表拆分为多张表。
- 水平分表 是对数据表行的拆分，把一张行比较多的表拆分为多张表，可以解决单一表数据量过大的问题。
  - 水平拆分只能解决单表数据量大的问题，为了提升性能，我们通常会选择将拆分后的多张表放在不同的数据库中。也就是说，水平分表通常和水平分库同时出现。

#### 什么情况下需要进行分库分表

- **单表的数据达到千万级别以上，数据库读写速度比较缓慢。**
- 数据库中的**数据占用的空间越来越大**，备份时间越来越长。
- 应用的**并发量太大**（应该优先考虑其他性能优化方法，而非分库分表）。

不过，分库分表的成本太高，如非必要尽量不要采用。而且，并不一定是单表千万级数据量就要分表，毕竟每张表包含的字段不同，它们在不错的性能下能够存放的数据量也不同，还是要具体情况具体分析

#### 常见的分片算法有哪些

- **哈希分片**：求指定分片键的哈希，然后根据哈希值确定数据应被放置在哪个表中。哈希分片比较适合随机读写的场景，不太适合经常需要范围查询的场景。哈希分片可以使每个表的数据分布相对均匀，但对动态伸缩（例如新增一个表或者库）不友好。
- **范围分片**：按照特定的范围区间（比如时间区间、ID 区间）来分配数据，比如 将 id 为 1~299999 的记录分到第一个表， 300000~599999 的分到第二个表。范围分片适合需要经常进行范围查找且数据分布均匀的场景，不太适合随机读写的场景（数据未被分散，容易出现热点数据的问题）。
- **映射表分片：**使用一个单独的表（称为映射表）来存储分片键和分片位置的对应关系。映射表分片策略可以支持任何类型的分片算法，如哈希分片、范围分片等。映射表分片策略是可以灵活地调整分片规则，不需要修改应用程序代码或重新分布数据。不过，这种方式需要维护额外的表，还增加了查询的开销和复杂度。
- **一致性哈希分片：**将哈希空间组织成一个环形结构，将分片键和节点（数据库或表）都映射到这个环上，然后根据顺时针的规则确定数据或请求应该分配到哪个节点上，解决了传统哈希对动态伸缩不友好的问题。
- **地理位置分片：**很多 NewSQL 数据库都支持地理位置分片算法，也就是根据地理位置（如城市、地域）来分配数据。
- **融合算法分片：**灵活组合多种分片算法，比如将哈希分片和范围分片组合

#### 分片键如何选择

分片键（Sharding Key）是数据分片的关键字段

- **具有共性，即能够覆盖绝大多数的查询场景，**尽量减少单次查询所涉及的分片数量，降低数据库压力；
  具有**离散性，即能够将数据均匀地分散到各个分片上**，避免数据倾斜和热点问题；
  具有**稳定性，即分片键的值不会发生变化，**避免数据迁移和一致性问题；
  具有**扩展性，即能够支持分片的动态增加和减少**，避免数据重新分片的开销

#### 分库分表会带来什么问题

引入分库分表之后，会给系统带来什么挑战呢？

- join 操作：**同一个数据库中的表分布在了不同的数据库中，导致无法使用 join 操作。这样就导致我们需要手动进行数据的封装，**比如你在一个数据库中查询到一个数据之后，再根据这个数据去另外一个数据库中找对应的数据。不过，很多大厂的资深 DBA 都是建议尽量不要使用 join 操作**。因为 join 的效率低，并且会对分库分表造成影响。对于需要用到 join 操作的地方，**可以采用多次查询业务层进行数据组装的方法。不过，这种方法需要考虑业务上多次查询的事务性的容忍度。
- 事务问题：**同一个数据库中的表分布在了不同的数据库中，如果单个操作涉及到多个数据库，那么数据库自带的事务就无法满足我们的要求了。**这个时候，我们就需要引入分布式事务了。关于分布式事务常见解决方案总结，网站上也有对应的总结：https://javaguide.cn/distributed-system/distributed-transaction.html 。
- 分布式 ID：**分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。我们如何为不同的数据节点生成全局唯一主键呢？这个时候，我们就需要为我们的系统引入分布式 ID 了。**关于分布式 ID 的详细介绍&实现方案总结，可以看我写的这篇文章：分布式 ID 介绍&实现方案总结。
- 跨库聚合查询问题：**分库分表会导致常规聚合查询操作，如 group by，order by 等变得异常复杂。这是因为这些操作需要在多个分片上进行数据汇总和排序，而不是在单个数据库上进行。**为了实现这些操作，需要编写复杂的业务代码，或者使用中间件来协调分片间的通信和数据传输。这样会增加开发和维护的成本，以及影响查询的性能和可扩展性。
- ……

#### 分库分表有没有什么比较推荐的方案？

Apache ShardingSphere 是一款分布式的数据库生态系统， 可以将任意数据库转换为分布式数据库，并通过数据分片、弹性伸缩、加密等能力对原有数据库进行增强。

现在很多公司都是用的类似于 TiDB 这种分布式关系型数据库，不需要我们手动进行分库分表（数据库层面已经帮我们做了），也不需要解决手动分库分表引入的各种问题，直接一步到位，内置很多实用的功能（如无感扩容和缩容、冷热存储分离）！如果公司条件允许的话，个人也是比较推荐这种方式

#### 分库分表后，数据怎么迁移

- 比较简单同时也是非常常用的方案就是停机迁移
- 如果你不想停机迁移数据的话，也可以考虑双写方案
  - 为了确保新库数据的最新性，在对老库进行增删改操作时，需要同步双写到新库；在迁移过程中，需通过脚本比对老库和新库的数据，将老库中缺失的新库数据插入，并清除新库中的冗余数据；重复比对和调整，直至两库数据完全一致。

### 数据冷热分离详解

数据冷热分离是指根据数据的访问频率和业务重要性，将数据分为冷数据和热数据，冷数据一般存储在存储在低成本、低性能的介质中，热数据高性能存储介质中

#### 冷数据和热数据

1. 时间维度区分
2. 访问频率区分

#### 冷热分离优缺点

1. 优点：
   1. 热数据的**查询性能**得到优化（用户的绝大部分操作体验会更好）
   2. **节约成本（**可以冷热数据的不同存储需求，选择对应的数据库类型和硬件配置，比如将热数据放在 SSD 上，将冷数据放在 HDD 上）
2. 缺点：
   1. 系统**复杂性和风险增加**（需要分离冷热数据，数据错误的风险增加）
   2. **统计效率低**（统计的时候可能需要用到冷库的数据）。

#### 冷数据如何迁移

1. 业务层代码实现：当有对数据进行写操作时，触发冷热分离的逻辑，判断数据是冷数据还是热数据，冷数据就入冷库，热数据就入热库。这种方案会影响性能且冷热数据的判断逻辑不太好确定，还需要修改业务层代码，因此一般不会使用。
2. 任务调度：可以利用 xxl-job 或者其他分布式任务调度平台定时去扫描数据库，找出满足冷数据条件的数据，然后批量地将其复制到冷库中，并从热库中删除。这种方法修改的代码非常少，非常适合按照时间区分冷热数据的场景。
3. 监听数据库的变更日志 binlog ：将满足冷数据条件的数据从 binlog 中提取出来，然后复制到冷库中，并从热库中删除。这种方法可以不用修改代码，但不适合按照时间维度区分冷热数据的场景。

#### 冷数据如何存储

存储要求：容量大，成本低，可靠性高，访问速度可以适当牺牲。

1. 中小厂：直接使用 MySQL/PostgreSQL 即可（不改变数据库选型和项目当前使用的数据库保持一致），比如新增一张表来存储某个业务的冷数据或者使用单独的冷库来存放冷数据（涉及跨库查询，增加了系统复杂性和维护难度）
2. 大厂：Hbase（常用）、RocksDB、Doris、Cassandra

### 常见SQL优化手段总结

#### 1. 避免使用SELECT*

1. 消耗更多的 CPU：使用 SELECT * 会导致数据库引擎额外消耗 CPU 资源，因为需要解析每个列的数据类型和值。
2. 增加网络带宽资源消耗：无用字段会增加数据传输时间，特别是大字段（如 varchar、blob、text）。
3. 无法使用 MySQL 优化器覆盖索引的优化：“覆盖索引”策略是速度极快、效率极高的查询优化方式，但 SELECT * 无法利用这一策略。
4. 减少表结构变更带来的影响：明确列出所需字段可以降低因表结构更改导致的影响。

使用 SELECT <字段列表> 更具优势，它能更高效地利用资源，提高查询性能，并降低表结构变化带来的影响。

#### 2. 深度分页优化

**查询偏移量过大的场景我们称为深度分页，这**会导致查询性能较低，例如：

```sql
# MySQL 在无法利用索引的情况下跳过1000000条记录后，再获取10条记录
SELECT * FROM t_order ORDER BY id LIMIT 1000000, 10
```

优化建议：

1. **范围查询**（在保证ID连续性时，使用id范围进行分页）

   ```sql
   # 查询指定 ID 范围的数据
   SELECT * FROM t_order WHERE id > 100000 AND id <= 100010 ORDER BY id
   # 也可以通过记录上次查询结果的最后一条记录的ID进行下一页的查询：
   SELECT * FROM t_order WHERE id > 100000 LIMIT 10
   ```

   1. ID 连续性要求高: 实际项目中，数据库自增 ID 往往因为各种原因（例如删除数据、事务回滚等）导致 ID 不连续，难以保证连续性。
   2. 排序问题: 如果查询需要按照其他字段（例如创建时间、更新时间等）排序，而不是按照 ID 排序，那么这种方法就不再适用。
   3. 并发场景: 在高并发场景下，单纯依赖记录上次查询的最后一条记录的 ID 进行分页，容易出现数据重复或遗漏的问题。

2. **子查询**

   我们先查询出 limit 第一个参数对应的主键值，再根据这个主键值再去过滤并 limit，这样效率会更快一些

   ```sql
   # 通过子查询来获取 id 的起始值，把 limit 1000000 的条件转移到子查询
   SELECT * FROM t_order WHERE id >= (SELECT id FROM t_order limit 1000000, 1) LIMIT 10;
   ```

   1. 不过，子查询的结果会产生一张新表，会影响性能，应该尽量避免大量使用子查询。并且，这种方法只适用于 ID 是正序的。在复杂分页场景，往往需要通过过滤条件，筛选到符合条件的 ID，此时的 ID 是离散且不连续的。
   2. 当然，我们也可以利用子查询先去获取目标分页的 ID 集合，然后再根据 ID 集合获取内容，但这种写法非常繁琐，不如使用 INNER JOIN 延迟关联。

3. **延迟关联**

   延迟关联与子查询的优化思路类似，都是通过将 LIMIT 操作转移到主键索引树上，减少回表次数。相比直接使用子查询，延迟关联通过 INNER JOIN 将子查询结果集成到主查询中，避免了子查询可能产生的临时表。在执行 INNER JOIN 时，MySQL 优化器能够利用索引进行高效的连接操作（如索引扫描或其他优化策略），因此在深度分页场景下，性能通常优于直接使用子查询。

   1. 子查询 (SELECT id FROM t_order LIMIT 1000000, 10) 利用主键索引快速定位目标分页的 10 条记录的 ID。
   2. 通过 INNER JOIN 将子查询结果与主表 t_order 关联，获取完整的记录数据。

4. 覆盖索引

   通过索引直接获取所需字段，避免回表操作，减少 IO 开销，适合查询特定字段的场景。但当结果集较大时，MySQL 可能会选择全表扫描。

#### 3. 尽量避免多表join

- 超过三个表禁止join，需要join的字段，数据类型保持绝对一致；多表关联查询时，保证被关联的字段需要有索引。
- join的效率比较低，主要是因为其使用嵌套循环（Nested Loop）来实现关联查询
- 避免多表join常见的做法：
  - 单表查询后在内存中自己做关联
    - 性能好
    - 拆分后的单表查询代码可复用性高
    - 单表查询更利于后续维护
  - 数据冗余

#### 4. 建议不要使用外键与级联

学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。**如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。**外键与级联更新适用于单机低并发，不适合分布式、高并发集群群集；级联更新是强阻塞，存在数据库更新风暴的风险；外键影响数据库的插入速度。

#### 5. 选择合适的字段类型

1. 某些字符串可以转换成数字类型存储 比如可以将IP地址转换成整型数据
   1. INET_ATON 把ip转为无符号整型
   2. INET_NTOA 把整型的ip转为地址
2. 对于非负型的数据，优先用无符号整数存储
3. 小数值类型优先使用TINYINT类型
4. 对于日期类型，不要用字符串，考虑DATIETIME、TIMESTAMP和数值型时间戳
5. 金额字段用decimal，避免精度丢失
6. 尽量使用自增id作为主键
7. 不建议用NULL作为列默认值

#### 6. 尽量用UNION ALL代替UNION

- UNION会把两个结果集的所有数据放到临时表中后在进行去重操作，更耗时，更消耗CPU资源
- UNION ALL不会再对结果集进行去重操作，获取到的数据包含重复项

#### 7. 批量操作

对于数据库中的数据更新，如果能使用批量操作就要尽量使用，减少请求数据库的次数，提高性能

#### 8. Show Profile分析SQL执行性能

为了更精准地定位一条 SQL 语句的性能问题，需要清楚地知道这条 SQL 语句运行时消耗了多少系统资源。`SHOW PROFILE` 和 `SHOW PROFILES` 显示 SQL 语句的资源使用情况，显示的消息包括 CPU 的使用，CPU 上下文切换，IO 等待，内存使用等。（已经被删除）

#### 9. 优化慢SQL

- 找到执行速度较慢的SQL语句——MySQL 慢查询日志是用来记录 MySQL 在执行命令中，响应时间超过预设阈值的 SQL 语句。

  出于性能层面的考虑，慢查询日志功能默认是关闭的，你可以通过以下命令开启：

  ```sql
  # 开启慢查询日志功能
  SET GLOBAL slow_query_log = 'ON';
  # 慢查询日志存放位置
  SET GLOBAL slow_query_log_file = '/var/lib/mysql/ranking-list-slow.log';
  # 无论是否超时，未被索引的记录也会记录下来
  SET GLOBAL log_queries_not_using_indexes = 'ON';
  # 慢查询阈值（秒），SQL 执行超过这个阈值将被记录在日志中
  SET SESSION long_query_time = 1;
  # 慢查询仅记录扫描行数大于此参数的 SQL
  SET SESSION min_examined_row_limit = 100;
  ```

  设置成功之后，使用 show variables like 'slow%'; 命令进行查看。

- 这里对日志中的一些信息进行说明：

  - Time：被日志记录的代码在服务器上的运行时间。
  - User@Host：谁执行的这段代码。
  - Query_time：这段代码运行时长。
  - Lock_time：执行这段代码时，锁定了多久。
  - Rows_sent：慢查询返回的记录。
  - Rows_examined：慢查询扫描过的行数。

  实际项目中，慢查询日志通常会比较复杂，我们需要借助一些工具对其进行分析。像 MySQL 内置的 `mysqldumpslow` 工具就可以把相同的 SQL 归为一类，并统计出类项的执行次数和每次执行的耗时等一系列对应的情况。

- 找到了慢 SQL 之后，可以通过 `EXPLAIN` 命令分析对应的 `SELECT` 语句。

  * 比较重要的字段说明：

    * select_type：查询的类型，常用的取值有 SIMPLE（普通查询，即没有联合查询、子查询）、PRIMARY（主查询）、UNION（UNION 中后面的部分）、SUBQUERY（子查询）等。
    * table：表示查询涉及的表或衍生表。
    * type：执行方式，判断查询是否高效的重要参考指标，结果值从差到好依次是：ALL < index < range ~ index_merge < ref < eq_ref < const < system。
    * rows：SQL 要查找到结果集需要扫描读取的数据行数，原则上 rows 越少越好。
    * ...

##### 如何优化慢SQL

给你张表，发现查询速度很慢，你有那些解决方案

1. 分析查询语句：使**用EXPLAIN命令分析SQL执行计划，找出慢查询的原因，**比如是否使用了全表扫描，是否存在索引未被利用的情况等，并根据相应情况对索引进行适当修改。
2. 创建或优化索引：**根据查询条件创建合适的索引，**特别是经常用于WHERE子句的字段、Orderby 排序的字段、Join 连表查询的字典、 group by的字段，并且如果查询中经常涉及多个字段，考虑创建联合索引，使用联合索引要符合最左匹配原则，不然会索引失效
3. **避免索引失效：**比如不要用左模糊匹配、函数计算、表达式计算等等。
4. 查询优化：避免使用SELECT *，只查询真正需要的列；使用覆盖索引，即索引包含所有查询的字段；联表查询最好要以小表驱动大表，并且被驱动表的字段要有索引，当然最好通过冗余字段的设计，避免联表查询。
5. **分页优化：**针对 limit n,y 深分页的查询优化，可以把Limit查询转换成某个位置的查询：select * from tb_sku where id>20000 limit 10，该方案适用于主键自增的表，
6. 优化数据库表：如果单表的数据超过了千万级别，考虑是否需要将大表拆分为小表，减轻单个表的查询压力。也可以将字段多的表分解成多个表，有些字段使用频率高，有些低，数据量大时，会由于使用频率低的存在而变慢，可以考虑分开。
7. 使用缓存技术：引入缓存层，如Redis，存储热点数据和频繁查询的结果，但是要考虑缓存一致性的问题，对于读请求会选择旁路缓存策略，对于写请求会选择先更新 db，再删除缓存的策略。




#### 10.正确使用索引

1. 选择合适的字段创建索引
   1. 不为null的字段
   2. 被频繁查询的字段
   3. 被作为条件查询的字段
   4. 频繁需要排序的字段
   5. 被频繁用于连接的字段
2. 被频繁更新的字段应该慎重建立索引
3. 尽可能地考虑建立联合索引而不是单列索引
4. 注意避免荣誉索引
5. 考虑在字符串类型的字段上使用前缀索引代替普通索引
6. 避免索引失效
7. 删除长期未使用的索引







