# MySQL

## MySQL：基础

### MySQL存储引擎架构

MySQL存储引擎采用的是插件式架构，存储引擎是基于表的，而不是数据库

### MySQL存储引擎-innoDB MyISAM MEMORY

解析:

存储引擎：InnoDB，MyISAM，MEMORY等

参考口述回答:

在这里，我主要对比两种最常用的存储引擎:InnoDB和MyISAM，同时简要提及其他几种引擎的特点。

1.InnoDB存储引擎:

**事务支持**：InnoDB是事务型数据库的首选引擎，它提供了提交、回滚和崩溃恢复能力来保护用户数据，确保数据的完整性。它遵循ACID原则，即原子性、一致性、隔离性和持久性。

**行级锁定**：InnoDB支持行级锁定，这意味着在多个用户并发访问数据库时，它只会锁定被访问的行，而不是整个表。这大大提高了数据库的并发性能。

**外键约束**：InnoDB还支持外键约束，可以确保数据的参照完整性。

**聚簇索引**：InnoDB使用聚簇索引，即数据和主键索引存储在一起，这有助于提高某些查询的效率。

2.MyISAM存储引擎:

**表级锁定**：MyISAM不支持事务和行级锁定，只支持表级锁。这意味着在读写操作时，会对整个表进行锁定，可能导致并发性能较差。

**全文索引**：MyISAM支持全文索引，适合进行文本搜索。

**高速读取**：MyISAM通常用于只读或大量读取的应用场景，如Web站点的归档数据或只读的数据仓库。其查询速度相对较快，

**压缩存储**：MyISAM还支持压缩表，可以节省存储空间。

3.其他存储引擎:

**Memory：**将所有数据保存在RAM中，提供极快的访问速度，适用于需要快速查找引用的场景。但服务器关闭时数据会丢失。

总的来说，选择哪种存储引擎取决于具体的应用需求和性能要求。例如，对于需要高并发写入和数据完整性的应用，InnoDB是更好的选择而对于只读或文本搜索为主的应用，MyISAM可能更合适。在实际应用中，我们需要根据具体情况进行权衡和选择。

### 一条sqL执行流程

解析

连接器->查询缓存->解析SQL->执行SQL

参考口述回答

**第一阶段：连接器**

当用户尝试连接到MySQL数据库时，首先会经过连接器。连接器的主要任务是验证客户端的身份，例如检查用户名和密码是否正确。如果验证通过，连接器还会到权限表中查询该用户所拥有的权限。连接成功后用户的会话信息会被缓存起来，这样在后续的连接请求中，如果连接参数相同，就可以重用之前的会话信息，避免重复进行身份验证。

**第二阶段：缓存**
在MySQL中，为了提高查询效率，有一个查询缓存的机制。当接收到个查询请求时，MySQL会先检查查询缓存，看看是否之前已经执行过相同的查询。如果是，那么MySQL会直接从缓存中获取结果并返回给客户端，从而跳过后续的解析和执行阶段。但需要注意的是，查询缓存的维护也是有一定开销的，特别是在数据变更频繁的场景下，查询缓存可能会导致性能下降，因为每次数据变更都会导致相关的查询缓存失效。

**第三阶段：解析SQL**
如果查询缓存中没有找到匹配的结果，MySQL就需要对SQL语句进行解析。解析器会先对SQL语句进行词法和语法分析，将其转换成一个“解析树”。这个过程中会检查SQL语句的语法是否正确，如果不正确，就会返回错误信息给客户端。

**第四阶段：预处理与优化**
解析完SQL语句后，MySQL会对其进行预处理和优化。预处理主要是检查解析树中的表和列是否存在，数据类型是否正确等。优化器则负责根据解析树生成一个高效的执行计划。优化器会考虑多种可能的执行路径，并选择其中成本最低的一种。这个成本是基于对数据的统计信息来计算的，比如表的大小、索引的分布情况等。

**第五阶段：执行SQL**
根据优化器生成的执行计划，MySQL开始执行SQL语句。如果是查询操作，MySQL会按照执行计划中的步骤逐步检索数据，并最终返回给客户端。如果是更新操作(如INSERT、UPDATE或DELETE)，MySQL会先对数据进行修改，并可能触发相关的约束和触发器。在这个过程中，MySQL还需要维护数据的完整性和一致性。

**第六阶段：返回结果**
最后，MySQL会将执行结果返回给客户端。对于查询操作，这通常是一个结果集，包含了满足条件的记录；对于更新操作，可能是一个表示操作成功或失败的状态码。

### SQL/NoSQL

SQL数据存储使用结构化存储，具有固定的行和列的表格，NoSQL是非结构化存储，比如：文档（MongoDB）、键值（Redis）、图，SQL提供ACID属性，既原子性、一致性、隔离性、持久性

### NULL/''

- NULL代表一个不确定的值
- ''的长度是0，是不占空间的，NULL需占空间
- NULL会影响聚合函数的结果，例如sum、avg、min、max会忽略null
- 查询NULL必须使用is NULL不能用=、！=、>、<

### 主键

在创建表时通过`PRIMARY KEY`关键字来设置，在表创建后通过   `ALTER TABLE table_name   ADD PRIMARY KEY (column1)`语句添加。

没有唯一性保证，主键通常与索引一起使用所以会导致性能受影响

### 联表

联表是一种数据库查询操作，它允许你将两个或多个表根据某些关联条件组合起来，以便于执行更复杂的查询。联表操作使得从不同的表中查询相关联的数据变得可能。常见的联表类型包括内连接（INNER JOIN）、左外连接（LEFT JOIN）、右外连接（RIGHT JOIN）等。

### 联表（内连接 左连接 右连接）

内连接：只返回两个表中有匹配的行，当需要获取两个表中都有对应匹配的数据时使用，例如：获取所有已经下单的客户信息

左连接：返回左表的所有行，当需要获取左表的所有记录，以及它们在右表中的匹配记录时使用，例如：获取所有客户的信息，以及他们的订单信息（如果有的话）

右连接：返回右表的所有行，即使左表中没有匹配，当需要获取右表的所有记录，以及它们在左表中的匹配记录时使用，例如，获取所有订单的信息，以及下单客户的信息（如果存在的话）

### 外连接中ON/WHERE

`ON`子句用于指定连接条件，即定义如何匹配左表和右表中的行，在外连接中，`ON`子句的条件是在连接发生之前应用的。这意味着，即使右表（在LEFT JOIN中）或左表（在RIGHT JOIN中）中没有匹配的行，左表或右表中的行仍然会出现在结果集中，未匹配的部分将以NULL填充，`WHERE`子句用于过滤结果集中的行，即在所有连接操作完成后，再对结果集应用`WHERE`子句中的条件

### 不推荐外连接

在连接大表时。如果没有适当的索引，连接操作可能会非常耗时，影响查询性能。涉及多个表和复杂的连接条件时连接查询可能会使SQL语句变得复杂，难以理解和维护。连接查询可能会产生重复的列或数据冗余。复杂的连接查询可能难以优化。

### 不推荐外键和级联

- 只适用于单机低并发，不适合分布式、高并发集群
- 对分库分表不友好

### Mysql架构

连接层：连接处理和安全验证、连接池

服务层：SQL接口、解析器、优化器、缓存

引擎层：存储引擎（InnoDB、MyISAM）

存储层：文件系统、具体来说，InnoDB存储引擎包括其自己的缓冲池用于缓存数据和索引，以及日志缓冲区用于事务日志的写入。

### 读文件/读数据库

文件：每个文件可以包含任意类型的数据，如文本、图片或二进制数据等。通常需要通过文件I/O操作，如打开文件、读写文件内容、关闭文件等。没有保护机制

数据库：数据以结构化的形式存储，表格形式。通过SQL语言来访问和操作数据。数据库设计有许多性能优化机制，如索引、查询优化器、缓存和分区等。数据库管理系统提供了如事务管理（ACID属性：原子性、一致性、隔离性、持久性）、约束（如主键、外键约束）和触发器等。提供了事务隔离级别和锁机制来管理并发访问，确保数据的一致性。

### MySQL的存储过程？游标？触发器？

`MySQL`中存储过程主要分为两类，一类是普通的存储过程，另一类则是触发器类型的存储过程

存储过程就是**将常用且复杂的`SQL`语句预先写好**，然后用一个指定名称存储起来，这个过程经过`MySQL`编译解析、执行优化后存储在数据库中，对比常规的`SQL`语句来说，普通`SQL`在执行时需要先经过编译、分析、优化等过程，最后再执行，而存储过程则不需要，一般存储过程都是预先已经编译过的

存储过程的语法

```sql
DELIMITER $

-- 创建的语法：指定名称、入参、出参
CREATE
PROCEDURE 存储过程名称 (返回类型参数名1参数类型1,....)
BEGIN
-- 具体组成存储过程的SQL语句....
-- 表示到这里为止，存储过程结束
END $

DELIMITER ;
```

procedure`/prəˈsiːdʒə(r)/`

`DELIMITER $`，表示以`$`作为结束标识

**游标**是所有数据库的存储过程中，很重要的一种特性，它可以对一个结果集中的数据按条处理，也就意味着原本查询出的数据是一个整体性质的集合，而使用游标可以对该集合中的数据逐条处理

阿里是禁止使用存储过程的

触发器本质上是一种特殊的存储过程，但存储过程需要人为手动调用，而触发器则不需要，它可以在执行某项数据操作后自动触发

CREATE TRIGGER`[ˈtrɪɡə(r)]`

### MongoDB MySQL

MongoDB：是一个基于文档的NoSQL数据库，它存储的是灵活的文档格式数据，类似JSON。这意味着在同一个集合内，每个文档可以有不同的字段和结构。

MySQL：是一个关系型数据库管理系统，它使用固定的表结构来存储数据。每个表都定义了固定的列和数据类型，所有的记录都遵循这个结构。

在MongoDB中，可以为文档的任何字段创建索引，以优化查询性能；同样，在MySQL中，也可以为表的列创建索引

### 操作 a数据库和操作 b数据库，数据库连接池是一个对象实例吗?

如果需要操作a数据库和b数据库，可以为每个数据库创建一个连接池实例，每个实例都是一个独立的对象，它们管理各自数据库的连接

### group order在哪一层做的?

执行SQL这一层。具体来说，它们是在SQL语句被解析并生成执行计划之后，实际执行查询时进行的操作。



## MySQL：索引

### *索引分类（索引类型）

解析
多个角度：数据结构，物理存储，特性，字段个数

首先，从数据结构的角度来看，MySQL的索引主要分为以下几类：
1.B+tree索引

这是最常用的索引类型，特别是在InnoDB和MyISAM存储引擎中。B+树索引能够处理全键值、键值范围和前缀查询。只有叶子节点存储value，非叶子节点只有指针和key。

2.Hash索引
主要在MEMORY存储引擎中使用，基于哈希表实现，适用于等值查询，但不适合范围查询和排序操作。

3.Full-text索引
主要用于文本搜索，支持在MyISAM和InnoDB存储引擎上创建它允许你在文本字段上进行高效的全文搜索。

接下来，从物理存储的角度来看，索引可以分为：
聚簇索引：索引结构和数据一起存放的索引，在InnoDB中，表数据实际上是按照主键的顺序存储的，InnoDB中的主键索引就属于聚簇索引。
二级索引(辅助索引)：索引结构和数据分开存放的索引，二级索引就属于非聚簇索引，在InnoDB中。二级索引的叶子节点存储的主键值，而不是实际的数据记录。

此外，从字段特性的角度，索引还可以分为：
主键索引：基于表的主键字段创建的索引，具有唯一性。
普通索引：允许在索引的列中插入重复值和空值。
唯一索引：确保索引列中的数据是唯一的，类似于主键索引，但允许有空值。

最后，从字段个数的角度，索引可以分为：
单列索引：仅包含表中的一个列。
联合索引(复合索引、组合索引)：包含表中的多个列，可以提高多个列的查询效率。

### *B+树索引结构

首先，它是一种多叉树结构，这意味着每个节点可以有多个孩子节点，不像二叉树那样每个节点只能有两个孩子。这样的设计使得树的高度降低，进而提高了检索效率，因为我们在查找数据时需要的比较次数减少了。

其次，B+树的内部节点，也就是非叶子节点，是不存储数据的，它们只存储键值和指向子节点的指针。而所有的数据都存储在叶子节点上。这样的设计让内部节点可以容纳更多的键值，进一步降低了树的高度。

再者，B+树的叶子节点之间是通过指针相连的，是以双向链表的形式相互链接的。这种结构特点让范围查询变得非常简单和高效，因为我们可以直接通过叶子节点的指针进行顺序访问。

最后，B+树还具有良好的平衡性。在插入或删除数据时，B+树会通过分裂或合并节点来保持平衡，从而确保查询性能的稳定。


### *B+树而不是B树

一、树的高度
B+树相较于B树，其树的高度更低。这是因为B+树的非叶子节点不存储数据，只存储索引，因此可以容纳更多的子节点，使得整个树的结构更加扁平化。这种设计减少了查询时需要经过的层级数，最重要是减少了磁盘I0的次数，从而提高了查询效率。相比之下，B树的每个节点都存储数据，导致节点容纳的子节点数量有限，树的高度相对较高。

二、插入删除效率
在插入和删除操作方面，B+树也表现出更高的效率。由于B+树的非叶子节点不存储数据，因此在插入或删除数据时，只需要调整索引结构，而不需要频繁地移动大量数据。这大大简化了插入和删除操作的过程，并提高了效率。相比之下，B树在插入或删除数据时可能需要更复杂的操作来保持树的平衡。

三、范围查询效率
B+树在范围查询方面具有显著优势。由于B+树的叶子节点包含所有数据，并且叶子节点之间通过指针相连，形成了一个有序链表结构。这种结构使得范围查询变得非常简单和高效，因为我们可以直接通过叶子节点的指针顺序访问范围内的数据。而在B树中，范围查询可能需要更复杂的中序遍历操作。

### 不使用Hash表（哈希表）索引

主要只因为Hash索引不支持顺序和范围查询，假如要对表中数据进行排序或范围查询，Hash效率很低

比如：select * from table where key = 'value'时非常高效

但select * from table where key > 'value'就非常低效

因为哈希是不连续的，无法通过一个哈希值来推断其他相关值的位置

### B+树层数?计算层数?B+结点占的空间?

B+树索引的层数一般是三到四层，因为B+树是一种高度平衡的数据结构，每个节点可以有很多子节点。

假设使用16KB的页大小，每个索引键占用100字节，那么每个页大约可以存储160个索引键。

- **第一层（根节点）**：
  - 根节点可以存储 160 个索引键，每个索引键指向一个子节点（页）。
  - 因此，根节点可以直接索引 160 个页。
- **第二层**：
  - 每个子节点（页）也可以存储 160 个索引键。
  - 因此，第二层可以索引 160^2 个页。
- **第三层**：
  - 每个子节点（页）也可以存储 160 个索引键。
  - 因此，第三层可以索引 160^3 个页。
- **以此类推**：
  - 第 n 层可以索引 160^n 个页。

非叶子节点的空间占用主要由存储键值和子节点指针的空间需求决定，InnoDB存储引擎的默认页大小是16KB，每个非叶子节点实际上是一个16KB大小的页，要精确计算B+树的层数和每个节点的空间占用，需要知道具体的索引键大小、页大小以及表的行数等信息。

### 红黑树、平衡二叉查找树（AVL树）、B+树

二叉查找树的性能非常依赖他的平衡程度

AVL树需要频繁旋转来保持平衡，而且每个树节点只存储一个数据，多次磁盘IO操作

红黑树只是大致平衡，且高度较高

不管平衡二叉查找树还是红黑树，都是二叉树，都会随着插入的元素增多，而导致树的高度变高，这就意味着磁盘 I/O 操作次数多，会影响整体数据查询的效率。

### B+树查询耗时稳定吗?为什么？

B+树查询的耗时相对稳定，主要原因在于B+树是一种平衡的多路搜索树，B+树的所有数据指针都存储在叶子节点中，并且叶子节点之间以链表形式相连，所有叶子节点都位于同一层，这意味着从根节点到任何叶子节点的路径长度都相同。因此，无论是查找哪个键值，所需的步骤数（或说是访问磁盘的次数）大致相同，B+树一般三层或者四层，树的高度低，查询时需要访问的节点数都差不多。B+树是为磁盘存储优化的数据结构，它通过减少磁盘访问次数来提高性能。

### 不用LSM树

一种用于存储和检索大量键值对数据的数据结构，充分利用了磁盘的顺序写性能，核心思想是将数据保存在内存中，当到一定大小后批量写入磁盘，提供了高效的写操作和批量写入能力，牺牲了一定读性能。

B+树提供了更稳定的读性能，LSM树则提供了更高效的写性能

### 二级索引?查找过程?覆盖索引?索引下推?

存储内容
在数据库中，二级索引通常存储在单独的数据结构中，如B+树。与聚簇索引不同，二级索引的叶子节点并不包含整行数据，而是存储了索引列的值和对应的主键值。这意味着，当我们通过二级索引查找数据时，我们首先找到的是与查询条件匹配的主键值，而不是直接找到完整的记录。

查找过程

1. 定位索引列的值：在二级索引中，根据查询条件中的索引列值，数据库可以快速定位到匹配的索引记录。这个过程是通过在B+树中进行搜索实现的，时间复杂度相对较低。
2. 获取主键值：一旦找到匹配的索引记录，我们可以从中获取到对应的主键值。这个主键值是连接二级索引和聚簇索引的桥梁。
3. 回表操作：有了主键值后，我们需要回到聚簇索引中进行查找，这个过程被称为“回表”。在聚簇索引中，主键值直接与数据记录相关联，因此我们可以通过主键值直接找到完整的数据记录。

覆盖索引
如果查询的列都包含在二级索引中，那么我们就可以直接使用二级索引进行查询，而无需进行回表操作。这种情况被称为覆盖索引”。覆盖索引可以显著提高查询效率，因为它减少了磁盘I/0操作和数据查找的复杂性。

索引下推
在某些情况下，数据库可以使用一种称为“索引下推”的优化技术来进一步提高查询效率。索引下推允许数据库在二级索引层面就进行部分过滤操作，从而减少需要回表的数据量。这种优化可以在某些复杂的查询条件下显著提高性能。

假设有一个包含大量数据的表 `employees`，并且在 `last_name` 列上有一个索引。我们需要查询 `last_name` 为 'Smith' 且 `age` 大于 30 的记录。

**使用索引下推前**

在没有索引下推的情况下，查询过程如下：

1. 遍历 `last_name` 索引，找到所有 `last_name` 为 'Smith' 的记录。
2. 对每条记录进行回表操作，读取完整的行数据。
3. 在 MySQL 服务器层检查 `age` 是否大于 30。
4. 返回符合条件的记录。

这种情况下，每条 `last_name` 为 'Smith' 的记录都需要进行回表操作，增加了磁盘 I/O 次数。

**使用索引下推后**

在使索引下推的情况下，查询过程如下：

1. 遍历 `last_name` 索引，找到所有 `last_name` 为 'Smith' 的记录。
2. 在存储引擎层检查 `age` 是否大于 30。
3. 只有符合条件的记录才进行回表操作，读取完整的行数据。
4. 返回符合条件的记录。

这种情况下，只有符合 `age` 大于 30 条件的记录才需要进行回表操作，减少了磁盘 I/O 次数，提高了查询效率。

### 最左匹配原则（联合索引原则）

以 `score` 和 `name` 两个字段建立联合索引：

```sql
ALTER TABLE `cus_order` ADD INDEX id_score_name(score, name);
```

一、什么是最左匹配原则
最左匹配原则是MySQL中**联合索引查询时遵循的一个重要原则**。它指的是在使用联合索引进行查询时，查询条件中必须包含索引的最左边(即第一个)字段，才能有效地利用索引进行查询优化。如果查询条件没有包含最左边的字段，那么索引将不会被使用，这可能会导致查询性能下降。

二、最左匹配原则的原理

最左匹配原则的原理基于B+树索引的结构特点。在MySQL中，索引是以B+树的形式存在的，联合索引也不例外。B+树索引是按照**从左到右的顺序进行构建**的，因此，在查询时也需要按照相同的顺序进行匹配。
当查询条件中包含了联合索引的最左字段时，数据库可以快速地定位到符合条件的记录所在的位置，从而提高查询效率。如果查询条件没有包含最左字段，那么数据库就需要进行全表扫描，这会大大降低查询性能。

![image-20240616172359369](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240616172359369.png)

1. **查询 `(a = 2)`**：
   - 查询条件包含最左字段 `a`，可以有效利用索引。
   - B+树会首先找到 `a = 2` 的分割点，然后在 `[2, 1, 5]` 和 `[2, 2, 6]` 中进行匹配。
2. **查询 `(b = 1)`**：
   - 查询条件不包含最左字段 `a`，无法有效利用索引。
   - B+树无法直接找到 `b = 1` 的位置，需要全表扫描。
3. **查询 `(a = 2 AND b = 1)`**：
   - 查询条件包含最左字段 `a`，可以有效利用索引。
   - B+树会首先找到 `a = 2` 的分割点，然后在 `[2, 1, 5]` 中找到 `b = 1` 的记录。

### 索引失效场景

1.where查询氛围过大，比如使用select *、in、between

2.在索引列上进行计算、函数、类型转换等操作

select a from table where b - 1 = 6√

select a from table where b = 6 - 1×

3.以%开头的like查询，比如like '%abc'，注意'abc%'是有效的

4.查询条件中使用or，且or的前后条件中有一个列没有索引，涉及的索引都不会被使用

5.order by，走全表索引反而更好

8.发生隐式转换

### 索引失效导致全表扫描？优化器怎么评估的？

因为数据库无法利用索引来快速定位所需的数据行，只能逐行扫描整个表来查询匹配的记录

生成多个执行计划

根据表的大小、索引的选择性、数据分布、数据页的密度来评估不同执行计划的成本

估算每个执行的成本：IO成本、CPU成本、网络成本

评估预计有多少行数据满足条件

选择最佳执行计划

### 索引实现？加快查询？

索引的实现依赖于B+树的结构。不使用索引时，数据库可能需要执行全表扫描，并且B+Tree索引可以快速数据检索和访问，索引通常按照特定的顺序存储键值，这使得它们非常适合执行排序和范围查询，**索引可以减少查询过程中的磁盘I/O操作数量**。有了索引后，数据库可以**直接通过索引定位到满足条件的记录**，而不需要扫描整个表。这大大减少了需要扫描的数据量。**索引存储在磁盘上，但由于索引的大小通常比数据表小得多**，索引可以更容易地被加载到内存中。这样，查询时可以减少磁盘I/O操作，从而提高查询速度。

### 建立索引的语法

单列索引

```sql
CREATE INDEX index_name ON table_name (column_name);
```

联合索引

```sql
CREATE INDEX index_name ON table_name (column1, column2, ...);
```

### 索引设计

等值查询，区分度高

### name、email，建立索引?需要去查询使用相同邮箱的?

（emalil，name）

```sql
SELECT email, COUNT(*) as num
FROM your_table
GROUP BY email
HAVING num > 1;
```

### 页格式？行格式？（页存储/行存储）

在InnoDB存储引擎中，数据是以页（Page）为单位存储在磁盘上的。一个页通常是数据库存储的基本单位，InnoDB的默认页大小是16KB。

行格式决定了表中行的物理存储方式。**REDUNDANT**、**COMPACT**、**DYNAMIC**、**COMPRESSED**

### 页格式有什么用（预读机制）（为什么页是16kb）

**预读机制**：16kb他的存储空间大，能够在一次预读操作中包含更多的数据块，当预读机制从磁盘读取一个 16KB 页面时，实际上可以一次性加载更多的相关数据。

较大的页面可以在一次磁盘读取中加载更多的数据，减少磁盘寻道和旋转延迟，提高读取效率。

16KB 是一个平衡空间效率和性能的选择。选择过大的页面大小可能会导致内存浪费，因为有些页面可能不会完全被使用。选择过小的页面大小可能会增加 I/O 操作的次数，并增加管理开销。16KB 是一个常见的折衷选择，能够提供较好的性能和空间效率。

现代磁盘和文件系统的块大小通常为 4KB、8KB 或 16KB。选择 16KB 页面大小可以更好地对齐磁盘块，从而减少磁盘寻道和旋转延迟

### 联合索引怎么存?回表哪步快？

每当创建一个索引，都会为该索引生成一个独立的B+树结构

对于联合索引，例如一个索引包含列`(a, b, c)`，B+树会按照这些列的顺序存储键值。在B+树中，每个键都是一个组合键，包含了这三个列的值。B+树的节点会根据组合键的顺序进行排序，首先是`a`列的值，然后是`b`列的值，最后是`c`列的值。

通常，**第一步按索引找主键是更快的**，因为非主键索引的B+树通常只包含索引列和主键列，而不包含所有的行数据。这意味着非主键索引的B+树更小，页的数量更少，因此查找速度更快。

### MySQL修改表字段会发生什么?

1. **如果该字段不是索引的一部分**：修改操作将直接在表的数据页中进行。如果修改后的数据大小不变，那么这个操作通常很快。如果数据大小发生变化，可能需要对数据页进行分裂或合并。
2. **如果该字段是索引的一部分**：除了更新表的数据页外，还需要更新包含该字段的所有索引。这意味着对应的B+树中的索引键也需要被更新。如果是主键或唯一索引的一部分，还需要检查数据的唯一性约束。
3. **如果该字段是主键字段**：由于InnoDB使用聚簇索引，主键的修改会更加复杂。它不仅需要更新主键索引的B+树，还可能导致所有依赖于该主键的二级索引进行更新，因为二级索引存储的是指向聚簇索引记录的主键值。

### B+树的新增和删除

1. **节点分裂**：如果插入新键后，某个节点的键数超过了该节点的最大容量，那么这个节点需要分裂成两个节点。分裂操作会将节点中的键分布到两个新节点中，并将一个新的键提升到父节点中以保持树的平衡。
2. **节点合并**：如果移除键后，某个节点的键数少于该节点的最小容量（通常是最大容量的一半），那么可能需要进行节点合并。合并操作可能涉及将一个节点的所有键移动到相邻节点，并可能需要从父节点中移除一个键。

### 随机 IO，顺序 IO?顺序 IO 比随机 IO 快?

**随机I/O：**随机I/O指的是访问存储介质上非连续、分散位置的数据，在随机I/O操作中，每次读写请求可能针对的是硬盘上相距甚远的不同部分。

**顺序I/O：**顺序I/O指的是按照存储介质上数据的物理或逻辑顺序进行的读写操作。在顺序I/O操作中，数据被连续地读取或写入。

在顺序I/O中，一旦磁盘头定位到正确的位置，数据可以连续不断地传输，这使得数据传输效率非常高。而随机I/O由于需要频繁地重新定位磁盘头，每次定位后只读取少量数据，导致传输效率较低。

由于叶子节点之间是顺序链接的，B+树非常适合执行范围查询和顺序访问操作

### EXPLAIN

![image-20240605205250808](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240605205250808.png)

![image-20240605205402958](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240605205402958.png)

**第一条：**

- `type` 为 `ref`，表示使用了非唯一索引扫描。
- `key` 为 `name_class_idx`，表示使用了 `name_class_idx` 索引。
- `Extra` 中的 `Using index` 表示查询使用了覆盖索引。

**第二条：**

- `type` 为 `index`，表示使用了索引全扫描。
- `key` 为 `name_class_idx`，表示使用了 `name_class_idx` 索引。
- `Extra` 中的 `Using where` 表示查询使用了 `WHERE` 子句进行过滤。
- `Extra` 中的 `Using index` 表示查询使用了覆盖索引。

**第三条：**

- `type` 为 `ref`，表示使用了非唯一索引扫描。
- `key` 为 `name_class_idx`，表示使用了 `name_class_idx` 索引。
- `Extra` 中的 `Using index` 表示查询使用了覆盖索引。

## MySQL：事务

### 事务的ACID

1.ACID介绍
事务的ACID特性是数据库管理系统在处理事务时所必须遵循的四个基本原则，它们分别是原子性、一致性、隔离性和持久性。下面我将逐一介绍这四个特性及其在实现层面是如何得到保证的。

首先是原子性。原子性意味着事务是一个不可分割的工作单位，事务中的操作要么全部完成，要么全部不完成。它确保了一组相关的数据库操作要么全部执行成功，要么在发生错误时全部回滚到事务开始前的状态。这主要通过undo log日志和回滚技术来实现。

接下来是一致性。一致性要求事务执行前后，数据库的状态必须保持一致。如果事务在执行过程中因为某些原因而中断，数据库系统必须能够恢复到事务开始前的状态，以保持数据的一致性。数据库管理系统通过一系列完整性约束、触发器和级联更新等操作来维护数据的一致性。同时，在事务结束时，系统会对数据库的状态进行检查，以确保所有的约束条件都得到满足，如果有不一致的情况，事务会被回滚。

再来说说隔离性。隔离性是指多个并发事务之间不会互相影响。为了实现隔离性，数据库管理系统采用了MVCC和锁机制来控制对数据的并发访问。通过锁定正在被访问的数据，可以确保在事务完成之前，其他事务无法修改这些数据。此外，数据库还提供了不同的隔离级别，如读未提交、读已提交、可重复读和串行化。

最后是持久性。持久性意味着一旦事务提交，其对数据库中数据的修改将是永久性的。在事务提交前，系统会先将事务的修改记录在日志中，并确保这些日志已经持久化到磁盘上。这样，即使数据库系统发生故障或重启已经提交的事务的修改也不会丢失，因为系统可以根据日志来恢复数据到一致的状态。(持久性是通过redo log(重做日志)来保证的)

2.ACID四种特性如何得到保证的的?

- 持久性是通过 redo log 来保证的
- 原子性是通过 undo log 来保证的
- 隔离性是通过 MVCC(多版本并发控制)或锁机制来保证的
- 一致性则是通过持久性+原子性+隔离性来保证

### 事务隔离级别?脏读/不可重复读/幻读?读未提交/读已提交/可重复读/幻读

1.读未提交(RU)
这是最低的隔离级别。在此级别下，一个事务可以读取另一个事务尚未提交的数据。
这种级别的问题在于，它可能导致“脏读”问题，即读取到未经确认的临时数据。
由于这个原因，这个隔离级别在实际应用中很少使用。

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240617110555668.png" alt="image-20240617110555668" style="zoom: 50%;" />

2.读已提交(RC)

在这个级别，每个查询都会在事务执行时获取当前可见的数据。
它可以避免脏读，因为只能读取已提交的数据，
但这种级别可能导致“不可重复读”问题，即同一个查询在多次执行时可能读取到不同的数据，因为其他事务可能在此期间修改了数据。

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240617110706377.png" alt="image-20240617110706377" style="zoom:50%;" />

3.可重复读(RR)
这是MySQL的默认事务隔离级别。
在这个级别下，事务在开始时“快照“当前状态，因此它看到的是一致的数据视图，在事务执行期间不会改变。这可以避免脏读和不可重复读问题。
但这种级别可能导致“幻读”问题即**当A事务读取某个范围内的记录时，另一个并发事务B插入了一些新的记录，导致事务A在再次读取该范围的记录时，会看到一些“幻影“记录**。
InnoDB通过**多版本并发控制(MVCC)**和**Next-Key-Lock**（行锁和间隙锁组合）来解决这个问题。

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240617110733131.png" alt="image-20240617110733131" style="zoom:50%;" />

幻读：

除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。

- T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id > 100 得到了 3 条记录。
- T2 时刻：事务 B 往插入一个 id= 200 的记录并提交；
- T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id > 100 for update 就会得到 4 条记录，此时也发生了幻读现象。

**要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句**，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

4.串行化(Serializable)
这是最高的隔离级别。
在这个级别下，事务是完全串行执行的，避免了脏读、不可重复读和幻读问题。
它通过在事务执行期间对数据加锁来实现，这可以防止其他事务对数据进行修改和插入。
但这种严格的控制可能会导致大量的锁竞争和性能开销。

### 设置事务隔离级别（隔离级别切换）

```sql
-- 查询 mysql 事务隔离级别(5.7+)
SELECT @@transaction_isolation;
-- 设置当前会话的事务隔离级别为提交读(`READ COMMITTED`)  
SET SESSION(当前会话)|GLOBAL(全局) TRANSACTION ISOLATION LEVEL READ COMMITTED;

```



### 多版本控制MVCC

一、MVCC的基本概念
MVCC允许多个事务同时读取同一行数据，而不会彼此阻塞，**每个事务看到的数据版本是该事务开始时的数据版本**。这意味着，如果其他事务在此期间修改了数据，正在运行的事务仍然看到的是它开始时的数据状态，从而实现了非阻塞读操作。

MVCC通过**保存数据的多个版本**来实现这一点，每个版本都与一个特定的事务相关联。

三、MVCC工作原理

MVCC的工作原理是基于数据版本的管理，**每当数据被修改时，数据库不是直接更新原始数据，而是创建一个新的数据库版本**，每个版本都与一个事务ID相关联，这个ID表示哪个事务创建了该版本，当事务进行读取操作时，数据库返回相应的数据版本。

MVCC的实现依赖于：**隐藏字段、Read View、undo log**。在内部实现中，InnoDB通过数据行的DB_TRX_ID（隐藏字段）和Read View来判断数据的可见性，如果不可见，则通过数据行的DB_ROLL_PTR（隐藏字段）找到undo log中的历史版本，每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建Read View之间已经提交的修改和该事务本身的修改。

MVCC解决了不可重复读，和一些幻读的问题

### 事务

MySQL中的事务是一组操作序列，这些操作作为一个单一的工作单元执行，要么全部完成，要么全部不完成，保证了数据库的完整性和一致性。

### 快照读/当前读

- **当前读**：读取的是当前最新的数据，但不允许写，写的时候也不允许读。
- 针对当前读（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读，因为当执行 select ... for update 语句的时候，会加上 next-key lock，如果有其他事务在 next-key lock 锁范围内插入了一条记录，那么这个插入语句就会被阻塞，无法成功插入，所以就很好了避免幻读问题
- 快照读：读写不冲突，每次读取的是快照数据，在rr级别下有可能读取的不是最新的数据，在rc级别下，快照读和当前读读取的数据是一样的，都是最新的
- 针对**快照读**（普通 select 语句），是**通过 MVCC 方式解决了幻读**，因为可重复读隔离级别下，事务执行过程中看到的数据，一直跟这个事务启动时看到的数据是一致的，即使中途有其他事务插入了一条数据，是查询不出来这条数据的，所以就很好了避免幻读问题。

## MySQL：优化

### 慢sqL

排查：监控;慢查询日志;执行计划

优化：优化索引；优化SQL；优化数据库参数；分库分表；缓存

一、慢SQL排查

1.监控数据库性能：数据库的性能指标，如CPU利用率、内存使用率和磁盘I/O等。比如，如果CPU或内存使用率过高，可能就意味着有慢SQL存在。这些指标能帮助我初步定位可能存在问题的地方。

2.查看**慢查询日志**：数据库通常会记录执行时间较长的查询，即慢查询日志。通过查看这些日志，我可以找到那些执行时间较长的SQL语句，这是定位慢SQL的直接方法。

3.使用**执行计划（EXPLAIN）**：对于疑似慢SQL的语句，我会使用数据库的执行计划工具进行分析。执行计划能展示SQL语句的执行步骤和涉及的表、索引等信息，从而帮助我找到性能瓶颈。

二、慢SQL优化

1.检查并优化索引：索引是提高查询性能的关键。我会检查现有的索引是否合理，是否能够支持SQL语句的快速执行。如果发现索引缺失或不合理，我会考虑添加、修改或删除索引。

2.改写SQL语句：有时候，慢SQL的产生是因为SQL语句本身存在问题，如查询条件不合理或表连接方式不正确。在这种情况下，我会尝试改写SQL语句，比如优化查询条件，合理使用索引，或者重构查询语句以提高性能。

3.调整数据库参数：数据库的性能也与其参数的设置有关。我会根据实际情况调整数据库的参数，如MySQL的innodb_buffer_pool_size（缓冲池大小）和max_connections（最大连接数）等，以提高性能。

4.考虑分库分表：如果数据库中的数据量过大，也可能导致SQL执行时间较长。在这种情况下，我会考虑对数据库进行分库分表，以分散数据存储和查询的压力。

5.使用缓存：对于一些热点数据，我会考虑使用缓存来提高访问速度。缓存可以将数据存储在内存中，减少数据库的访问次数，从而提高系统的性能。

### 分库分表

1.为什么要进行分库分表?

一、解决数据库连接资源不足问题
在高并发的场景下，大量请求同时访问数据库，可能会导致数据库连接资源紧张。通过分库，可以将连接分散到多个数据库上，从而有效地减轻单个数据库的连接压力。

二、缓解磁盘IO的性能瓶颈
当单库数据量过大时，磁盘IO可能成为性能瓶颈。分库可以将数据分散到不同的物理存储上，降低单个数据库的磁盘IO负载，提高整体性能。

三、解决单表数据量过大问题
随着数据的持续增长，单表的数据量可能会达到数百万、数千万甚至更多。这会导致查询效率下降，尤其是当查询条件未命中索引时，全表扫描的耗时将显著增加。分表可以将大表拆分成多个小表，减少单个表的数据量，从而提高查询效率

四、提高并发性能
分库分表能够将数据和请求分散到多个数据库和表上，从而提高系统的并发处理能力。每个数据库或表只处理部分数据，使得并发请求可以更加均匀地分布，减少了资源争用和锁冲突的可能性。

五、增强系统的容错能力
分库分表还可以提高系统的容错能力。当某个数据库或表出现故障时，其他数据库或表仍然可以继续运行，从而保证了系统的整体稳定性。此外，通过数据余和备份策略，可以进一步降低数据丢失的风险。

2.分库分表的方案?

1.垂直分库分表

- 垂直分库：根据业务的耦合性，将一个大的数据库拆分成多个小的数据库，每个数据库服务于不同的业务。这有助于将不同业务的数据进行隔离，提高系统的模块化和可维护性。
- 垂直分表：将一个大表中的某些列拆分到另一个表中。这通常用于将不经常使用的数据或大字段(如文本、图片等)拆分出去，以提高查询效率和减少IO压力。

2.水平分库分表

- 水平分库：将数据按照某个字段(如用户ID、订单ID等)的取值范围分配到不同的数据库中。这种方法可以均匀地分散数据压力，提高系统的可扩展性。
- 水平分表：将数据按照某个字段的取值范围分配到同一个数据库的不同表中。这通常用于解决单表数据量过大的问题，通过将数据分散到多个表中，可以提高查询性能和插入/更新的效率。

### 分库分表缺点

分库分表能有效缓解单机和单表带来的性能瓶颈和压力，突破网络IO、硬件资源、连接数的瓶颈，同时也带来一些问题

**分布式事务**：当更新内容同时存在于不同库，不可避免会带来跨库事务问题，可以使用**两阶段提交**来解决，但是推后了提交事务的时间点，延长了事务的执行时间，导致事务在访问共享资源时发生冲突或死锁的概率增高。

**查询问题：**跨节点多库进行查询时，会出现limit分页、order by 排序等问题。分页需要按照指定字段进行排序，当排序字段就是分页字段时，通过分片规则就比较容易定位到指定的分片；当排序字段非分片字段时，就变得比较复杂

### 分库分表数据迁移?

1. **双写方案**：在迁移期间，新旧系统同时写入，直到新系统数据完全同步且稳定后，切换到新系统。
2. **增量同步**：通过binlog或其他日志工具，实时同步旧系统到新系统的数据变更。
3. **数据校验**：迁移过程中要定期进行数据校验，确保数据的一致性。
4. **业务逐步切换**：可以先将部分不太重要的业务切换到新系统，逐步扩大范围，直到全部切换完成。
5. **回滚计划**：确保有完善的回滚计划，以应对迁移过程中可能出现的问题。

### 分布式事务的解决方案？

**两阶段提交（2PC）**

协调者向所有参与节点发送“**准备提交**”的请求，询问它们是否可以成功地执行事务操作。

如果**所有参与者**都发送了“**准备好**”的响应，协调者会向所有参与者发送“提交”请求

### 跨节点join，分表怎么做联查?

- **应用层join**：在应用层通过多次查询不同的表或数据库，然后在内存中进行数据合并。
- **中间件支持**：使用支持分库分表的中间件，如ShardingSphere、MyCAT、TDDL等，这些中间件提供了跨库join的能力

### 数据库深度分页问题，offset有什么问题

使用`OFFSET`进行分页时，数据库需要扫描从第一条记录开始到`OFFSET + LIMIT`指定的记录，然后丢弃前`OFFSET`条结果，只返回之后的`LIMIT`条记录。当`OFFSET`值很大时，数据库需要处理大量的数据，但实际上只返回其中很小一部分，这导致了大量的资源浪费。

解决办法：

1.使用唯一标识符（如主键ID）进行分页，而不是使用`OFFSET`。在每次查询时，记录上一次查询返回的最后一条记录的ID，然后在下一次查询时，使用这个ID作为起点，查询下一批数据。这种方法避免了扫描大量不需要的记录，提高了查询效率。

2.在某些场景下，可以考虑限制用户访问的最大分页深度，避免深度分页的性能问题。例如，提供“加载更多”而不是传统的分页导航，或者限制用户只能访问前几百页

### 一个慢SQL，如何去优化，select*from test where a=1 and b='北京市%' order by c desc 其中b字段含义是详细地址，大小是varchar(500);给了explain表格，看表格发现查询语句没有用索引，且进行了文件排序

- 首先，确保`a`字段和`b`字段上有合适的索引。由于`b`字段使用了前缀匹配（`'北京市%'`），可以考虑对`b`字段创建前缀索引

  ```sql
  CREATE INDEX idx_b_prefix ON test(b(20));
  ```

  这条语句创建了一个名为`idx_b_prefix`的索引，它只包含列`b`的前20个字符。

- 如果查询中经常同时包含`a`和`b`字段的条件，可以考虑创建一个包含`a`和`b`字段的复合索引。复合索引的顺序应根据字段在WHERE子句中的出现频率和过滤效果来决定

- 指定具体需要查询的字段，而不是使用`SELECT *`

- 可以在索引中包含排序字段，以便利用索引进行排序，避免文件排序。在这个例子中，如果经常需要按`c`字段降序排序，可以考虑将`c`字段也包含在复合索引中

## MySQL：锁

### MySQL锁？(全局锁/行锁/表锁/记录所/临键锁/间隙锁)

一、**全局锁**
全局锁是对整个数据库实例加锁，通常在执行某些特定的全局操作时需要使用，如备份、数据库迁移等。在MySQL中，可以通过执行FLUSH TABLES WITH READ LOCK 语句来获取全局锁。这种锁会阻塞其他会话对数据库的写操作，直到全局锁被释放。需要注意的是，获取全局锁会对数据库的正常运行产生影响，因此应谨使用，并在操作完成后及时释放。

二、**表级锁**
表级锁是锁定整个表，从而阻止其他用户并发访问。在MySQL中，表级锁主要包括以下几种：

1.表锁：最简单的表级锁，直接对整个表进行加锁。这种锁的优点是开销小、加锁快、无死锁，但缺点是锁定粒度大，发生锁冲突的概率最高，并发度最低。

2.元数据锁(MDL)：当对一个表结构进行修改时(例如ALTER TABLE)，MySQL会自动为该表加上元数据锁，以防止其他事务在该表上进行DDL或DML操作。这种锁主要用于保证表结构的完整性。

3.意向锁：在InnoDB存储引擎中，当事务准备在某个数据行上加共享锁或排他锁之前，会先在表上获取相应的意向锁。意向锁是兼容的，主要用于在行锁和表锁之间建立一种层级关系。

4.AUTO-INC锁：当插入操作中涉及到自增字段时，MySQL会使用AUTO-INC锁来确保自增值的唯一性。这种锁在插入完成后会立即释放。

三、**行级锁**
行级锁是锁定表中的某一行或某些行，从而允许其他事务并发访问表中的其他行。在MySQL中，行级锁主要包括以下几种：

1.**记录锁**(Record Locks)：直接锁定索引记录，常在更新操作时使用。当某个事务正在对一条记录进行修改时，其他事务无法对该记录进行修改或删除操作。

2.**间隙锁**(Gap Locks)：锁定一个范围，但不包括记录本身。这种锁主要用于防止幻读(Phantom Reads)，即在一个事务读取某个范围内的记录时，另一个事务插入新的记录到这个范围内。

3.**临键锁**(Next-Key Locks)：是记录锁和间隙锁的结合，锁定一个范围并包括记录本身。这种锁可以确保范围内的记录和间隙都被锁定，从而提供更严格的并发控制。

四、数据库中的乐观锁与悲观锁

1. **乐观锁**(Optimistic Lock)
   乐观锁的核心思想是认为在大多数情况下数据并不会发生冲突，因此在数据更新时不主动加锁，而是在提交数据更新时检查是否存在冲突。它常用于在高并发环境下减少锁的开销。
   
   实现方式：
   
   版本号机制：最常见的乐观锁实现方式是在数据表中增加一个版本号字段(version)，每次更新数据时同时更新版本号。
   
   例子：假设我们有一张用户表user，包括字段 id、name和version
   
   ```sql
   SELECT version FROM user WHERE id = 1;
   WHERE id =1 AND version = old version;
   ```
   
   在更新数据时，只有 version 字段与之前读取的一致，更新操作才会成功，否则说明数据在此期间被其他事务修改过，更新失败，需要重新读取和处理。
   
   适用场景：
   
   适合于大量读操作而较少写操作的场景，这样可以提升系统的并发性能。
   高并发环境下避免频繁加锁带来的性能开销。
   
   缺点：需要重新读写数据处理冲突，适用于读多写少的场景
   
2. **悲观锁**(Pessimistic Lock)
   悲观锁的核心思想是认为对数据的冲突会频繁发生，因此在对数据进行操作之前主动加锁，避免其他事务访问该数据。悲观锁在操作过程中会保持锁定状态，直到事务提交或回滚。

   实现方式：
   数据库中的行级锁：常见的实现方法是使用SQL语句加锁，如SELECT...FOR UPDATE
   
   例子：
   
   假设我们有一张用户表 user，包括字段 id 和 name
   
   ```sql
   BEGIN;
   SELECT * FROM user WHERE id = 1 FOR UPDATE;
   COMMIT;
   ```
   
   在执行 SELECT ...FOR UPDATE 语句时，会对返回的行加锁，其他事务在该行被锁定期间无法更新数据。
   
   适用场景

   适合于写操作频繁且数据竞争激烈的场景，可以有效防止并发写操作的冲突

   需要确保数据的强一致性，不允许丢失任何更新时。

   缺点：锁的开销较大，容易造成锁等待或死锁，适用于写多读少的场景

### MySQL-死锁排查

一、死锁的排查

1.查看错误日志：MySQL的错误日志中可能会记录有关死锁的信息。通过查看这些日志，可以初步判断是否存在死锁以及死锁发生的时间和原因。

2.使用SHOW ENGINE INNODB STATUS命令：这个命令可以提供InnoDB存储引擎的详细信息，包括最近的死锁信息。在返回的结果中，可以查找"LATEST DETECTEDDEADLOCK"部分，这里会详细描述死锁的情况，包括涉及的事务、锁定的资源等。

![image-20240716095445581](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240716095445581.png)

![](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240716095500759.png)



3.性能监控工具：使用如Percona Monitoring andManagement(PMM)、InnoDB Plugin等工具可以帮助监控和分析数据库性能，并在发生死锁时提供警报。

![image-20240716100101605](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240716100101605.png)

4.慢查询日志：如果死锁是由于某些查询执行时间过长导致的，那么慢查询日志可能会记录这些查询。通过分析慢查询日志，可以找到可能导致死锁的长时间运行的查询。

二、死锁的解决

1.调整事务隔离级别：适当降低事务的隔离级别可以减少锁的竞争，但也可能带来其他问题，如不可重复读或幻读，需要根据具体业务需求权衡利弊。

2.优化查询语句：对涉及多表操作、复杂连接或子查询的语句进行优化，以减少锁的持有时间和范围

3.控制事务大小：尽量将大事务拆分成多个小事务，以减少锁的持有时间。同时，避免在事务中执行不必要的操作。

4.设置合理的锁超时时间：通过设置合理的锁超时时间(如innodb_lock_wait timeout 参数)，可以在一定程度上避免长时间的锁等待和死锁。

5.手动介入：如果以上方法都无法解决死锁问题，或者需要立即解决已经发生的死锁，可以考虑手动终止其中一个事务来打破死锁状态。这通常需要谨慎操作，并确保不会影响到业务的正常运行。

### 行锁实现

InnoDB的行锁是通过**给索引项加锁**来实现的。这意味着只有通过索引条件来检索数据时，InnoDB才会使用行级锁。如果没有使用索引条件，InnoDB将使用表级锁。

### 怎么加锁

**唯一索引**和**主键索引**：行锁和间隙锁。

**普通索引**：临键锁（包括行锁和间隙锁）。

### 触发行锁

1. `select ... for update`
2. `update`
3. `delete`
4. `insert`
5. `replace`
6. 外键：当更新或删除具有外键约束的行时，会对相关的父表和子表的行加锁

### MySQL-锁退化

当我们用唯一索引进行等值查询的时候，查询的记录存不存在，加锁的规则也会不同：
当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会退化成「记录锁」。
当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会退化成「间隙锁」

### sql实现死锁

```sql
CREATE TABLE t_order (
id int NOT NULL AUTO_INCREMENT,
order_no int DEFAULT NULL,
create_date datetime DEFAULT NULL,
PRIMARY KEY (id),
KEY index_order (order_no) USING BTREE
) ENGINE=InnoDB ;
//order_no为普通索引
BEGIN;

-- 检查订单号为1007的记录是否存在并加锁
SELECT id FROM t_order WHERE order_no = 1007 FOR UPDATE;

-- 插入新订单
INSERT INTO t_order (order_no, create_date) VALUES (1007, NOW());

BEGIN;

-- 检查订单号为1008的记录是否存在并加锁
SELECT id FROM t_order WHERE order_no = 1008 FOR UPDATE;

-- 插入新订单
INSERT INTO t_order (order_no, create_date) VALUES (1008, NOW());

```



## MySQL：日志

### binlog|undolog|redolog日志?

首先是binlog，也就是**二进制日志**。

- 作用：binlog记录了数据库**所有的DDL(数据定义语言)和DML(数据操作语言)语句**事件，这些语句会导致数据的改变。它主要用于主从复制和数据备份。
- 格式：MySQL支持**三种格式**的binlog，分别是Statement-Based Replication(SBR)，记录SQL语句本身；Row-Based Replication(RBR)，记录行的改变；以及Mixed-Based Replication(MBR)，根据情况选择SBR或RBR
- 工作原理：当一个事务被提交时，MySQL会将该事务的所有修改操作作为一个事件写入到binlog中。并且MySQL会定期将内存中的binlog缓存刷新到磁盘上的binlog文件中。

接下来是redo log，也就是重做日志。

- 作用：redo log是InnoDB存储引擎特有的，用于保证事务的持久性和崩溃恢复。即使数据库崩溃，也可以通过redo log的回放来恢复已提交的事务。实现事务中的持久性。当数据库对数据做修改的时候，需要把数据页从磁盘读到buffer pool中，然后在buffer pool中进行修改，那么这个时候buffer pool中的数据页就与磁盘上的数据页内容不一致，称buffer pool的数据页为dirty page 脏数据，如果buffer pool缓冲池中的脏页【脏数据】还没有进行刷盘的时候，此时数据库发生crash，重启服务后，我们可以通过redo log日志找到需要重放到磁盘文件的那些数据记录
- buffer pool缓冲池中的数据直接刷新到磁盘，是一个随机IO，效率较差，而把buffer pool中的数据记录到**redo log，是一个顺序IO**，可以提高事务提交的速度
- 结构：redo log是一个循环的、固定大小的日志文件，通常由多个物理文件组成。
- 写入过程：当事务执行修改操作时，MySQL首先将修改操作记录到内存中的redo log缓冲区，然后定期将这些日志记录刷写到磁盘上的redo log文件中。

**Binlog**：在**事务提交时写入**，记录的是已完成的事务。

**Redo Log**：在**事务执行过程中实时写入**，确保即使系统崩溃也能重做已提交的事务。

最后是undo log，即回滚日志。

- 作用：undo log主要用于事务的回滚和并发控制。当事务需要回滚时，可以使用undo log中保存的旧值将数据恢复到事务开始之前的状态。此外，undolog还用于多版本并发控制(MVCC)，以提供事务的隔离性和并发访问的一致性。
- 工作原理：**对于每个事务来说，其都有一个对应的undo log**。**当我们开启事务并对表进行操作后，这些操作都会被记录到undo log中**。如果事务执行错误或需要回滚，就可以根据undo log中的信息将数据恢复到原来的状吞

### 1000w的表，MySQL修改一条字段的类型，binlog中插入的是多少数据量，一条记录还是全表1000w的数据?

1.STATEMENT格式

在STATEMENT格式下，binlog记录的是**SQL语句**本身。因此，当你修改一个字段的类型时，binlog中只会记录这条ALTER TABLE语句，而不会记录具体的数据行。

2.ROW格式

在ROW格式下，binlog记录的是每一行数据的变化。然而，**对于DDL操作**，如ALTER TABLE，MySQL在ROW格式下也只会记录操作本身，而不是每一行的变更。这是因为DDL操作通常涉及大量数据行，如果记录每一行的变更，会导致binlog文件过大，影响性能。

3.MIXED格式

MIXED格式是STATEMENT和ROW格式的结合体。MySQL会根据具体的操作选择合适的格式进行记录。对于ALTER TABLE这样的操作，通常会使用STATEMENT格式，因此binlog中只会记录ALTER TABLE语句。

### MySQL崩溃恢复（宕机）

过程包括：

1. 重做：MySQL会扫描redo log，重做所有已提交的事务修改
2. 回滚：对于崩溃时还未提交的事务，MySQL会使用undo log来回滚这些事务的修改

## MySQL：高可用

### MySQL容灾

确保数据在灾难发生时的安全和可恢复性

主从复制，半同步复制

### MySQL写请求太多（写优化）

和慢SQL优化一样

1. 优化查询和索引（答慢SQL优化）
2. 数据库参数（还是和慢SQL一样）
3. 分库分表

### 多机MySQL写的位置

像主从复制的话，一般在主节点写，从节点读

或者用负载均衡器来管理

### MySQL多机部署

主从复制，分库分表

### MySQL主从同步机制

MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。··

MySQL 集群的主从复制过程梳理成 3 个阶段：

- **写入 Binlog**：主库写 binlog 日志，提交事务，并更新本地存储数据。
- **同步 Binlog**：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志（relay log）中。
- **回放 Binlog**：回放 binlog，并更新存储引擎中的数据。

在完成主从复制之后，就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。

一般采用**半同步复制**：MySQL 5.7 版本之后增加的一种复制方式，介于两者之间，事务线程不用等待所有的从库复制成功响应，只要一部分复制成功响应回来就行，比如一主二从的集群，只要数据成功复制到任意一个从库上，主库的事务线程就可以返回给客户端。这种**半同步复制的方式，兼顾了异步复制和同步复制的优点，即使出现主库宕机，至少还有一个从库有最新的数据，不存在数据丢失的风险**。

### 主从复制/主从同步

**主从复制**应用场景：

**读写分离**：适用于读多写少的场景，将**读请求分流**到从库，提高系统性能。

**数据备份**：从库可以用作数据备份，防止主库数据丢失。

**分布式部署**：适用于跨地域的分布式系统，通过异步复制减少网络延迟的影响。

**主从同步**应用场景：

**强一致性要求**：适用于对数据**一致性要求高**的场景，如金融、支付等系统。

**高可用性**：需要快速故障恢复和高可用性的场景，确保数据一致性和系统可靠性。

**主从复制（异步）**：适用于性能要求高、读多写少的场景，提供较高的读性能和数据备份能力，但存在数据一致性和延迟问题。

**主从同步**：适用于对数据一致性要求高的场景，确保数据实时一致，但写性能较低，适用于高可用性和一致性要求高的应用。

### 主从数量配置

主从复制：1主4从、1主8从（一般是2的幂次，更好负载均衡）

主从同步：1主1备、1主2备，备库作为热备，随时可以切换为主库

### MySQL-主从延迟解决办法

主从复制延迟是指在 MySQL 主服务器（Master）上的数据修改不能及时反映到从服务器（Slave）上的现象。这会导致读取从服务器数据时的不一致性。

- 调整复制参数：**`relay_log`**，增加 Relay Log 的大小，减少从服务器读取 Relay Log 的频率
- 启用**半同步复制**，**确保主服务器提交的事务至少被一个从服务器接收到**，提升数据一致性
- 利用缓存机制（Redis）减少对从服务器的查询压力。

### 数据空洞

当对一条数据执行`delete`操作时，MySQL将数据删除后，并未将数据占用的空间返还给操作系统，而是将当前空间标记为"可复用"，当有新的数据插入时，则不会重新申请空间，而是插入到"可复用"空间中，这种"可复用"空间，称之为数据空洞

`OPTIMIZE TABLE`来整理表空间结构

# Java

## Java基础

### Java vs C++

Java 和 C++ 都是面向对象的语言，都支持封装、继承和多态

- Java 不提供指针来直接访问内存，程序内存更加安全
- Java 的类是单继承的，C++ 支持多重继承
- Java 有自动内存管理垃圾回收机制(GC)

### 基础数据类型及其大小?（byte short int long float double char boolean）

java中有8种基础数据类型，它们分别是:

1. byte：占用1字节，也就是8位，它的取值范围是-128到127。
2. short：占用2字节，即16位，可以表示的数值范围是-32768到327673
3. int：占用4字节，32位，能表示的数值范围是从负的2的31次方到正的2的31次方减1
4. long：占用8字节，64位，数据范围为负的2的63次方到正的2的63次方减1
5. float：占用4字节，32位，可以表示的数据范围在3.4e-45到1.4e38之间
6. double：占用8字节，64位，数据范围在4.9e-324到1.8e308之间
7. char：占用2字节，16位，用于存储Unicode码，可以表示一个字符
8. boolean：它只有两个取值，true和false，其大小没有明确的规定

### 基本类型/包装类型

**用途**：除了定义一些常量和局部变量之外，我们在其他地方比如方法参数、对象属性中很少会使用基本类型来定义变量。并且，包装类型可用于**泛型**，而基本类型不可以。

**存储方式**：基本数据类型的局部变量存放在 Java 虚拟机栈中的局部变量表中，基本数据类型的成员变量（未被 `static` 修饰 ）存放在 Java 虚拟机的堆中。包装类型属于对象类型，我们知道几乎所有对象实例都存在于堆中。

**占用空间**：相比于包装类型（对象类型）， 基本数据类型占用的空间往往非常小。

**默认值**：成员变量包装类型不赋值就是 `null` ，而基本类型有默认值且不是 `null`。

**比较方式**：对于基本数据类型来说，`==` 比较的是值。对于包装数据类型来说，`==` 比较的是对象的内存地址。所有整型包装类对象之间值的比较，全部使用 `equals()` 方法。

### 包装类(Byte Integer Long Character Double Float Boolean)

**`Integer`**

- 对象头：16字节
- `int`字段：4字节
- 对齐填充：4字节（确保对象大小是8字节的倍数）
- **总大小**：24字节

**`Long`**

- 对象头：16字节
- `long`字段：8字节
- **总大小**：24字节（不需要额外的对齐填充）

**`Character`**

- 对象头：16字节
- `char`字段：2字节
- 对齐填充：6字节
- **总大小**：24字节

**`Boolean`**

- 对象头：16字节
- `boolean`字段：1字节
- 对齐填充：7字节
- **总大小**：24字节

### 静态方法不能调用非静态成员

1. 静态方法是属于类的，在**类加载**的时候就会分配内存，可以通过类名直接访问。而非静态成员属于实例对象，只有在对象实例化之后才存在，需要通过类的实例对象去访问。

2. 在类的非静态成员不存在的时候静态方法就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作。

### 重载 vs 重写

重载就是**同样的一个方法**能够根据输入数据的不同，做出不同的处理

发生在同一个类中（或者父类和子类之间），方法名必须相同，参数类型不同、个数不同、顺序不同，方法返回值和访问修饰符可以不同

重写就是当**子类继承自父类**的相同方法，输入数据一样，但要做出有别于父类的响应时，你就要覆盖父类方法

方法名、参数列表必须相同

### 面向对象和面向过程的区别

- 面向过程把解决问题的过程拆成一个个**方法**，通过一个个方法的执行解决问题
- 面向对象会先抽象出**对象**，然后用对象执行方法的方式解决问题

面向过程表达 ：吃（猪八戒，西瓜）； 面向对象 ：猪八戒.吃（西瓜）

### 面向对象三大特征（多态、继承、封装）

**封装**是指把一个对象的状态信息（也就是属性）**隐藏在对象内部**，不允许外部对象直接访问对象的内部信息。

**继承**是使用**已存在的类的定义作为基础建立新类的技术**，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。

**多态**，表示一个**对象具有多种的状态**，具体表现为父类的引用指向子类的实例。编译时类型由声明该变量时使用的类型决定。
运行时类型由实际赋给该变量的对象决定。
如果**编译时类型**和**运行时类型**不一致，就可能出现所谓的多态。

父类 对象名称=new 子类（）

### 深拷贝/浅拷贝

![浅拷贝、深拷贝、引用拷贝示意图](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/shallow&deep-copy.png)

浅拷贝。浅拷贝是指仅仅**复制对象的引用**，而不是对象本身，地址是指向同一个地址。换句话说，新旧对象还是**共享同一块内存**。在浅拷贝中，如果原对象内部的子对象发生变化，这种变化也会反映到拷贝对象中，因为两者引用的是同一个子对象。实现浅拷贝，我们需要**实现Cloneable接口并重写0bject类中的clone()方法**，如果没有重写clone()方法，则会直接调用0bject的clone()方法，而Object的clone()方法是一个受保护的方法，不能直接调用。一般来说，程序默认使用的都是**浅拷贝**

深拷贝。与浅拷贝不同，深拷贝会将对象完整地拷贝出来，开辟新的内存来存放新对象，新对象与原对象完全独立，**不共享内存**，修改新对象不会影响到原对象，反之亦然。在深拷贝中，对于对象中的每一个字段，如果字段是值类型，那么对该字段执行逐位复制；如果字段是引用类型，则递归地复制该字段所引用的对象。要注意的是，实现深拷贝可能需要考虑的问题更多，比如需要避免**循环引用**的问题，否则可能会导致无限递归。用 `Serializable` 接口和 `ObjectOutputStream`、`ObjectInputStream` 可以实现深拷贝

```java
node1.next = node2;
node2.next = node1; 
//循环引用
```

### ==/equals()

**`==`** 对于基本类型和引用类型的作用效果是不同的：

- 对于基本数据类型来说，`==` 比较的是**值**。
- 对于引用数据类型来说，`==` 比较的是对象的**内存地址**。

**`equals()`** 不能用于判断基本数据类型的变量，只能用来判断两个对象是否相等，我们一般都认为`equals`是比较值而不是地址，但是实际上没有重写 `equals()`方法和`==`是一样的，还是比较地址，我们使用发现比较的是值是因为`interger`或者`string`已经被重写了

```java
String a = new String("ab"); // a 为一个引用
String b = new String("ab"); // b为另一个引用,对象的内容一样
String aa = "ab"; // 放在常量池中
String bb = "ab"; // 从常量池中查找
System.out.println(aa == bb);// true
System.out.println(a == b);// false
System.out.println(a.equals(b));// true
System.out.println(42 == 42.0);// true

```

### String不可变

Java中的String类被设计为不可变的，这意味着一旦一个String对象被创建，它的内容就不能被改变。这种设计提供了很多好处，比如安全性、可预测性和性能优化。String的不可变性主要是通过以下几个方面来实现的：

1. **私有字段**：string类内部使用**私有字符数组**来存储字符串的内容。由于这个字段是私有的，外部代码无法直接访问或修改它，从而保护了字符串内容的安全性。

2. **没有提供修改方法**：String类没有提供任何可以修改其内部状态的方法。例如，没有提供方法来改变字符串中的某个字符或添加/删除字符
3. **返回新的对象**：当对String对象进行操作(如连接、替换等)时，String类会创建一个新的String对象来保存结果，而不是修改原始对象。这样，原始String对象的状态始终保持不变。
4. **final修饰类**：为了防止子类可能通过继承来破坏String的不可变性，Java中的String类被声明为final，这意味着它不能被继承。这样，就没有子类可以重写String类的方法或添加可能破坏不可变性的新方法。

### String、StringBuffer、StringBuilder？字符串拼接

- 加号(+)拼接
  1. 性能：当使用加号进行字符串拼接时，每次拼接都会创建一个**新的string对象**，这会导致大量的中间对象创建和内存分配。如果拼接操作频繁或字符串较大，这种方式可能会带来**较大的性能开销**。
  2. 线程安全：加号拼接本身是**线程安全**的，因为它不涉及共享状态或并发操作。
  3. 使用场景：加号拼接适用于**简单的、少量的**字符串拼接操作，或者在不需要考虑性能的场合
- StringBuffer拼接
  1. 性能：StringBuffer在进行字符串拼接时，不会像加号那样每次都创建新的对象。它内部维护了一个**可变的字符序列**，通过**append()**方法可以将新的字符串添加到已有字符串的末尾，从而避免了频繁的内存分配。
  2. 线程安全：StringBuffer是**线程安全**的。大部分修改字符串内容的方法（如 `append()`, `insert()`, `delete()` 等）都通过 **`synchronized`** 关键字进行同步。锁的粒度是**方法级别**的，也导致了较大的性能损耗，因为每次操作都需要进行加锁和解锁。
  3. 使用场景：当你需要在**多线程环境下**进行字符串拼接，或者需要频繁地进行大量的字符串拼接操作时StringBuffer是一个很好的选择。
- StringBuilder拼接
  1. 性能：一个**可变的字符序列**来进行字符串拼接。然而，与StringBufer不同的是，StringBuilder没有提供同步机制，因此它在单线程环境下的性能更高。实际上，在处理大量或频繁的字符串拼接时，StringBuilder的性能通常是最优的。
  2. 线程安全：由于StringBuilder没有提供同步机制，所以它**不是线程安全**的。在多线程环境下使用StringBuilder进行字符串拼接可能会导致数据混乱或不一致的问题。
  3. 使用场景：当你只需要在单线程环境下进行字符串拼接，并且追求最高的性能时，可以选择使用StringBuilder。特别是在处理大量的、频繁的字符串拼接操作时，如日志记录、文本处理等场景StringBuilder是最佳的选择。

### String s1 = new String("abc");这句话创建了几个字符串对象？

会创建 1 或 2 个字符串对象。

1、如果字符串常量池中不存在字符串对象“abc”的引用，那么它会在堆上创建两个字符串对象，其中一个字符串对象的引用会被保存在字符串常量池中。

2、如果字符串常量池中已存在字符串对象“abc”的引用，则只会在堆中创建 1 个字符串对象“abc”。

![image-20240911162803291](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240911162803291.png)

### Java中的异常（Throwable Exception Error 异常处理方式）

![Java 异常类层次结构图](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/types-of-exceptions-in-java.png)

首先，我们来看看Java异常的分类。在Java中，所有的异常类都继承自 Throwable 类。**Throwable** 类有两个主要的子类：**Error和Exception**。
1. Error：
    Error 类表示严重问题，通常是JVM无法或不应该尝试处理的问题，例如 Java 虚拟机运行错误（`Virtual MachineError`）、虚拟机内存不够错误(`OutOfMemoryError`)、类定义错误（`NoClassDefFoundError`）等 。这些异常发生时，Java 虚拟机（JVM）一般会选择线程终止。
2. Exception：
    Exception 类是程序需要处理的异常情况。它又分为两大类：
  - **检查型异常**(Checked Exceptions)和**非检查型异常**(Unchecked Exceptions)。
  - 检查型异常：这类异常在编译时必须被处理，要么通过 try-catch 块捕获，要么在方法签名中声明抛出，要不就**无法通过编译**。例如，IOException、FileNotFoundException、classNotFoundException等
  - 非检查型异常：这类异常是运行时异常，继承自 RuntimeException 类。它们通常是由程序错误导致的，如空指针访问、数组越界等。常见的运行时异常有 NullPointerException、ArrayIndex0utOfBoundsException、ArithmeticException 等。运行时异常不需要在方法签名中声明，也应该尽量避免。

Java异常的处理方式

- try-catch块：使用 try 块包含可能引发异常的代码，并使用catch 块捕获并处理这些异常。可以在 catch 块中执行特定的错误处理逻辑，如记录日志、回滚事务或执行清理操作。
- finally块：finally 块包含的代码无论是否发生异常都会被执行，通常用于执行清理操作，如关闭文件流或数据库连接
- throw关键字：Java允许程序员**显式**地抛出一个异常，使用 throw 关键字后跟一个异常对象。这通常用于在特定条件下触发异常处理流程。
- throws关键字：在**方法签名**中使用 throws 关键字可以声明该方法可能抛出的异常类型，从而告知调用者需要处理这些异常。

处理异常时还需要注意以下几点：

1. 尽量避免使用空的 catch 块，因为它会吞异常，使得问题难以追踪和调试。
2. 在处理异常时，尽量提供有用的错误信息和堆栈跟踪，以便更好地定位问题。
3. 合理规划异常处理策略，避免过多的嵌套和复杂的逻辑结构。

### 泛型

Java中的泛型是一种机制，它允许类、接口和方法在定义时使用类型参数。

泛型一般有三种使用方式：**泛型类**、**泛型接口**、**泛型方法**。

### 异常的抛出机制

异常通过`throw`关键字抛出，并**沿着调用栈传播**，直到找到合适的`catch`块处理。

当异常在方法中抛出而没有被捕获时，异常会沿着调用栈向上传播，直到找到合适的`catch`块或者最终被JVM捕获并终止程序。

### 为什么不推荐捕获`error`

**不可恢复**：`Error`通常表示JVM本身的严重问题，如内存不足或系统资源耗尽。捕获这些错误无法让程序恢复正常运行。

**隐藏问题**：捕获`Error`可能会隐藏问题，使得程序继续运行但处于不稳定或不可预期的状态。

**复杂性**：处理这些严重错误需要深入理解底层平台和环境，而不是简单的应用程序级别的处理。

### OOM属于什么类的异常?因为什么情况导致OOM?

`OutOfMemoryError`是`Error`的子类，表示**JVM内存耗尽**，无法分配更多的对象。

`OutOfMemoryError`的异常堆栈信息会显示导致OOM的代码路径。这有助于调试和优化代码，防止类似问题再次发生

内存泄漏是指程序中分配的内存没有被正确释放，导致内存占用不断增加，最终耗尽可用内存：

- **未关闭资源**：如数据库连接、文件输入输出流、网络连接等未关闭。

- **短时间内创建了过多对象**，超出了JVM堆内存的处理能力：

- **大数据处理**：一次性加载或处理大量数据到内存中。

数据结构持续增长且没有有效的限制机制：

- **无限制的集合增长**：如`ArrayList`、`HashMap`等集合类无限制地添加元素。

### 异常/错误相同点?

**继承关系**：两者都继承自`Throwable`类。

**可以抛出和捕获**：可以使用`throw`关键字抛出，并在`try-catch`块中捕获。

**用于错误处理**：都用于处理程序中的错误和异常情况。

### java8新特性

1. Lambda 表达式
2. Stream API 

### stream操作

Stream 可以用于对集合进行各种操作，例如过滤、映射、排序、聚合等

`filter`：过滤元素

```java
Stream<String> stream = Stream.of("a", "b", "c", "d");
stream = stream.filter(s -> s.startsWith("a"));
```

`map`：映射元素

```java
stream = stream.map(String::toUpperCase);
```

`sorted`：排序

```java
stream = stream.sorted();
```

`limit`：限制结果数量

```java
stream = stream.limit(2); // 只保留前两个元素
```

`skip`：跳过前几个元素

```java
stream = stream.skip(2); // 跳过前两个元素
```

`forEach`：遍历元素

```java
stream.forEach(System.out::println);
```

### java是值传递/引用传递

Java 中的参数传递机制是**值传递**

参数传递过程都是一个值传递，都是复制一个引用，然后对复制的引用进行相应的操作

传递对象时传递的是**引用的副本**（地址），但这个副本仍然指向原来的对象。因此，**你可以通过引用副本修改对象的属性或内容**，这些修改会反映在原始对象上

**基本数据类型**：传递的是值的副本，对参数的任何更改不会影响原始值。

**对象引用**：传递的是引用的副本，通过引用可以修改对象的内部状态，但改变引用本身不会影响原始引用。

**引用传递**：方法接收的是对象的引用本身，而不是引用的副本。这样，方法内部的操作可以直接影响到外部对象

### 设计模式（单例模式 工厂模式 装饰器模式 适配器模式 代理模式）

**工厂模式**

定义一个创建对象的抽象方法，由子类决定要实例化的类。用于创建不同类型对象

定义了一个接口用于创建相关或有依赖关系的对象族，而无需明确指定具体类

**单例模式**

确保一个类最多只有一个实例，并提供一个全局访问点

**适配器模式**

适配器模式将某个类的接口转换成客户端期望的另一个接口表示，目的是消除由于接口不匹配所造成的类的兼容性问题

**装饰器模式**

动态的将新功能附加到对象上。在对象功能扩展方面，它比继承更有弹性

**代理模式**

代理模式给某一个对象提供一个代理对象，并由代理对象控制对原对象的引用。通俗的来讲代理模式就是我们生活中常见的中介

### 设计模式-单例模式（懒汉 饿汉 双重检查锁定）

**确保一个类只有一个实例**，并提供一个全局访问点来访问这个实例。单例模式适用于那些需要频繁访问的类，例如**数据库连接池、日志管理器、线程池**等

**饿汉式单例**：在**类加载时就创建实例**，**线程安全**，但如果实例占用资源大且不常用，会造成资源浪费

**懒汉式单例：**在**第一次使用**时才创建实例，但它不是线程安全的，多线程环境下可能会创建多个实例。

**双重检查锁定**：是一种优化的单例模式实现方法，用于在**多线程环境**下，确保只创建一个实例，同时避免每次访问实例时都加锁带来的性能开销

​	**第一次检查**：在进入同步代码块之前，检查实例是否已经初始化。如果已经初始化，直接返回实例，避免不必要的加锁操作。

​	**加锁**：如果第一次检查发现实例尚未创建，进入同步代码块，进行线程同步，以确保只有一个线程能够创建实例。

​	**第二次检查**：在加锁之后，再次检查实例是否已被初始化（因为其他线程可能已经在这个时刻创建了实例）。如果实例仍然未被创建，才真正进行实例化。

​	**返回实例**：实例化完成后，返回单例实例。

注意一定要用`volatile`来保证可见性

比如说线程池管理类

```java
public class ThreadPoolManager {
    private static volatile ThreadPoolManager instance;
    private ExecutorService executor;

    private ThreadPoolManager() {
        executor = Executors.newFixedThreadPool(10);
    }

    public static ThreadPoolManager getInstance() {
        if (instance == null) {
            synchronized (ThreadPoolManager.class) {
                if (instance == null) {
                    instance = new ThreadPoolManager();
                }
            }
        }
        return instance;
    }

    public void submitTask(Runnable task) {
        executor.submit(task);
    }

    public void shutdown() {
        executor.shutdown();
    }
}
```

## Java集合

### List/Set/Map

1.List(列表)

- 有序性：List中的元素按照它们被添加的顺序进行存储，这意味着元素的顺序与添加顺序相同，因此可以通过索引访问每个元素。

- 可重复性：List允许存储相同的元素多次，即可以在List中添加重复的元素，且它们可以保持各自的位置和顺序。

- 常用实现类：如**ArrayList**、**LinkedList**等，其中ArrayList基于动态数组实现，适用于查找和随机访问操作。

2.Set(集合)

- 无序性：Set中的元素是无序的，不能通过索引或位置来访问元素
- 元素唯一性：Set集合中的元素是唯一的，不会存在重复的元素。当试图添加重复元素时，新元素不会被添加进集合中。
- 常用实现类：如HashSet、LinkedHashSet和TreeSet，其中HashSet基于哈希表实现，提供了较快的插入、查询和删除操作速度。

3.Map(映射)

- 键值对存储：Map集合是双列集合，每个元素由键和值组成(键值对)。键是唯一。
  的，而值可以重复。
- 无序性：Map的键是无序的，不可以通过索引访问。
- 常用实现类：如HashMap、LinkedHashMap和TreeMap。其中，HashMap基于哈希表实现，提供了快速的键值对查找；LinkedHashMap保持了插入顺序；TreeMap则基于红黑树实现，键通过自然排序或自定义比较器进行排序。

使用场景：

- List：适用于需要按照特定顺序存储和访问元素的场景，如日志记录、任务列表等
- Set：适用于需要快速判断某个元素是否存在于集合中的场景，如用户唯一性校验、过滤重复数据等。
- Map：适用于需要根据键快速查找对应值的场景，如缓存系统、用户配置管理等综上所述

List强调有序性和元素可重复性；Set强调元素唯一性和无序性；而Map则侧重于键值对的存储和快速查找。

### ArrayList/LinkedList

- 是否保证**线程安全**： ArrayList 和 LinkedList 都是不同步的，也就是**不保证线程安全**；

- **底层数据结构**： ArrayList 底层使用的是 Object **数组**；LinkedList 底层使用的是 **双向链表** 数据结构

- 插入和删除是否受元素位置的影响：
  ArrayList 采用数组存储，所以**插入和删除元素的时间复杂度受元素位置**的影响。加到此列表的末尾，这种情况时间复杂度就是 O(1)；加到列表头也是O(n)；但是如果要在指定位置 i 插入和删除元素的话，时间复杂度就为 O(n)。因为在进行上述操作的时候集合中第 i 和第 i 个元素之后的(n-i)个元素都要执行向后位/向前移一位的操作。

  LinkedList 采用链表存储，所以**在头尾插入或者删除元素不受元素位置**的影响

  ，时间复杂度为 O(1)，如果是要在指定位置 i 插入和删除元素的话

  ，时间复杂度为 O(n) ，因为需要先移动到指定位置再插入和删除。

- 是否支持快速**随机访问**： LinkedList 不支持高效的随机元素访问，而 ArrayList（实现了 RandomAccess 接口） 支持。快速随机访问就是通过元素的序号快速获取元素对象(对应于get(int index)方法)。

- **内存空间占用**： ArrayList 的空间浪费主要体现在在 list 列表的结尾会预留一定的容量空间，而 LinkedList 的空间花费则体现在它的每一个元素都需要消耗存放直接后继和直接前驱以及数据。

- ArrayList ：从数据库中加载并展示大量商品，用户在浏览商品时，通常需要快速访问商品列表中的元素

- LinkedList：购物车中的商品可能会频繁插入或删除


### for对ArrayList删除安全?迭代器？（Iterator remove）

不安全，增强的 `for` 循环和普通的 `for` 循环直接删除元素可能导致 `ConcurrentModificationException` 或索引错误。

```java
for (String s : list)
for (int i = 0; i < list.size(); i++)
```

安全的方式：使用 `Iterator` 的 `remove` 方法删除

迭代器不会发生**索引错误**

**迭代器（Iterator）**的原理是通过提供一个统一的接口来遍历集合中的元素，而无需暴露集合的内部实现。它的核心在于**维护一个游标**，用于跟踪集合中当前的位置，以便顺序访问元素

```java
List<String> list = new ArrayList<>(Arrays.asList("a", "b", "c"));
Iterator<String> iterator = list.iterator();
while (iterator.hasNext()) {
    String s = iterator.next();
    if (s.equals("b")) {
        iterator.remove(); // 安全删除元素
    }
}
```

使用`for`倒序遍历

```java
List<String> list = new ArrayList<>(Arrays.asList("a", "b", "c"));
for (int i = list.size() - 1; i >= 0; i--) {
    if (list.get(i).equals("b")) {
        list.remove(i); // 安全删除元素
    }
}
```

这是因为 `ArrayList` 是一种基于数组实现的数据结构，删除元素会导致元素位置的移动，影响索引的准确性。

使用普通的 `for` 循环进行删除时，删除操作会导致 `ArrayList` 中的元素左移，影响后续索引的准确性，可能导致跳过元素或访问到错误的元素。

### ArrayList怎么实现线程安全?

 使用 `synchronized` 关键字：手动在访问 `ArrayList` 时进行同步控制。

```java
List<String> list = new ArrayList<>();

// 添加元素时同步
synchronized (list) {
    list.add("a");
    list.add("b");
}

// 读取元素时同步
synchronized (list) {
    for (String s : list) {
        // 迭代操作
    }
}
```

使用 `java.util.concurrent` 包下的并发集合

虽然 `ArrayList` 本身没有线程安全的变体，但 Java 提供了其他线程安全的集合，如 `ConcurrentLinkedQueue`、`ConcurrentSkipListSet` 和 `ConcurrentHashMap`，可以根据需求选择合适的集合类型。

### Vector/ArrayList

**`Vector`**：线程安全的。`Vector` 实现了 `List` 接口，主要的操作方法（如 `add()`, `remove()`, `get()`, `size()` 等）都被 `synchronized` 修饰。

`Vector` 类是 `ArrayList` 的一个线程安全的变体。所有方法都被同步，所以它是线程安全的。但是由于同步开销大，并且大部分情况下其性能不如 `ArrayList`，所以在现代 Java 编程中使用较少。

**`Vector`**：默认情况下，`Vector` 的扩展大小是当前容量的两倍，即每次扩展时容量翻倍

**`ArrayList`**：`ArrayList` 默认的扩展机制是当前容量的 50%（即每次扩展时，增加原容量的 50%）。这使得 `ArrayList` 在扩展时可能需要更多的内存重新分配，但增长更为平滑

### ArrayList扩容（多线程扩容）

ArrayList首次扩容，容量为10，不满足则扩容到到原来的1.5倍
第一次扩容后，容量还是小于minCapacity（就是我们添加的容量如15，则minCapacity=15），就将容量扩容为minCapacity
如果容量足够就直接添加，不够就需要进行扩容
扩容过程的空间复杂度是O(2n)，因为需要拷贝一个次旧的数组
ArrayList是基于动态数组实现的，在增删时候，需要数组的拷贝复制

**多线程扩容**

会导致线程安全问题。`ArrayList` 是非线程安全的集合类，它的内部操作（包括扩容）并没有加锁保护

扩容的代码位于 `ArrayList` 的 `grow()` 方法中，这个方法并没有同步锁

 `ArrayList` 在扩容时会将原数组中的元素复制到新的更大数组中，两个线程最后就会指向不同的数组

### HashSet/LinkedHashSet/TreeSet 三者的异同

- `HashSet`、`LinkedHashSet` 和 `TreeSet` 都是 `Set` 接口的实现类，都能保证元素唯一，并且都不是线程安全的。
- `HashSet`、`LinkedHashSet` 和 `TreeSet` 的主要区别在于底层数据结构不同。`HashSet` 的底层数据结构是哈希表（基于 `HashMap` 实现）。`LinkedHashSet` 的底层数据结构是链表和哈希表，元素的插入和取出顺序满足 FIFO。`TreeSet` 底层数据结构是红黑树，元素是有序的，排序的方式有自然排序和定制排序。
- 底层数据结构不同又导致这三者的应用场景不同。`HashSet` 用于不需要保证元素插入和取出顺序的场景，`LinkedHashSet` 用于保证元素的插入和取出顺序满足 FIFO 的场景，`TreeSet` 用于支持对元素自定义排序规则的场景。

### Set/Map的区别

Set底层像`HashSet`基于`HashMap`，`LinkedHashSet`是基于`linkedHashMap`实现的，当然`TreeSet`是基于`TreeMap`，区别就是，Set是唯一的，Map是键唯一，值不唯一。

### Set实现，唯一性

当你把对象加入`HashSet`时，`HashSet` 会先计算对象的`hashcode`值来判断对象加入的位置，同时也会与其他加入的对象的 `hashcode` 值作比较，如果没有相符的 `hashcode`，`HashSet` 会假设对象没有重复出现。但是如果发现有相同 `hashcode` 值的对象，这时会调用`equals()`方法来检查 `hashcode` 相等的对象是否真的相同。如果两者相同，`HashSet` 就不会让加入操作成功。

### hashtable/hashmap

HashMap继承自AbstractMap类

Hashtable继承自Dictionary类

HashMap中key和value都允许为null。key为null的键值对永远都放在以table[0]为头结点的链表中

HashTable键值对都不能为空，否则包空指针异常

HashMap在求hash值对应的位置索引时，`index = (n - 1) & hash`。将哈希表的大小固定为了2的幂，因为是取模得到索引值，**故这样取模时，不需要做除法，只需要做位运算。位运算比除法的效率要高很多。**

`Hashtable` 是线程安全的，因为 `Hashtable` 内部的方法基本都经过`synchronized` 修饰。

### HashMap底层

![jdk1.8之后的内部结构-HashMap](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/jdk1.8_hashmap.png)

数据结构：

- HashMap内部使用了一个数组加链表和红黑树的组合。数组用于存储键值对的桶，而链表和红黑树则用于解决哈希冲突。实际上底层还是哈希表，只是哈希冲突时使用链表和红黑树。
- 当发生哈希冲突时，HashMap会在数组的同一个桶中形成链表。为了提升性能，在Java 8中，当链表长度超过定阈值(默认为8)时，链表会转化为红黑树。

哈希计算与桶定位：

- HashMap使用哈希函数来计算键的哈希值，并根据这个哈希值确定键值对应该存放在数组的哪个桶中。
- 这个哈希函数被设计为尽量减少冲突，并确保键值对能够均匀分布到各个桶中。

冲突解决与数据结构转换：

- 当插入新的键值对时，如果计算出的桶已经有元素存在(即发生哈希冲突)，则新的键值对会被添加到该桶对应的链表中。
- 如果链表长度过长(超过8)，并且HashMap的数组长度大于或等于64，则链表会转换为红黑树，以提高搜索效率。

扩容机制：

- HashMap内部有一个扩容机制，当元素数量达到数组长度与加载因子(默认为0.75)的乘积时，就会触发扩容操作。
- 扩容时，HashMap的容量会翻倍，并且所有的元素都需要重新散列到新的数组中。这个过程是为了保持哈希表的均匀分布和减少冲突。

线程安全：

- HashMap并不是线程安全的。如果在多线程环境下使用HashMap，可能会导致数据不一致的问题。如果需要线程安全的哈希表，可以考虑使用ConcurrentHashMap。

### HashMap阈值8

之所以是8，是因为Java的源码贡献者在进行大量实验发现，hash碰撞发生8次的概率已经降低到了0.00000006，几乎为不可能事件

### HashMap put流程

1.初始化判断与数组准备

- 首次调用put方法时，会检查HashMap内部的数组(称为桶数组或table)是否为空或者长度为0。
- 如果是，那么会进行初始化操作，根据设定的初始容量(或默认值16)来分配数组空间，并设定加载因子(默认为0.75)。

2.计算hash值并确定桶的位置

- 对传入的key计算hash值，HashMap会结合key的hashCode()和一些额外的位操作来确保散列的均匀性。
- 使用这个hash值来确定键值对应该存放在数组的哪个位置，这通常通过高效的位操作实现。

3.处理hash冲突与数据结构选择：

- 如果计算出的桶位置已经有元素存在(即发生hash冲突)，HashMap会检查该位置的数据结构。
- 如果该位置是链表结构，新的键值对会被添加到链表的末尾。在添加前会检查链表长度：
  - 如果链表长度小于8，直接插入链表尾部
  - 如果链表长度达到8，并且当前HashMap的数组长度大于或等于64，则将链表转换。
    为红黑树，然后在红黑树中插入新的键值对。
- 如果该位置已经是红黑树结构，新的键值对会被直接插入到红黑树中，

4.扩容判断与执行

- 在插入新元素后，HashMap会检查当前元素数量(即size)是否超过了容量(数组长度)与加载因子(默认为0.75)的乘积。
- 如果超过了这个乘积(即达到了扩容阈值)，就会触发扩容操作。
- 扩容时，HashMap的容量会大致翻倍(通常是当前容量的两倍)，并且所有的元素都需要重新散列到新的数组中。

### HashMap不安全

在 `HashMap` 中，多个键值对可能会被分配到同一个桶（bucket），并以链表或红黑树的形式存储。多个线程对 `HashMap` 的 `put` 操作会导致线程不安全，具体来说会有数据覆盖的风险。

- 两个线程 1,2 同时进行 put 操作，并且发生了哈希冲突（hash 函数计算出的插入下标是相同的）。
- 不同的线程可能在不同的时间片获得 CPU 执行的机会，当前线程 1 执行完哈希冲突判断后，由于时间片耗尽挂起。线程 2 先完成了插入操作。
- 随后，线程 1 获得时间片，由于之前已经进行过 hash 碰撞的判断，所有此时会直接进行插入，这就导致线程 2 插入的数据被线程 1 覆盖了。

还有一种情况是这两个线程同时 `put` 操作导致 `size` 的值不正确，进而导致数据覆盖的问题：

1. 线程 1 执行 `if(++size > threshold)` 判断时，假设获得 `size` 的值为 10，由于时间片耗尽挂起。
2. 线程 2 也执行 `if(++size > threshold)` 判断，获得 `size` 的值也为 10，并将元素插入到该桶位中，并将 `size` 的值更新为 11。
3. 随后，线程 1 获得时间片，它也将元素放入桶位中，并将 size 的值更新为 11。
4. 线程 1、2 都执行了一次 `put` 操作，但是 `size` 的值只增加了 1，也就导致实际上只有一个元素被添加到了 `HashMap` 中

### ConcurrentHashMap实现线程安全

在**Java 7**中，ConcurrentHashMap采用了分段锁的机制来实现线程安全。"它将整个Map划分为多个**段**(Segment)，每个段都相当于一个小型的HashMap，并且有自己的锁。当线程占用锁访问其中一个段数据的时候，其他段的数据也能够被其他线程访问，从而实现真正的并发访问。简单来说，就是多个线程可以同时操作不同的段，从而实现了高效的并发性能。

然而，在Java 8中，ConcurrentHashMap的实现方式发生了显著变化。Java 8中引入了更细粒度的同步控制——使用了一种称为"**同步控制块 SCB**”的机制，也被称为**“桶锁“或"节点锁”**。这种锁机制允许多个线程在高并发的情况下，更精细地控制数据的访问和修改。

具体来说，在Java 8的ConcurrentHashMap中，每个桶(bucket)都关联一个锁(sychronized)。当线程尝试访问或修改某个桶中的数据时，它只需要获取该桶对应的锁，而不是锁定整个Map或某个大的数据段。这种设计显著减少了线程之间的锁竞争，从而提高了并发性能。此外，Java 8的ConcurrentHashMap还采用了**CAS**(非阻塞同步原语）操作来支持无锁化(lock-free)的读写，进一步提升了性能。

做插入操作时，首先进入乐观锁，
然后，在乐观锁中判断容器是否初始化，
如果没初始化则初始化容器，
如果已经初始化，则判断该hash位置的节点是否为空，如果为空，则通过CAS操作进行插入。
如果该节点不为空，再判断容器是否在扩容中，如果在扩容，则帮助其扩容。
如果没有扩容，则进行最后一步，先加锁，然后找到hash值相同的那个节点(hash冲突)，
循环判断这个节点上的链表，决定做覆盖操作还是插入操作。
循环结束，插入完毕。

### HashMap的rehash

当hash表中的负载因子达到负载极限的时候（容量*负载因子），hash表会自动成倍的增加容量（桶的数量），并将原有的对象
重新的分配并加入新的桶内，这称为rehash。这个过程是十分好性能的，一般建议设置比较大的初始化容量，防止rehash，但是也不能设置过大，初始化容量过大 浪费空间

Rehash（重新哈希）操作。Rehash的主要目的是调整HashMap的内部结构，以确保其性能和空间利用率。

1. 扩容：创建一个新的数组，其容量是原数组容量的两倍（在Java 8及以后的版本中，扩容后的容量是原容量的两倍加一，以减少哈希冲突）。
2. 重新计算哈希值：遍历原数组中的每个桶，对每个桶中的键值对重新计算哈希值，以便在新的数组中找到合适的位置。
3. 插入新数组：根据重新计算出的哈希值，将键值对插入到新的数组中对应的桶中。

### JDK1.8 HashMap扩容rehash算法优化

1. 单个节点：其实重新进行hash寻址算法，找到对应数组的下标，放上就行了

2. 链表：就是将之前的链表rehash之后重新拆分为了两个链表，一个链表rehash之后还是在当前的位置index，另一个链表rehash之后的位置变成了index + oldCap

   ![image-20240715211007032](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240715211007032.png)

3. 红黑树：拆成两个红黑树，分别挂到新的数组的位置上，只不过最后加个判断，就是判断这个红黑树是需要变成链表还是继续是红黑树

### HashMap中的迭代（hashMap使用迭代器）

HashMap的迭代分为键迭代和键值对迭代

**使用迭代器**：调用`keySet().iterator()`、`values().iterator()`或`entrySet().iterator()`来获取迭代器

### Java 8对HashMap的主要改进

1. **引入红黑树**：如前所述，当链表长度达到一定阈值时，链表会被转换为红黑树，这显著提高了在哈希冲突严重情况下的查询性能。
2. **容量调整机制**：调整了扩容的逻辑，使其在高负载下更加高效，减少了rehash的次数和开销。
3. **更强的迭代器**：虽然HashMap仍然是非线程安全的，但其迭代器采用了弱一致性迭代器，可以在一定程度上容忍并发修改，而不会抛出`ConcurrentModificationException`

### 红黑树

红黑树是一种**自平衡二叉查找树**，它具有以下性质：

- 每个节点要么是红色，要么是黑色。
- 根节点是黑色。
- 所有叶子节点（NIL节点，空节点）是黑色的。
- 如果一个节点是红色的，则它的两个子节点都是黑色的（红色节点不能相邻）。
- 从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。

### HashMap红黑树

为了优化在哈希冲突较多情况下的性能，红黑树是一种自平衡二叉查找树，提高查找效率，能够确保树的高度始终保持在对数级别，从而使得查找、插入和删除等操作的平均时间复杂度维持在O(log n)。

对于非常短的链表，直接遍历可能比树结构更高效。而当冲突增多，链表过长时，红黑树的优势就显现出来

### HashMap中key value查找?

1. **计算哈希值**：首先，HashMap会调用key的`hashCode()`方法计算其哈希值。
2. **定位桶位置**：通过上述计算得到的索引，可以直接访问到桶数组中的相应位置。这个位置称为桶（bucket），可能是空的，也可能已经有一个或多个Node（链表或红黑树的节点）。
3. **遍历桶内元素**：
   - **链表情况**：如果桶内是一个链表，HashMap会遍历这个链表，对于每个Node，它会比较Node的key与目标key是否相等（通过`equals()`方法）。如果找到相等的key，那么对应的value就被找到了。
   - **红黑树情况**：如果桶内是红黑树，HashMap会利用红黑树的特性进行快速查找。从树的根节点开始，根据key的比较结果选择左孩子或右孩子进行递归查找，直到找到匹配的key或遍历到叶子节点未找到。
4. **返回结果**：如果找到了匹配的key，就返回对应的value。

### CopyOnWriteArrayList

将读操作性能发挥到极致，`CopyOnWriteArrayList` 中的**读取操作是完全无需加锁**的。写入操作也不会阻塞读取操作，只有**写写才会互斥**。`CopyOnWriteArrayList` 线程安全的核心在于其采用了 **写时复制（Copy-On-Write）** 的策略。

当需要修改（ `add`，`set`、`remove` 等操作） `CopyOnWriteArrayList` 的内容时，不会直接修改原数组，而是会先创建底层数组的副本，对副本数组进行修改，修改完之后再将修改后的数组赋值回去，这样就可以保证写操作不会影响读操作了

也有以下缺点：

内存占用：每次写操作都需要复制一份原始数据，会占用额外的内存空间，在数据量比较大的情况下，可能会导致内存资源不足。

写操作开销：每一次写操作都需要复制一份原始数据，然后再进行修改和替换，所以写操作的开销相对较大，在写入比较频繁的场景下，性能可能会受到影响。

数据一致性问题：修改操作不会立即反映到最终结果中，还需要等待复制完成，这可能会导致一定的数据一致性问题。



## Java并发

### 线程的生命周期?线程状态？（new runnable waiting timed_waiting blocked terminated）

![Java 线程状态变迁图](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/640.png)

Java 线程在运行的生命周期中的指定时刻只可能处于下面 6 种不同状态的其中一个状态：

- NEW：初始状态，线程被创建出来但没有被调用 `start()` 。
- RUNNABLE：运行状态，线程被调用了 `start()`等待运行的状态。
- BLOCKED：阻塞状态，需要等待锁释放。
- WAITING：等待状态，表示该线程需要等待其他线程做出一些特定动作（通知或中断）。
- TIME_WAITING：超时等待状态，可以在指定的时间后自行返回而不是像 WAITING 那样一直等待。
- TERMINATED：终止状态，表示该线程已经运行完毕。

- 线程创建之后它将处于 **NEW（新建）** 状态，调用 `start()` 方法后开始运行，线程这时候处于 **READY（可运行）** 状态。可运行状态的线程获得了 CPU 时间片（timeslice）后就处于 **RUNNING（运行）** 状态。

- 当线程执行 `wait()`方法之后，线程进入 **WAITING（等待）** 状态。进入等待状态的线程需要依靠其他线程的通知才能够返回到运行状态。

- **TIMED_WAITING(超时等待)** 状态相当于在等待状态的基础上增加了超时限制，比如通过 `sleep（long millis）`方法或 `wait（long millis）`方法可以将线程置于 TIMED_WAITING 状态。当超时时间结束后，线程将会返回到 RUNNABLE 状态。

- 当线程进入 `synchronized` 方法/块或者调用 `wait` 后（被 `notify`）重新进入 `synchronized` 方法/块，但是锁被其它线程占有，这个时候线程就会进入 **BLOCKED（阻塞）** 状态。

- 线程在执行完了 `run()`方法之后将会进入到 **TERMINATED（终止）** 状态

### Thread#sleep() / Object#wait() (wait sleep)

**共同点**：两者都可以暂停线程的执行。

**区别**：

- **`sleep()` 方法没有释放锁，而 `wait()` 方法释放了锁** 。
- `wait()` 通常被用于线程间交互/通信，`sleep()`通常被用于暂停执行。
- `wait()` 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 `notify()`或者 `notifyAll()` 方法。`sleep()`方法执行完成后，线程会自动苏醒，或者也可以使用 `wait(long timeout)` 超时后线程会自动苏醒。
- `sleep()` 是 `Thread` 类的静态本地方法，`wait()` 则是 `Object` 类的本地方法

### 并发/并行

- **并发**：两个及两个以上的作业在同一 **时间段** 内执行。
- **并行**：两个及两个以上的作业在同一 **时刻** 执行。

### 线程安全，解决办法（数据竞争 原子性）

线程安全问题：

1. 数据竞争和一致性问题：
   - 当多个线程同时访问和修改同一个共享数据时，可能会导致数据的不一致性。例如，两个线程同时对一个。计数器进行增加操作，可能会因为线程调度导致计数不准确。
2. 原子性问题:
   - 某些操作需要多个步骤完成，若这些步骤被多个线程交错执行，则可能导致数据状态错误。比如，一个线程正在写入数据，而另一个线程在此期间读取该数据，可能会读取到一个不一致的中间状态。
3. 可见性问题
   - 一个线程修改了共享变量的值，但其他线程可能无法立即看到这个修改，因为它们可能缓存了旧的值。这会导致数据不一致和其他线程做出错误的决策。
4. 有席性问题:
   - 由于指令重排序或JVM优化，代码的执行顺序可能与编写顺序不同，这在多线程环境中可能导致难以预料的结果。

解决方法：

1. `synchronized`关键字：
   - 使用 `synchronized` 关键字可以确保同一时间只有一个线程能够执行某个代码块或方法，从而防止数据竞争。它保证了可见性和原子性。
2. `volatile`关键字：
   - `volatile`关键字用于声明变量，确保变量的更新对所有线程立即可见。它解决了可见性问题，但不保证原子性。
3. `Lock`接囗和`ReentrantLock`类:
   - Java提供了 Lock 接口和 ReentrantLock 类作为更灵活的线程同步机制。它们允许更精细的锁控制，包括尝试锁、定时锁、可中断锁等。
4. 原子类(Atomic Classes):
   - Java的 `java.util.concurrent.atomic` 包提供了一组原子变量类，如 `AtomicInteger`、`AtomicLong` 等。这些类的每个方法都是原子的，可以在多线程环境中无锁地安全操作。
5. 线程安全的数据结构：
   - 使用Java提供的线程安全的数据结构，如`concurrentHashMap`、`CopyOnWriteArrayList`等，可以避免显式的同步操作。
6. 避免共享状态：
   - 尽可能地减少共享状态是避免并发问题的最佳方法。例如，使用`ThreadLocal`来为每个线程提供独立的变量副本。
7. `Happens-Before`关系
   - 利用Java内存模型中的Happens-Before关系来确保多线程之间的操作顺序和可见性

### 死锁?死锁排查？避免死锁?

**什么是死锁?**

死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的种阻塞的现象。具体来说，当多个进程或线程互相等待对方释放资源时，就会导致死锁。若无外力作用，这些进程都将无法推进下去，此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。

**死锁的四个必要条件：**

1. 互斥条件：该资源任意一个时刻只由一个线程占用。
2. 请求与保持条件：一个线程因请求资源而阻塞时，对已获得的资源保持不放。
3. 不剥夺条件:线程已获得的资源在未使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。
4. 循环等待条件:若干线程之间形成一种头尾相接的循环等待资源关系。

**Java死锁后怎么排查?**

在Java中，如果怀疑出现了死锁，可以通过以下几种方法进行排查：

1. 使用`jstack`命令：首先使用`jps`查看Java进程编号，然后使用`jstack`查看进程信息。如果出现死锁，`jstack`会在最后给出进程的分析信息，表示出现了死锁。
2. 使用图形化工具`jconsole`：打开`jconsole`，选择要检测的程序，然后点击检测死锁。检测后，可以看到存在死锁的线程。
3. 使用图形化工具`jvisualvm`：在左侧选择要检测的Java程序，如果检测到死锁，右侧会出现红色的字提醒。然后可以查看堆栈信息，堆信息和`jstack`显示的差不多，但堆栈信息中间部分可能比`jstack`多出一些信息。

**如何避免死锁?**

避免死锁可以从以下几个方面入手：

1. 破坏互斥条件：这个条件比较难以破坏，因为某些资源本身就需要独占使用。不过有些资源可以通过共享的方式使用，比如只读文件。
2. 破坏请求与保持条件：可以一次性申请所有需要的资源，而不是逐步申请。或者当某个进程申请资源时，如果它之前已经占有了其他资源，那么需要先将之前占有的资源释放掉。
3. 破坏不剥夺条件：当某个进程获得部分资源而得不到其他资源时，必须放弃已获得的资源，让其他进程使用。等以后再重新申请。
4. 破坏循环等待条件：可以通过顺序申请资源来预防。即对每个设备编号，进程在申请资源时必须按照编号的顺序进行申请，不能逆序申请。另外，还可以采用避免无限等待的方法，即给每个进程设置一个最大等待时间，超过这个时间就放弃申请。

### JMM

JMM是Java内存模型。

JMM 看作是 Java 定义的并发编程相关的一组规范，定义了多线程环境下的**内存可见性和指令重排序规则**

抽象了线程和主内存之间的关系，还规定了从 Java 源代码到 CPU 可执行指令的这个转化过程要遵守哪些和并发相关的原则和规范，其主要目的是为了简化多线程编程，增强程序可移植性的。

 使用 `Happens-Before` 规则来定义操作间的顺序性。

### volatile

`volatile` 是Java中的一个关键字，主要用于确保多线程环境中变量的**可见性和有序性**。可以修饰**实例变量**、**静态变量**、**对象引用**、和 **数组引用**

一、可见性：当一个线程修改了一个 `volatile` 变量的值，其他线程能够立即看到这个修改。这是因为 `volatile` 关键字会禁止CPU缓存和编译器优化，确保每次读写都是直接**从主内存**中进行的。这样，当一个线程修改了 volatile 变量的值后，其他线程在读取这个变量时，会直接从主内存中读取最新的值，而不是从CPU缓存中读取可能过时的值

二、有序性：Java内存模型允许编译器和处理器对指令进行重排序，以提高执行效率。但是，在某些情况下，重排序可能会导致多线程环境中的问题。`volatile` 关键字通过插入内存屏障来禁止指令重排序，从而确保代码的执行顺序与程序的顺序一致。这可以防止因重排序而导致的并发问题。

**内存屏障指令**：`LOCK` 前缀确保写入操作之前和之后的内存访问操作不会被重排，避免了指令重排带来的不可见性问题

通过内存屏障（写屏障，读屏障）（`StoreStore`, `StoreLoad`, `LoadLoad`, `LoadStore` 等）确保对 `volatile` 变量的读写操作不会与其他操作发生重排序，保证代码执行的顺序与程序预期一致

需要注意的是，虽然 `volatile` 关键字可以确保变量的可见性和有序性，但它并不能确保原子性。也就是说，`volatile` 不能替代锁来确保复杂操作的原子性。在处理需要多个步骤的复合操作时，仍然需要使用锁或其他同步机制来确保操作的原子性。

### 乐观锁/悲观锁

**悲观锁**总是假设最坏的情况，认为共享资源每次被访问的时候就会出现问题(比如共享数据被修改)，所以每次在获取资源操作的时候都会上锁，这样其他线程想拿到这个资源就会阻塞直到锁被上一个持有者释放。也就是说，**共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程**，但可能会降低系统的并发性能，因为它会阻止其他事务访问被锁定的数据。

Java 中**`synchronized`**和**`ReentrantLock`**等独占锁就是悲观锁思想的实现

**乐观锁**总是假设最好的情况，认为共享资源每次被访问的时候不会出现问题，线程可以不停地执行，无需加锁也无需等待，只是在提交修改的时候去验证对应的资源（也就是数据）是否被其它线程修改了（具体方法可以使用**版本号机制**或 **CAS 算法**）。

这种方式可以提高系统的并发性能，因为它允许多个事务同时访问和修改数据，但需要在数据提交时进行冲突检测和处理。

- 悲观锁通常多用于写比较多的情况（**多写**场景，竞争激烈），这样可以避免频繁失败和重试影响性能，悲观锁的开销是固定的。不过，如果乐观锁解决了频繁失败和重试这个问题的话（比如`LongAdder`），也是可以考虑使用乐观锁的，要视实际情况而定。

- 乐观锁通常多用于写比较少的情况（**多读**场景，竞争较少），这样可以避免频繁加锁影响性能。不过，乐观锁主要针对的对象是单个共享变量（参考`java.util.concurrent.atomic`包下面的原子变量类）

### 实现乐观锁

乐观锁的实现主要依赖于两种机制：版本号机制和CAS

**版本号机制**。这种机制的核心思想是在数据中加入一个版本号，每次数据变更时，版本号都会递增。当我们要对数据进行修改时，首先会**检查数据的版本号**。如果数据的版本号与我们预期的一致，那么我们就可以进行数据修改，并将版本号递增，以此确保数据的一致性。如果版本号与我们预期的不一致，那就说明在我们读取数据后，有其他操作对数据进行了修改，这时我们就需要采取一些策略，比如重新读取数据或者放弃修改，以此避免数据冲突。

**CAS**。CAS是一种原子操作，它涉及到三个关键的值：内存中的当前值，我们预期的值，以及我们想要设置的新值。CAS操作会先比较内存中的值与预期的值是否相等，如果相等，那么就会把内存中的值替换为我们设置的新值。这个过程是原子的，也就是说，在这个操作过程中，不会有其他线程来打断。如果内存中的值与预期的值不相等，那么CAS操作就会失败，这时我们可以选择重试或者采取其他策略。

### CAS？ABA问题？
**CAS是怎么实现的（操作系统层面）？**

CAS，是一种无锁的原子操作。在操作系统层面，CAS的实现依赖于硬件支持的**原子性指令**（`Test and Set`这个指令会将一个内存位置的值设置为 1，并返回该位置的旧值。它通常用于实现锁）。这些指令能够确保在操作过程中不会被其他线程打断，从而保证操作的原子性。具体来说，CAS操作会先比较**内存中的值**与**预期的值**是否相等，如果相等，则会把内存中的值更新为新值。这个过程是原子的，也就是说，在这个操作过程中，不会有其他线程来打断。

**CAS的ABA问题是什么?**

CAS的ABA问题是一个在多线程环境下可能出现的问题。简单来说，就是当一个线程准备进行CAS操作时，它会先读取某个内存地址的值(假设为A)，然后去做一些其他事情。在这期间，另一个线程可能已经将这个内存地址的值从A改成了B，然后又改回了A。当第一个线程回来准备进行CAS操作时，它发现内存中的值还是A，就会误认为这个值没有被其他线程修改过，从而进行错误的CAS操作。这就是所谓的ABA问题。

**Java如何解决ABA问题的?**

Java为了解决CAS的ABA问题，提供了两种主要的类：`AtomicStampedReference`和`AtomicMarkableReference`。这两种类都引入了一个额外的标记或版本号来记录数据的变化。

- **`AtomicStampedReference`**使用一个标记(`stamp`)来记录对象的版本号。当对象发生变化时，**版本号**会自动增加。在进行CAS操作时，不仅需要比较对象的值，还需要比较版本号。只有在值和版本号都匹配的情况下，CAS操作才会成功。
- **`AtomicMarkableReference`**则使用一个**布尔标记**(`mark`)来表示对象的状态是否发生过改变。同样地，在进行CAS操作时，需要同时比较对象的值和标记。

这两种方式都能有效地解决CAS的ABA问题，因为它们都能检测到数据在CAS操作期间是否发生过变化，即使数据的值最终又回到了原来的状态

### 并发原子类

**Java并发原子类介绍**

Java中的并发原子类是一组用于实现线程安全的可变变量的类，这些类位于`java.util.concurrent.atomic` 包下。

1. **原子类的基本特性**：
   - 原子性：原子类提供的操作都是原子性的
   - 高效性：比通过`synchronized`和`Lock`等，更高的性能，因为是无锁(`lock-free`)
2. **常用的原子类：**
   - `AtomicInteger`、`AtomicLong`、`AtomicBoolean`
   - 还有其他如 `AtomicReference`、`AtomicstampedReference`、`AtomicMarkableReference`等用于引用类型的原子操作。
3. **原子类的实现机制：**
   - 原子类的实现大多基于**CAS**指令。CAS是一种非阻塞算法，通过比较当前变量的值和期望值是否相等来判断是否可以进行更新操作。
   - 如果相等，则执行更新操作；如果不等，则重新尝试，直到成功为止。这种方式避免了加锁带来的性能开销。
4. **使用场景：**实现计数器、状态标志等需要线程安全访问的变量。
5. **注意事项：**原子类可以确保单个操作的原子性，但是多个操作之间的组合并不一定是原子性的

### synchronized，锁升级（锁膨胀），监视器锁（synchronized修饰实例/静态方法）(可重入性)

**synchronized关键字的介绍**

`synchronized` 是Java中的一个关键字，它用于控制多线程对共享资源的访问，以防止并发问题。具体来说，它可以确保被**修饰的方法或代码块**在任意时刻只能有一个线程执行。

**synchronized的使用**

1. 修饰**实例方法**：，它锁定的是调用该方法的对象实例 (`this`)，new不同的对象，不同线程去调用不同对象是不会加锁的
2. 修饰**静态方法**：作用于当前类对象加锁，进入同步方法前要获得当前类对象的锁。也就是给`Class`加锁，不管有多少个实例，这些实例共享同一个类锁
3. 修饰代码块：指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。这种用法比较灵活，可以用于对任意代码块进行同步。

**synchronized的底层实现**

`synchronized` 关键字的底层实现依赖于Java虚拟机(JVM)中的**监视器锁(`MonitorLock`)**或称为内部锁。每个Java对象都有一个与之关联的监视器锁。当线程进入一个`synchronized` 方法或代码块时，它会自动获取该锁，而在离开时则会自动释放该锁。

`synchronized`是**可重入**的，每个锁都有一个**计数器**，当线程第一次获取锁时，计数器被设置为 1。如果同一个线程再次获取该锁（即进入同一个 `synchronized` 方法或块），计数器会递增。当线程退出 `synchronized` 块或方法时，计数器会递减。如果计数器减为 0，锁才真正释放，其他线程才能获取该锁。

**锁升级过程**

JVM对 `synchronized` 的优化包括锁升级机制，具体过程如下

1. **无锁状态**：一开始并没有任何锁。
2. **偏向锁**：当一个线程首次访问 `synchronized` 块时，会尝试使用偏向锁。偏向锁是一种优化手段，它的思想是偏向第一个获取它的线程，该线程在后续访问时无需再进行同步操作。如果在此期间没有其他线程访问该锁，则线程无需进行任何同步操作。
3. **轻量级锁**：如果有其他线程尝试访问已经被偏向锁定的资源，偏向锁会升级为轻量级锁。轻量级锁使用**CAS**操作尝试获取锁，如果成功则顺利执行，否则进行**自旋等待**(即不断重试)。这个过程不需要线程挂起，因此效率相对较高。
4. **重量级锁**：如果自旋过程中仍有大量线程竞争锁，轻量级锁会升级为重量级锁。此时，**未获取到锁的线程将被阻塞**，进入等待状态。当锁被释放时，这些线程会被唤醒并尝试重新获取锁。重量级锁涉及到线程的挂起和唤醒，这需要通过操作系统来完成，因此性能开销相对较大。

锁的升级过程在很大程度上是通过**对象头中**的 **Mark Word** 标志位来实现的，可以通过对象头的 Mark Word标志位来观察，Mark Word 中的标志位用于表示不同的锁状态

### 双重CAS

双重CAS 是对 CAS 操作的扩展，允许同时比较和交换两个变量的值

### ReentrantLock（可重入锁）

`ReentrantLock` 实现了 `Lock` 接口，是一个**可重入且独占式**的锁。

`ReentrantLock` 里面有一个内部类 `Sync`，`Sync` 继承 **AQS**（`AbstractQueuedSynchronizer`），添加锁和释放锁的大部分操作实际上都是在 `Sync` 中实现的。`Sync` 有公平锁 `FairSync` 和非公平锁 `NonfairSync` 两个子类。

### ReetrantLock(可重入锁)和sychronized区别?公平锁/非公平锁

**一、锁的获取方式**

1. `synchronized`：**隐式**获取锁，即当线程进入同步代码块或方法时自动获取锁，并在退出时自动释放锁。
2. `ReentrantLock`：**显式**获取锁，需要程序员手动调用`lock()`方法获取锁，并在使用完毕后调用`unlock()`方法释放锁。

**二、锁的公平性**

1. `synchronized`：是**非公平锁**，它不能保证等待时间最长的线程最先获取锁。这意味着线程的调度顺序是不确定的，可能会导致某些线程长时间得不到执行。
2. `ReentrantLock`：**可以是公平锁，也可以是非公平锁**。当设置为公平锁时，它能够保证等待时间最长的线程最先获取锁，从而提供了一种更公平的线程调度方式。

**三、锁的灵活性**

1. `synchronized`：功能相对固定，缺乏一些高级功能。例如，无法设置超时时间，无法判断锁是否被其他线程持有，也不支持使用`Condition类`实现线程等待/通知机制。
2. `ReentrantLock`：提供了更多的灵活性。例如，可以设置获取锁的超时时间，可以判断锁是否被其他线程持有，以及支持使用`Condition类`实现更复杂的线程等待/通知机制。

**四、锁的状态判断与中断**

1. `synchronized`：无法判断锁的状态，且不可被中断。即使在发生异常时，它也会自动释放锁。
2. `ReentrantLock`：可以通过`isLocked()`方法判断锁的状态(是否被锁定)。此外，它支持可中断的获取锁方式(`lockInterruptibly()`)，这意味着在等待获取锁的过程中，如果线程被中断，它将能够响应这个中断并停止等待。

**五、实现层次与锁信息存储**

1. `synchronized`：是JVM层次的锁，锁信息保存在对象头中。这意味着它的实现与Java虚拟机紧密相关。
2. `ReentrantLock`：是JDK层次的锁，通过代码中`int`类型的`state`标识来标识锁的状态这使得它在某些方面比`synchronized`更加灵活和可配置

### ThreadLocal？ThreadLocal内存泄漏？

**ThreadLocal介绍**

`ThreadLocal`是Java中的一个**类**，它提供了线程局部(`thread-local`)变量。这些变量与普通的可共享变量不同，因为每一个访问这个变量的线程都有它自己的独立初始化的变量副本。通过`ThreadLocal`，我们可以避免多线程环境下的数据不一致问题，因为每个线程都操作自己的数据，不存在线程安全问题。`ThreadLocal`适用于每个线程需要自己独立的数据副本，并且该数据可以在多个方法中使用，但在线程之间应该是隔离的场景。

`ThreadLocalMap` 存储在**线程的私有内存**中，key 是 `ThreadLocal` 对象，value 是存储的值。

**ThreadLocal底层实现原理**

`ThreadLocalMap`类型的变量。`ThreadLocal`本身并不存储数据，而是作为`ThreadLocalMap`的管理者。当调用`ThreadLocal` 的`set()`方法时，会将**`ThreadLocal`的引用作为`key`，用户传入的值作为`value`存入`ThreadLocalMap`中**。每个线程内部都维护着自己的`ThreadLocalMap`，这样每个线程的读写操作都是基于线程本身的一个私有的变量副本线程之间的数据是相互隔离的。

具体来说，`ThreadLocalMap`是通过一个`Entry数组`实现的，每个`ThreadLocal`实例在数组中都有对应的索引位置，这个位置是通过`ThreadLocal`的`hashcode()`方法和数组长度取模得到的。当线程调用`ThreadLocal`的`get()`或`set()`方法时，它会根据自己线程内的`ThreadLocalMap`来存取数据。

**ThreadLocal内存泄漏问题**

`ThreadLocal`如果使用不当，可能会导致内存泄漏。这主要是因为**`ThreadLocalMap`中的`KEY`也就是`ThreadLocal`实例是弱引用，而对应的值`value`则是强引用**。

为了避免这种情况，有几种方法：

调用其`remove()`方法清除对应的`Entry`，避免因为线程长时间存活而导致的内存泄漏。

### 线程池参数？

1. corePoolSize(核心线程数)
2. maximumPoolSize(最大线程数)
3. keepAliveTime(线程空闲时间)
4. unit(时间单位)
5. workQueue(工作队列)
6. threadFactory(线程工厂)
7. handler(拒绝策略)

### 饱和策略（拒绝策略）

1. AbortPolicy(中止策略)
   - 这是默认的拒绝策略。当无法处理新任务时，它会直接抛出一个`RejectedExecutionException`异常。这种策略适用于关键业务，因为它能及时反馈程序的运行状态。
2. CallerRunsPolicy(调用者运行策略)
   - 当线程池无法处理新任务时，该策略不会抛出异常，而是直接在提交任务的线程中。执行该任务。这提供了一种简单的任务降级机制，但需要注意，如果提交任务的线程是一个重要线程(如UI线程)，则可能会导致问题
3. DiscardPolicy(丢弃策略)
   - 当线程池无法处理新任务时，该策略会默默地丢弃这个任务，不会有任何反馈。这适用于那些无关紧要的任务。
4. DiscardOldestPolicy(丢弃最旧策略)
   - 当线程池无法处理新任务时，该策略会丢弃工作队列中最旧的任务，然后尝试重新提交当前的任务。这种策略适用于需要发布消息等场景。

### 线程池处理过程（任务调度）（线程池流程）

![图解线程池实现原理](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/thread-pool-principle.png)

### 线程池优势（线程池解决了什么问题）

线程池解决的核心问题就是**资源管理问题**。在并发环境下，系统不能够确定在任意时刻中，有多少任务需要执行，有多少资源需要投入，就会导致频繁申请/销毁资源和调度资源

- **降低资源消耗**：通过池化技术重复利用已创建的线程，降低线程创建和销毁造成的损耗。
- **提高响应速度**：任务到达时，无需等待线程创建即可立即执行。
- **提高线程的可管理性**：线程是稀缺资源，如果无限制创建，不仅会消耗系统资源，还会因为线程的不合理分布导致资源调度失衡，降低系统的稳定性。使用线程池可以进行统一的分配、调优和监控。

### 创建线程池

**通过`ThreadPoolExecutor`构造函数来创建**

```java
ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(
    corePoolSize,    // 核心线程数
    maximumPoolSize, // 最大线程数
    keepAliveTime,   // 空闲线程存活时间
    TimeUnit.SECONDS,// 存活时间的单位
    new LinkedBlockingQueue<Runnable>(), // 任务队列
    new ThreadFactory() { // 线程工厂
        @Override
        public Thread newThread(Runnable r) {
            return new Thread(r);
        }
    },
    new ThreadPoolExecutor.AbortPolicy() // 拒绝策略
);
```

### 任务队列种类？（阻塞队列种类）

![image-20240907161647206](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240907161647206.png)

### 线程池最优的线程数

- CPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1。比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。
- I/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。`2n` 是在充分考虑系统资源利用效率、上下文切换开销、线程池管理复杂性和实际应用需求的基础上得出的平衡点

### 设计线程池?

这是一个常见的面试问题，本质其实还是在考察求职者对于线程池以及阻塞队列的掌握。

我们上面也提到了，不同的线程池会选用不同的阻塞队列作为任务队列，比如`FixedThreadPool` 使用的是`LinkedBlockingQueue`（有界队列），默认构造器初始的队列长度为 `Integer.MAX_VALUE` ，由于队列永远不会被放满，因此`FixedThreadPool`最多只能创建核心线程数的线程。

假如我们需要实现一个优先级任务线程池的话，那可以考虑使用 `PriorityBlockingQueue` （优先级阻塞队列）作为任务队列（`ThreadPoolExecutor` 的构造函数有一个 `workQueue` 参数可以传入任务队列）。

`PriorityBlockingQueue` 是一个支持优先级的无界阻塞队列，可以看作是线程安全的 `PriorityQueue`，两者底层都是使用小顶堆形式的二叉堆，即值最小的元素优先出队。不过，`PriorityQueue` 不支持阻塞操作。

要想让 `PriorityBlockingQueue` 实现对任务的排序，传入其中的任务必须是具备排序能力的，方式有两种：

1. 提交到线程池的任务实现 `Comparable` 接口，并重写 `compareTo` 方法来指定任务之间的优先级比较规则。
2. 创建 `PriorityBlockingQueue` 时传入一个 `Comparator` 对象来指定任务之间的排序规则(推荐)。

不过，这存在一些风险和问题，比如：

- `PriorityBlockingQueue` 是无界的，可能堆积大量的请求，从而导致 OOM。
- 可能会导致饥饿问题，即低优先级的任务长时间得不到执行。
- 由于需要对队列中的元素进行排序操作以及保证线程安全（并发控制采用的是可重入锁 `ReentrantLock`），因此会降低性能。

对于 OOM 这个问题的解决比较简单粗暴，就是继承`PriorityBlockingQueue` 并重写一下 `offer` 方法(入队)的逻辑，当插入的元素数量超过指定值就返回 false 。

饥饿问题这个可以通过优化设计来解决（比较麻烦），比如等待时间过长的任务会被移除并重新添加到队列中，但是优先级会被提升。

对于性能方面的影响，是没办法避免的，毕竟需要对任务进行排序操作。并且，对于大部分业务场景来说，这点性能影响是可以接受的。

**线程池的实现逻辑：**

1. 任务队列

2. 任务提交

   `execute()`函数

3. 线程管理

   管理线程的启动、复用和销毁，使用 `Worker` 类封装线程执行逻辑

4. 定义任务拒绝策略

5. 提供方法启动和关闭线程池

   `shutdown()`方法

### AQS

 **一、AQS概述**

AQS是一个用于构建锁和同步器的**框架**，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是基于 **CLH 锁** （Craig, Landin, and Hagersten locks） 实现的

**二、AQS的主要作用**

AQS为Java中的各种同步器提供了一个统一的基础框架，如`ReentrantLock`、`Semaphore`等

**三、AQS的工作原理**

1. 同步状态：AQS使用一个`int`类型的变量来表示同步状态，通过 CAS 操作`volatile`修饰来实现状态的改变。这个状态可以被视为一种资源或者锁的占用情况。
2. 队列管理：AQS内部维护了一个双向队列(基于CLH队列的变体实现)，用于管理请求资源的线程。当线程请求资源时，如果资源被占用，线程会被封装成一个Node节点并加入到同步队列中。
3. 线程唤醒与阻塞：当持有资源的线程释放资源时，AQS会从同步队列中唤醒一个线程来获取资源。这一过程中，AQS会进行线程的阻塞与唤醒操作。
4. 独占与共享模式：AQS支持独占模式和共享模式。在独占模式下，只有一个线程能够获取资源；在共享模式下，多个线程可以同时获取资源。

**四、AQS的特点**

1. 可重入性：AQS支持锁的重入，即同一个线程可以多次获取同一把锁而不会出现死锁。
2. 灵活性：AQS提供了一个框架，开发者可以基于此框架进行扩展和定制化，以满足不同的同步需求。
3. 高效性：AQS通过精细化的状态管理和线程唤醒机制，实现了高效的并发控制，

### AQS等待队列、CLH队列

AQS 的等待队列是一个变种的 CLH 队列锁，结合了 CLH 队列的链表结构和先进先出的特点，但在等待机制上更为复杂，

AQS 使用一个等待队列来管理争夺资源的线程，这个等待队列是一个 这个等待队列是一个 **FIFO（先进先出）双向链表**。每个线程在争夺资源时，会被放入等待队列的尾部，等待前驱节点释放资源。

等待队列中的节点，每个节点代表一个线程及其等待状态。

###  Java里协程

Java 原生并不直接支持协程，从 Java 19 开始引入的 Project Loom 提供了对虚拟线程（Virtual Threads）的支持，这在一定程度上类似于协程。

虚拟线程在轻量级和高效性方面兼具了协程的优点和传统线程的灵活性。

虚拟线程的上下文切换不涉及内核态的切换，主要在用户态完成。JVM 负责在虚拟线程之间切换上下文，这种切换的开销远小于操作系统线程的上下文切换。

尽管虚拟线程的创建和管理在用户态完成，但它们的调度和阻塞处理仍依赖于 JVM 和操作系统的协作。这种设计结合了用户态轻量级线程管理和内核态线程调度的优势

### 锁超时释放

锁超时释放是指在某些情况下，如果一个线程在特定时间内无法获得锁，它将放弃尝试并采取其他措施（如抛出异常、进行其他操作或重试）。

**自旋和挂起**：

- 在超时等待过程中，线程通常会先进行短暂的自旋尝试获取锁。如果在短时间内未能获取锁，线程会挂起（通过 `LockSupport.parkNanos`）以减少 CPU 占用。

**状态变量**：

- 锁的状态通常由一个 `volatile` 变量（如 `state`）表示，`compareAndSetState` 方法通过 CAS 操作确保状态更新的原子性。

### 读写锁

读写锁进行并发控制的规则：读读不互斥、读写互斥、写写互斥

底层也是基于 AQS 实现的

### 线程池的原理

**线程复用**：线程池维护了一组线程，这些线程可以被重复使用来执行多个任务，避免了频繁创建和销毁线程的开销。

**任务队列**：线程池使用一个任务队列来存储待执行的任务。当所有工作线程都在忙碌时，新提交的任务会进入队列等待。

**工作线程管理**：线程池管理一定数量的工作线程，这些线程会不断从任务队列中取出任务进行执行。

**线程池大小**：线程池通过配置核心线程数和最大线程数来控制线程池的大小。

**拒绝策略**：当线程池已达到最大线程数且任务队列已满时，新任务提交会触发拒绝策略。

### 线程安全的数据结构

1. 线程安全集合类

   `Collections.synchronizedXXX` 方法可以将非线程安全的集合（如 `ArrayList`、`HashMap` 等）转换为线程安全的集合类

   `Collections.synchronizedList(List<T> list)`

2. **并发集合类**：

   `java.util.concurrent`

   比如：`ConcurrentHashMap`

3. 阻塞队列：

   `ArrayBlockingQueue`、`LinkedBlockingQueue`、`PriorityBlockingQueue`

4. 原子队列：

   如 `AtomicInteger`、`AtomicLong`、`AtomicReference` 等

### 线程池线程数过多/过少（线程池监控）

- 如果我们设置的线程池数量**太小**的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者**大量任务堆积在任务队列导致 OOM**。这样很明显是有问题的，**CPU 根本没有得到充分利用**。
- 如果我们设置线程数量**太大**，大量线程可能会**同时在争取 CPU 资源**，这样会导致大量的**上下文切换**，从而增加线程的执行时间，影响了整体执行效率。
- 用**JConsole**：是 JDK 附带的一个图形化监控工具，可以用来连接到运行中的 JVM，并查看线程池的状态，在命令行中运行 `jconsole`，选择目标 JVM 进程，查看线程池的状态指标，例如核心线程数、最大线程数、活动线程数等。

### 线程池优化 IO 密集型，IO优化方法？

**使用非阻塞 IO**：如果可能的话，使用非阻塞 IO 操作（如 NIO），可以减少线程在等待 IO 操作完成时的阻塞时间，提高线程的利用率。

**使用异步 IO**：对于支持异步 IO 的操作系统和框架，可以通过异步 IO（AIO） 操作来管理大量的并发 IO 请求，减少线程的阻塞和等待时间。

**调整线程池大小**：根据实际的 IO 负载和系统资源情况，适当调整线程池的大小。可以考虑增加线程池中的线程数，以便更多的 IO 操作可以并发执行。

**使用专门的 IO 线程池**：可以考虑将 IO 操作和 CPU 计算分离，使用专门的 IO 线程池来处理长时间的 IO 操作，以避免影响 CPU 密集型任务的执行。

**优化 IO 操作**：分析和优化具体的 IO 操作，包括减少 IO 请求的频率、使用缓存、批量处理等方法，以减少 IO 操作的耗时和系统资源消耗。

###  `wait()` 、 `notify()`加锁？wait、notify在synchronized代码块中

 `wait()` 和 `notify()`方法进行线程间的通信时，**需要在调用这些方法之前获取对象的锁**（即通过 `synchronized` 或 `ReentrantLock` 等机制锁定对象）。**因为其依赖于对象监视器（monitor）的机制**

只有持有对象监视器锁的线程才有资格调用对象的 `wait()` 或 `notify()` 方法。**`synchronized` 代码块或方法用于确保只有一个线程可以持有该对象的监视器锁。**

因为它们依赖于对象的监视器锁。线程在进入 `wait()` 或 `notify()` 状态时，必须确保没有其他线程同时修改共享资源。

- 在调用 `wait()` 方法时，当前线程会释放对象的锁，进入等待状态。当其他线程调用对象的 `notify()` 或 `notifyAll()` 方法时，被等待的线程将被唤醒，并尝试重新获取对象的锁以继续执行。
- 通过锁对象，确保在调用 `wait()` 和 `notify()` 方法时，操作的原子性和可见性。这样可以避免因并发操作导致的竞态条件和不一致的状态。
- Java 中的每个对象都有一个关联的监视器（monitor），用于实现对象的锁定和线程的同步。`wait()` 和 `notify()` 方法依赖于对象监视器来管理线程的等待和唤醒。

### 对象头/markword

对象的结构包括对象头、对象体、对象填充

对象头里包括Markword、类型指针、数组长度

Markword包括对象的`hashCode()`、分代年龄、锁标志位

![image-20240717152830485](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240717152830485.png)

### 线程池最大线程数不生效?线程自动消亡?

**最大线程数设置不生效的情况**：

使用 `LinkedBlockingQueue` 作为任务队列时，任务队列是无界的，那么即使核心线程池已满，也不会创建新的线程，而是将任务放入队列中。这种情况下，最大线程数设置可能没有实际效果。

**实现到期的线程自动消亡**：

核心线程处于空闲状态也不会被销毁，非核心线程超过线程空闲时间会被销毁

可以通过 `setKeepAliveTime(long time, TimeUnit unit)` 设置

### 动态线程池？

动态线程池是指可以在运行时根据实际需要调整其参数（如核心线程数、最大线程数、空闲线程存活时间等）的线程池。这种线程池的特点是能够适应不同的工作负载和需求变化，提供更灵活和高效的资源管理。

1. 有时高峰期任务量大，需要更多的线程来处理；有时低谷期任务量小，需要减少线程数以节省资源。
2. 通过动态调整线程池的大小，可以在保证性能的前提下，最大限度地提高资源利用率，减少不必要的资源浪费。
3. 动态线程池减少了手动调优的工作量，简化了系统的运维和管理

```java
  dynamicThreadPool.setCorePoolSize(4); // 动态调整核心线程数
  dynamicThreadPool.setMaximumPoolSize(10); // 动态调整最大线程数
```

### Java锁？为什么只用CAS不行?

1. **内置锁（synchronized）**

- **实现方式**：Java 中的 `synchronized` 关键字是最基本的锁机制，它可以应用于方法或代码块。

2. **重入锁（ReentrantLock）**

- **实现方式**：`ReentrantLock` 是 `java.util.concurrent.locks` 包中的一种锁机制，提供了比 `synchronized` 更多的灵活性和扩展性，如可中断锁、公平锁等。
- **特点**：支持公平性设置，可以通过 `tryLock()` 方法尝试获取锁，支持条件变量等高级功能。

3. **读写锁（ReadWriteLock）**

- **实现方式**：`ReadWriteLock` 接口及其实现类 `ReentrantReadWriteLock` 提供了读写分离的锁机制，允许多个线程同时读取共享数据，但只允许一个线程写入数据。

4. **CAS（Compare-And-Swap）**

- **实现方式**：CAS 是一种乐观锁技术，不是基于阻塞来实现线程安全，而是使用原子操作来实现。Java 中的 CAS 操作主要通过 `java.util.concurrent.atomic` 包中的原子类来实现。
- **特点**：高效、非阻塞，适用于并发量较低、冲突较少的场景。但是 CAS 无法解决多步操作的原子性问题，存在 ABA 问题（解决方式可通过增加版本号或时间戳来避免）。

**为什么只用 CAS 不行？**

**ABA 问题**：CAS 操作在比较并交换时只关注变量的当前值，无法感知到变量的历史变化。因此，如果一个变量从 A 变为 B，然后又重新变为 A，CAS 操作就无法检测到这种变化。解决方案可以是使用版本号或者增加时间戳来避免这个问题。

由于 CAS 是**自旋操作**，会导致大量的 CPU 自旋消耗，降低了性能。传统的锁机制可以在无法获取锁时进行线程阻塞，释放 CPU。

CAS 只能保证单个变量的原子性操作，对于复合操作（如读-改-写），需要通过加锁或者其他机制来实现。

### 线程池的核心线程在空闲的时候是处于什么状态的?那处于waiting状态下，是怎么来唤醒的?

核心线程在空闲的时候通常处于 **`WAITING`**（等待）状态

因为即使核心线程处于空闲状态，它们也不会被终止，将继续存活并等待新的任务。

任务队列是一个阻塞队列，当任务队列为空时，核心线程会进入 `WAITING` 状态，等待有新的任务被提交到队列中，一旦有新的任务被提交到阻塞队列中，阻塞队列会唤醒一个或多个正在等待的线程，使得核心线程能够继续获取任务并执行。

是的，使用阻塞队列，但是刚开始的任务是先创建核心线程，并把任务放进去，不走阻塞队列

当线程数小于核心线程数时，即使现有的线程空闲，线程池也会优先创建新线程来处理任务，而不是直接交给现有的线程处理。

这里走阻塞队列，是因为假设核心线程已满，并且有空闲的时候，任务进来还是会先检测核心线程数量，发现已满，所以进入阻塞队列，核心线程处于空闲状态，发现阻塞队列有新的任务就被唤醒

### `ReentrantLock`中 `tryLock()` / `lock()` 

- **`tryLock()`**：
  - 尝试获取锁，如果获取成功立即返回 `true`，如果锁被其他线程持有或者获取锁超时，则返回 `false`。
  - 可以设置超时时间，例如 `tryLock(long time, TimeUnit unit)`，在指定时间内尝试获取锁，超过时间未获取到则返回 `false`。
- **`lock()`**：
  - 获取锁，如果锁已经被其他线程持有，则当前线程会被阻塞直到获取到锁。
  - 如果不调用 `unlock()` 方法释放锁，其他线程将无法获取该锁，可能会导致死锁。

### 线程池异常

**使用Future获取异常**：

如果你使用的是 `ThreadPoolExecutor` 或其子类创建线程池，并且提交任务时使用了 `Future` 或 `CompletableFuture`，可以通过 `Future.get()` 方法获取任务执行的结果，包括异常信息。

### 内存重排序和指令重排序

**内存重排序**是指编译器和处理器在执行程序时，为了**优化性能**，可能会对指令的执行顺序进行调整。这种调整不会改变单线程环境下程序的正确性，但在多线程环境下可能会引起数据竞争和可见性问题。

**内存重排序的类型**

1. **编译器重排序**：编译器在生成机器码时可能会调整代码顺序。
2. **指令重排序**：系统在执行代码的时候并不一定是按照你写的代码的顺序依次执行。
3. **内存系统重排序**：由于缓存和内存之间的数据同步，可能会导致数据在不同线程之间的可见性问题。

**指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致**

## Java：JVM

### JVM数据区(JVM内存结构)?

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/java-runtime-data-areas-jdk1.7.png" alt="Java 运行时数据区域（JDK1.7）" style="zoom:50%;" />

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/java-runtime-data-areas-jdk1.8.png" alt="Java 运行时数据区域（JDK1.8 ）" style="zoom:50%;" />

- 首先是**程序计数器**，这是一块较小的内存空间。它可以看作是当前线程所执行的字节码的行号指示器。每个线程都有一个独立的程序计数器，**用于记录下一条要执行的字节码指令的地址**。
- 接下来是**虚拟机栈**，这也是线程私有的内存区域。每个线程在创建时都会创建一个虚拟机栈，它描述的是Java方法执行的内存模型。栈中的每个栈帧都存储着**局部变量、操作数栈、动态链接和方法出口**等信息。当方法被调用时，一个新的栈帧会被压入栈中，当方法执行完毕后，对应的栈会被弹出。
- 与虚拟机栈相似的是**本地方法栈**，它为虚拟机使用到的`Native`方法提供内存空间。它也是线程私有的，与虚拟机栈不同的是，虚拟机栈为执行Java方法服务，而本地方法栈为执行`Native`方法服务。本地方法栈包含了本地方法执行所需的所有信息，包括**本地变量、输入参数和中间结果**。
- 再来看**Java堆区**，这是JVM内存中最大的一块区域，用于存储Java对象实例。Java 堆是垃圾收集器管理的主要区域，因此也被称作 **GC 堆**，堆区是所有线程共享的，它还可以被细分为年轻代和老年代，其中年轻代又可以分为Eden区和Survivor区。堆区的大小是在虚拟机启动时就已经设定好的，但可以通过JVM参数来进行调整。
- 最后是**方法区**，这也是一个线程共享的内存区域。它用于存储已被虚拟机加载的**类信息、常量、静态变量**、即时编译器编译后的代码等数据。方法区中有一个非常重要的部分叫做**运行时常量池**，它存放着**编译期生成的各种字面量和符号引用**。

### 哪些区域可能会发生ful l  GC？哪些区域会发生OOM问题？方法区在什么情况下会发生OOM？什么情况下会发生full  GC?为什么说full  GC效率比较低？为什么full  GC的时候用户程序不能运行？

**发生full GC**（整堆回收）：

**堆内存（Heap Memory）**

- **新生代（Young Generation）：** 包括 Eden 区和两个 Survivor 区（S0 和 S1）。
- **老年代（Old Generation）：** 包含长期存活的对象。Full GC 通常发生在老年代中，特别是当老年代空间不足时，JVM 会尝试回收整个堆中的对象，包括新生代和老年代。

**方法区（Method Area）**

- 在 Java 8 及以前，方法区的实现称为永久代（Permanent Generation）。在 Java 8 之后，永久代被移除，取而代之的是元空间（Metaspace）。
- **永久代/元空间（Permanent Generation/Metaspace）：** 存储类的元数据、常量池、方法信息等。当这些区域中的空间不足时，也可能会触发 Full GC。

**发生OOM：**

**除了程序计数器**都有可能

**方法区发生OOM（内存溢出）：**

当元空间溢出时会得到如下错误：`java.lang.OutOfMemoryError: MetaSpace`

Java7使用永久代时，超过方法区（永久代）最大大小将会抛出 `OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen`

具体实例：

1. **大量加载类**：当大量的类被加载时，每个类的常量池都需要占用一定的内存空间。如果加载的类过多，可能会导致元空间内存耗尽，从而引发 OOM。
2. **过多的静态变量**：
   - 静态变量属于类级别的变量，当类被加载时，这些静态变量会被放入元空间中。如果类中定义了过多的静态变量，或者静态变量引用了大量对象，会导致元空间内存耗尽。

**Full GC效率低**：

**回收范围广**：

- **Full GC** 不仅回收年轻代（Eden 和 Survivor 区域），还包括老年代（Tenured 区域）。这意味着它需要检查和清理整个堆内存，这比只回收年轻代要复杂和耗时得多。

**频繁的标记-清理过程**：

- Full GC 通常涉及标记-清理或标记-整理（Compact）算法。这些算法需要遍历整个堆，标记所有活跃对象，并清理掉不再使用的对象。这个过程需要花费较多的时间和资源，特别是当堆内存很大时。

**需要整理内存**：

- 在进行 Full GC 时，老年代可能需要整理内存（尤其是使用标记-整理算法）。这不仅要清理无用的对象，还要移动存活的对象，以防止内存碎片化，这会进一步增加耗时。

**用户程序暂停：**

**Stop-The-World（STW）事件**：

- 在 Full GC 期间，Java 虚拟机会触发 Stop-The-World 事件，即暂停所有用户线程的执行。这是因为在进行垃圾回收时，系统需要确保没有线程正在修改对象的引用，以确保垃圾回收的准确性和一致性。

**对象引用处理**：

- 在进行 Full GC 时，所有的线程都需要暂停，以确保垃圾回收器可以准确地标记和回收对象。线程暂停期间，JVM 需要对堆中的所有对象进行遍历和标记，以确定哪些对象是可回收的，哪些对象仍然在使用中。

### 堆内存/栈内存

堆主要用来**存放**对象的，栈主要是用来**执行**程序的

**存储内容**：

- 堆内存：存放由**new**创建的对象和数组
- 栈内存：存储方法调用的栈帧，包括局部变量和方法参数。基本类型的变量和对象的引用变量都存储在栈内存中。

**分配和管理**

- 堆内存：由垃圾收集器自动管理，分配和释放是动态的。
- 栈内存：超过变量的作用域后，Java会自动释放掉为该变量所分配的内存空间

**线程访问**：

- 堆内存：所有线程共享，需要线程同步。
- 栈内存：每个线程私有，不需要线程同步

**存取速度**，**内存大小**：

- 栈内存的存取速度比堆快，栈的内存通常较小，堆的比较大

### Integer a=3存放在哪个区？int a=3存放哪个区？

**`Integer a = 3;`**：

- `Integer a` 是一个对象引用变量，存储在栈内存中。
- `3` 是一个常量，先从常量池中查找是否有值为 `3` 的 `Integer` 对象。如果没有，则会创建一个新的 `Integer` 对象，存储在堆内存中。
- 对于整数常量 `3`，如果它在 `-128` 到 `127` 的范围内，且常量池中已存在对应的 `Integer` 对象，则这个对象的引用会直接指向常量池中的对象，不会额外创建堆内存中的对象。
- `a` 引用变量指向堆内存中的 `Integer` 对象。

**`int b = 3;`**：

- `b` 是一个基本类型变量，存储在栈内存中。
- 变量 `b` 的值 `3` 直接存储在栈内存中。

###  Java内存泄漏

内存泄漏是指程序在申请内存后，**无法释放已申请的内存空间**，一次内存泄露危害可以忽略，但内存泄露堆积后果很严重，无论多少内存,迟早会被占光，而OutOfMemoryError 是内存泄漏可能导致的一个严重后果之一。解决内存泄漏可以预防或减少发生 OutOfMemoryError 的可能性。

- 内存加载数据量过大：例如不受行数限制的数据库查询语句，或者不限制字节数的文件读取等
- 内存泄漏：当系统存在大量未关闭的 IO 资源，或者错误使用`ThreadLocal`等场景时也会发生`OOM`
- 系统内存不足：系统内存不足以支撑当前业务场景所需要的内存，过小的机器内存或者不合理的JVM内存参数

例子：

每次循环迭代都会创建一个新的字节数组对象，并将其添加到 `list` 中。

由于 `list` 是静态的，并且在循环外部声明，所以 `list` 的引用会持续存在，并且持有所有添加到其中的字节数组对象的引用。

虽然在循环中每个新的字节数组对象被创建时，旧的对象不再被引用，但它们由于被添加到 `list` 中，导致垃圾回收器无法回收它们的内存。

```java
public class MemoryLeakExample {

    private static List<byte[]> list = new ArrayList<>();

    public static void main(String[] args) {
        while (true) {
            byte[] data = new byte[100 * 1024]; // 申请100KB的字节数组
            list.add(data); // 将字节数组添加到列表中
        }
    }
}
```

- **`StackOverFlowError`：** 若栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 `StackOverFlowError` 错误。

- **`OutOfMemoryError`：** 如果栈的内存大小可以动态扩展， 如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出`OutOfMemoryError`异常

堆这里最容易出现的就是 `OutOfMemoryError` 错误，并且出现这种错误之后的表现形式还会有几种，比如：

1. **`java.lang.OutOfMemoryError: GC Overhead Limit Exceeded`**：当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。
2. **`java.lang.OutOfMemoryError: Java heap space`** :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发此错误。

当元空间溢出时会得到如下错误：`java.lang.OutOfMemoryError: MetaSpace`

Java7使用永久代时，超过方法区（永久代）最大大小将会抛出 `OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen`

### Java1.7 1.8有哪些主要区别?

**永久代 (PermGen) 替换为元空间 (MetaSpace)** ：

整个永久代有一个 JVM 本身设置的固定大小上限，无法进行调整（也就是受到 JVM 内存的限制），而元空间使用的是本地内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小

### 在java中创建对象，JVM会怎么做（类加载检查-分配内存-初始化零值-设置对象头-init）?分配多大空间呢？怎么算？

1. 类加载检查

   - 虚拟机遇到一条 new 指令时，首先将去检查这个指令的参数是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程

2. 分配内存

   - 在**类加载检查**通过后，接下来虚拟机将为新生对象**分配内存**。**分配方式**有 **“指针碰撞”** 和 **“空闲列表”** 两种，**选择哪种分配方式由 Java 堆是否规整决定，而 Java 堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定**。

   - 指针碰撞： 

     - 适用场合：堆内存规整（即没有内存碎片）的情况下。
     - 原理：用过的内存全部整合到一边，没有用过的内存放在另一边，中间有一个分界指针，只需要向着没用过的内存方向将该指针移动对象内存大小位置即可。
     - 使用该分配方式的 GC 收集器：Serial, ParNew

     空闲列表： 

     - 适用场合：堆内存不规整的情况下。
     - 原理：虚拟机会维护一个列表，该列表中会记录哪些内存块是可用的，在分配的时候，找一块儿足够大的内存块儿来划分给对象实例，最后更新列表记录。
     - 使用该分配方式的 GC 收集器：CMS

   - 内存分配并发问题：
     - **CAS+失败重试**
     - TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配

3. 初始化零值

   - 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。

4. 设置对象头

   - 初始化零值完成之后，**虚拟机要对对象进行必要的设置**，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 **这些信息存放在对象头中。** 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。

5. 执行 init 方法

   - 执行 new 指令之后会接着执行 `<init>` 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。

分配内存空间：**对象头**、**实例数据**和**对齐填充**。

例如：

```java
public class Example {
    private int a;
    private long b;
    private boolean c;
}
```

对象头：16字节

实例数据：

`int a`：4 字节

`long b`：8 字节

`boolean c`：1 字节

对齐填充：为了使对象大小成为 8 字节的整数倍，需要添加 3 字节的填充。



### JVM异常

1. OutOfMemoryError (OOM)：内存溢出
2. StackOverflowError：方法调用栈深度超过了 JVM 分配的栈大小
3.  ClassNotFoundException：类加载器无法找到指定的类
4.  NoClassDefFoundError：JVM 在运行时找不到已经编译好的类，通常是由于类路径问题或类加载器问题。
5. UnsatisfiedLinkError：JVM 无法找到本地库文件或库文件中的本地方法
6. IllegalStateException：JVM 在不合法或不适当的时间检测到方法调用
7. ConcurrentModificationException：集合进行遍历时，集合被其他线程修改，或在单线程中错误地修改集合。

### JVM内存分配策略

首先，对象优先分配在Eden区。当一个新的对象需要被创建时，JVM会尝试在Eden区域进行分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。如果Eden还是没有足够空间，新对象将被分配到老年代。GC 期间虚拟机又发现无法存入 Survivor 空间（这里需要理解一下：在 GC 过程中，存活的对象会从 Eden 区和一个 Survivor 区（假设S0）复制到另一个 Survivor（假设S1） 区，无法存入Survivor的意思就是S1存不下Eden + S0），所以只好通过 **分配担保机制** 把新生代的对象（全部，不是新对象）提前转移到老年代中去

（在一次新生代垃圾回收后，如果对象还存活，则会进入 S0 或者 S1，并且对象的年龄还会加 1(Eden 区->Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。）

其次，大对象直接进入老年代。为了避免在Eden区和Survivor区之间发生大量的内存复制，对于那些需要大量连续内存空间的Java对象，如很长的字符串以及数组，JVM会直接将它们分配到老年代。

再者，长期存活的对象会进入老年代。JVM为了识别哪些是长期存活的对象，会给每个对象定义一个对象年龄计数器。当一个对象经过多次Minor GC后仍然存活，其年龄计数器会增加，当达到一定的阈值(默认是15)，该对象就会被移动到老年代。

此外，还有动态对象年龄判定的策略。如果Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，那么年龄大于或等于该年龄的对象可以直接进入老年代，而无需等到年龄阈值中要求的年龄。

最后，为了保障内存分配的顺利进行，IM还采用了空间分配担保机制。在进行Minor GC之前，JVM会检查老年代是否有足够的空间来容纳Survivor区中的所有对象，以确保不会发生内存溢出。

### JNI

**JNI（Java Native Interface）** 是一种 Java 语言提供的**接口**，允许 Java 代码与其他语言（如 C 或 C++）编写的本地代码进行交互。JNI 的主要作用是使 Java 程序能够调用和使用本地库中的函数，以及允许本地代码调用 Java 程序中的方法。

### 可达性分析算法？GC Roots?引用类型？

**可达性分析算法：**

可达性分析算法是JVM中用于确定哪些对象应该被垃圾收集的重要算法。它的核心思想是从一组被称为GC Roots的对象开始，沿着引用链向下搜索，找到所有可达的对象。如果某个对象从GC Roots出发无法被搜索到，即没有引用链相连，那么这个对象就被视为垃圾，可以被回收。

**GC Roots对象：**

- 虚拟机栈(栈帧中的局部变量表)中引用的对象，这包括当前活跃线程中的本地变量和正在执行的方法中的参数对象，这些都是由活动线程所持有的引用。

- 本地方法栈(Native 方法)中引用的对象

- 方法区中类静态属性引用的对象，这类对象是通过类的静态变量引用的，它们在程序运行期间一直存在。

- 方法区中常量引用的对象

- 所有被同步锁持有的对象

- JNI（Java Native Interface）引用的对象

**引用类型：**

首先，我们来谈谈**强引用**。强引用是Java中最常见的引用类型，它是默认的引用方式。当一个对象被强引用所指向时，垃圾回收器是不会回收这个对象的，当内存空间不足，Java 虚拟机宁愿抛出 `OutOfMemoryError` 错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足问题。例如，通过 `new` 关键字创建一个对象并将其赋值给一个变量时，这个变量就是对该对象的一个强引用。

接下来是**软引用**。软引用是通过 `SoftReference` 类来实现的。它用于描述那些还有用但并非必需的对象。当系统内存足够时，软引用指向的对象不会被回收；但当系统内存不足时，垃圾回收器会开始考虑回收这些对象。这种特性使得软引用非常适合用于实现内存敏感的缓存，比如图片缓存等。当内存紧张时，可以自动释放这些缓存，从而避免内存溢出，

再来说说**弱引用**。弱引用通过 `weakReference` 类来实现。只要垃圾回收器开始运行，无论当前内存是否充足，弱引用指向的对象都会被回收。弱引用的生命周期相对较短，主要用于解决内存泄漏问题，如缓存、监听器等情况。当对象只被弱引用所指向时，垃圾回收器会毫不犹豫地将其回收。

最后是**虚引用**。虚引用是最弱的一种引用类型，它不会对对象的生存时间构成影响，也无法通过虚引用来获取一个对象实例。虚引用的主要作用是跟踪对象被垃圾回收的状态。当垃圾回收器准备回收一个被虚引用指向的对象时，会将该虚引用加入到与之关联的引用队列中。这样，我们可以通过检查引用队列来了解对象何时被回收。虚引用通常用于实现一些特殊的内存管理需求。

### 垃圾回收算法(标记清除/复制/整理)

**标记-清除**算法，这是最基本的垃圾回收算法。它的原理很简单，分为两个阶段：标记阶段和清除阶段。在标记阶段，垃圾回收器会从根对象开始，递归地访问这些对象的引用，将访问到的对象都标记为"存活”。然后在清除阶段，垃圾回收器会遍历整个堆内存，将没有被标记的对象全部清除。这种算法的优点是实现简单，但缺点是会造成内存碎片化，导致后续分配大对象时可能找不到连续的内存空间。

**标记-复制**算法，这种算法将内存划分为两个等大小的区域。在任意时间点，只有其中一个区域被使用，另一个区域是空闲的。当当前使用的区域内存耗尽时，垃圾回收器会将所有存活的对象复制到另一个空闲区域中，然后清空当前区域。这种算法的优点是垃圾回收速度快，但缺点是内存使用率只有50%。复制算法通常用于新生代的垃圾回收，特别是针对大量短命对象进行回收时效率很高。

**标记-整理**算法，它标记存活对象的处理方式与标记-清除算法相同，但在清除阶段，它会将所有存活的对象都移动到一端，然后清理掉边界以外的内存。这种算法的优点是不会造成内存碎片化，但缺点是移动存活对象的成本较高。这种算法通常用于老年代的垃圾回收，因为老年代中存活的对象较多，不适合使用复制算法。

除了以上几种基本算法外，还有一些更先进的垃圾回收算法，如分代收集算法和区域化垃圾回收算法等。这些算法根据对象的存活周期和内存布局特点进行优化，以提高垃圾回收的效率和性能。

### 垃圾收集器(G1 CMS)（垃圾回收器）

首先是**`Serial`**收集器，这是最基本的收集器，采用单线程方式进行垃圾回收。它在执行时会暂停所有工作线程，因此不适用于需要高并发处理的系统。`Serial`收集器简单高效，在单核CPU或小内存环境下表现良好，适用于`Client`模式下的虚拟机。

接下来是**`ParNew`**收集器，它是`Serial`收集器的多线程版本，适用于多核CPU环境。`ParNew`收集器通过多个线程并行执行垃圾回收，从而提高了回收效率。它通常与CMS收集器配合使用，以减少用户线程的停顿时间。

**`Parallel Scavenge`**收集器则是一个以吞吐量优先的并行收集器。它使用多线程和复制算法进行垃圾回收，旨在达到一个可控制的吞吐量。`Parallel Scavenge`收集器适用于后台运算等弱交互场景，能够高效利用CPU时间，尽快完成程序运算任务。收。

**CMS**收集器是一种以获取最短停顿时间为目标的收集器，非常适合对响应时间有要求的系统。它使用`标记-清除`算法，并分为初始标记、并发标记、重新标记和并发清除四个阶段。然而，CMS收集器可能会产生内存碎片，并且对CPU资源敏感。(**初始标记-并发标记-重新标记-并发清除**)，主要依赖**增量更新**机制，**CMS 的标记阶段是增量更新（增量更新是一种逐步处理数据的策略）的**，尤其是在**并发标记**阶段，CMS 通过卡表和写屏障来追踪对象的修改，并进行相应的垃圾回收处理。

​	**卡表记录**：CMS 使用卡表来记录内存区域的修改。这种机制用于追踪堆中的对象引用变化，使得垃圾回收能够仅对发生变化的区域进行处理。

​	**写屏障（Write Barrier）**：CMS 在进行写操作时，通过写屏障技术将内存中的修改记录到卡表中，从而实现增量更新。

适用于**金融交易系统**（如高频交易系统），系统需要快速处理大量交易，响应时间至关重要，系统中有大量的老年代对象，因为交易数据需要长期存储

**G1**收集器则是一个面向大堆内存和多核处理器的收集器，满足 GC 停顿时间要求的同时，还具备高吞吐量性能特征，它将堆内存分割成多个区域进行回收，旨在提供**可预测的停顿时间**。G1收集器适用于大堆内存和需要低延迟的应用程序，能够在多核处理器上发挥更好的性能。分为**初始标记、并发标记、最终标记、筛选回收**。G1 使用**已记住集合（Remembered Set）**来记录对象引用的变化。每个区域都有一个 Remembered Set，用于跟踪其他区域中对象的引用。这样可以在进行垃圾回收时，仅处理那些引用发生变化的区域，G1 垃圾回收器将堆分成多个相同大小的区域，这些区域被分为 **Eden 区、Survivor 区和 Old 区**。G1 还会有一个额外的区域用于 **Humongous 对象**（即较大的对象），根据内存使用情况动态调整老年代的占用

适用于**大数据分析系统**（如大规模日志处理系统），应用程序需要处理大量的数据，堆内存非常大，并且需要对停顿时间有一定的控制，以确保系统的响应性

**ZGC**收集器是一个面向大堆内存、低延迟的垃圾收集器。它通过并发压缩来减少内存碎片，并且能够在多核环境下实现较低的停顿时间。ZGC收集器适用于需要快速响应和低停顿时间的应用场景。

### 新生代和老年代不同垃圾收集算法？

新生代使用`标记-复制`，老年代使用`标记-整理`

在新生代中，每次收集都会有大量对象死去，所以可以选择”标记-复制“算法，只需要**付出少量对象的复制成本就可以完成每次垃圾收集**。而老年代的对象存活几率是比较高的，而且**没有额外的空间**对它进行分配担保，所以我们必须选择`标记-清除`或`标记-整理`算法进行垃圾收集，使用`标记-整理`，是因为`标记-清除`后会产生大量不连续的内存碎片。

### 引用计数法

死亡对象判断方法，除了**可达性分析算法**之外，还有**引用计数法**。

给对象中添加一个引用计数器：

- 每当有一个地方引用它，计数器就加 1；
- 当引用失效，计数器就减 1；
- 任何时候计数器为 0 的对象就是不可能再被使用的

这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决**对象之间循环引用**的问题。

### G1收集器，`Cardtable`和`RememberSet`解决跨代引用？（卡表/记忆集）

高效地追踪和管理不同代（年轻代和老年代）之间的引用关系，从而优化垃圾收集的性能。

卡表是一个字节数组，每个字节对应一个卡片，用来标识该卡片是否包含了跨代引用。

**记忆集**是用来跟踪卡表中的脏卡片的集合，它记录了每个堆区（region）中的跨代引用

- **写屏障（Write Barrier）**：当一个对象引用发生改变时，写屏障机制会检测并更新卡表。如果某个对象的引用从年轻代指向了老年代，或者从老年代指向了年轻代，写屏障会将对应的卡片标记为脏。

- **卡表更新**：被写屏障标记为脏的卡片，其对应的卡表条目会被设置为“脏”。这意味着这个卡片中的对象引用需要被垃圾收集器特别关注。

- **记忆集更新**：卡表中的脏卡片会被加入到记忆集中。

- **垃圾收集**：在进行垃圾收集时，垃圾收集器会首先查看记忆集，以确定哪些卡片包含了跨代引用。然后，垃圾收集器只需要扫描这些卡片，而不必扫描整个堆，从而提高了效率。

（**对象A**在年轻代的Region 1。

**对象B**在老年代的Region 2。

**对象A**引用了**对象B**。）

### 一个对象在新生代最大的年龄，分代年龄记在哪里？

**年龄：**

- **最大年龄**：一个对象在新生代中达到一定年龄（通常是 15 岁，具体取决于 JVM 参数 `MaxTenuringThreshold` 的设置）后，如果它还存活，那么就会被晋升（Promote）到老年代。这一过程是为了减少在新生代中长期存活对象的垃圾收集开销。
- 对象的年龄通常保存在对象头的 `Mark Word` 中。`Mark Word` 是对象头的一部分，用来存储对象的元数据

### 内存泄漏/内存溢出？什么情况会导致内存溢出？什么情况下堆内存溢出、方法区溢出？

**内存泄漏**：

- 程序在申请内存后，无法释放已申请的内存空间

**内存溢出（Out of Memory, OOM）**：

- 程序在尝试分配内存时，没有足够的内存可用，导致抛出 `OutOfMemoryError`。
- 例子：频繁创建大对象，超过了堆内存限制；方法区加载过多类信息，超过了方法区的限制。

**方法区发生OOM（内存溢出）：**

当元空间溢出时会得到如下错误：`java.lang.OutOfMemoryError: MetaSpace`

Java7使用永久代时，超过方法区（永久代）最大大小将会抛出 `OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen`

具体实例：

1. **大量加载类**：当大量的类被加载时，每个类的常量池都需要占用一定的内存空间。如果加载的类过多，可能会导致元空间内存耗尽，从而引发 OOM。
2. **过多的静态变量**：
   - 静态变量属于类级别的变量，当类被加载时，这些静态变量会被放入元空间中。如果类中定义了过多的静态变量，或者静态变量引用了大量对象，会导致元空间内存耗尽。

**堆发生OOM**

当新生代（Eden区）没有足够的空间分配新对象时，会触发一次 Minor GC。

如果 Minor GC 后新生代仍然没有足够的空间，并且无法将对象移到老年代（比如老年代也没有足够的空间），就会发生堆内存溢出。

### JVM访问对象？句柄访问/指针访问？

 Java 程序通过栈上的 `reference` 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有：**使用句柄**、**直接指针**

**句柄**

 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而**句柄中包含了对象实例数据与对象类型数据各自的具体地址信息**

**直接指针**

reference 中存储的直接就是对象的地址

使用句柄来访问的最大好处是 `reference` 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 `reference` 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。

**使用场景**

**句柄访问**：适用于内存管理要求高、需要频繁压缩和动态加载类的场景

比如：在嵌入式设备中，如智能家电、工业控制系统，内存资源有限且需要高效管理；在构建一个具有插件机制的应用程序时，需要频繁加载和卸载插件类

**指针访问：**适用于性能要求高、需要快速对象访问的场景

比如：在游戏引擎中，需要实时渲染和操作大量游戏对象

### 在JVM变量分布在哪里？

**线程中的变量分布**

**局部变量**：

- 存储在虚拟机栈的局部变量表中。

**对象实例变量**：

- 存储在堆中。

**类变量（静态变量）**：

- 存储在方法区中。

### 检查gc指标有哪些?gc次数/gc持续时间

**常见 GC 指标**：

GC 频率、GC 持续时间、GC 总时间、GC 触发原因

GC 的频率应该尽量低，通常每分钟几次或者更少

年轻代的 GC（Minor GC）通常会比较频繁，因为年轻代的对象生命周期较短。正常情况下，Minor GC 的持续时间应该较短，通常在几十毫秒到几百毫秒之间。

老年代的 GC（Full GC 或 Major GC）应尽量少发生。正常情况下，Full GC 的持续时间应小于 1 秒

### 新生代比例

8：1：1

### GC种类？minorGC/FulIGC？

两种，**Minor GC** 和 **Full GC**

Minor GC 指的是对 **年轻代（Young Generation）** 进行的垃圾回收，当年轻代的 Eden 区满了，JVM 会触发 Minor GC，频繁发生，但每次发生的垃圾回收时间较短

Full GC 指的是对整个 **堆内存（包括年轻代和老年代）** 进行的垃圾回收。老年代的内存不足，或者显式调用 `System.gc()`，执行时间较长，发生频率较低。

### 类的生命周期（类加载过程）

类加载过程：**加载->连接->初始化**

连接过程：**验证->准备->解析**

首先是 **加载阶段** 。这个阶段的主要任务是把类的`.class`文件从各种介质，如文件系统、网络或其他来源读入到JVM的方法区中。加载过程是通过类的全限定名来获取定义此类的二进制流，然后将这个字节流所代表的静态存诸结构转化为方法区的运行时数据结构，最后在内存中生成一个代表这个类的`java.lang.class`对象，作为方法区这个类的各种数据的访问入口。

加载这一步主要是通过 **类加载器** 完成的，具体是哪个类加载器加载由 **双亲委派模型** 决定

接下来是 **验证阶段** 。这一阶段的目的是为了确保被加载的类的正确性，防止因类文件被篡改或损坏而导致系统崩溃。验证阶段会进行文件格式验证、元数据验证、字节码验证和符号引用验证等。

**准备阶段** 是紧随验证阶段之后的，它的主要任务是为类的**静态变量分配内存**，并将其初始化为默认值。这里需要注意的是，仅为静态变量分配内存，实例变量将会在对象实例化时随着对象一起分配在堆中。

然后是 **解析阶段** 。解析阶段的任务是把类中的**符号引用转换为直接引用**。符号引用是以一组符号来描述所引用的目标，而直接引用是可以直接指向目标的指针、相对偏移量或一个能间接定位目标的句柄。如果符号引用指向的是一个方法，那么这个符号引用会被解析为该方法的直接引用。

接下来是 **初始化阶段** 。初始化阶段是执行类构造器方法`clinit()`的过程。此方法由编译器自动收集类中的所有类变量的赋值动作和静态代码块集合来的。当初始化一个类的时候，如果发现其父类还没有进行过初始化，则需要先触发其父类的初始化。虚拟机会保证一个类的`clinit()`方法在多线程环境中被正确地加锁和同步。当遇到 `new`（创建一个类的实例）、 `getstatic`（程序访问类的静态变量）、`putstatic`（给类的静态变量赋值） 或 `invokestatic` （调用类的静态方法）这 4 条字节码指令时，会初始化类。

**使用阶段** 就是类的实际应用过程，包括创建类的对象、调用类的方法等。这个阶段没有特别的操作，主要是根据程序的需求来使用类。

最后是 **卸载阶段** 。当一个类不再被使用时，或者系统内存不足时，JVM就会回收其占用的空间，这个过程就是类的卸载，就是该类的Class对象被GC。类的卸载是由垃圾回收器来完成的，它会检测类是否还在被使用，如果没有被使用，就会将其卸载以释放内存空间。

### 类加载器？双亲委派模型？打破双亲委派模型？类加载器类型有哪些？

**类加载器**

类加载器是Java虛拟机(JVM)的一部分，主要作用就是加载 Java 类的字节码（ `.class` 文件）到 JVM 中（在内存中生成一个代表该类的 `Class` 对象）。

Java中的类加载器主要分为三种类型：

**启动类加载器**(Bootstrap Class Loader)：这是JVM的一部分，最顶层的加载类，它是由C++实现的，因此在Java代码中无法直接获取，所以通常表示为 null，并且没有父级，主要用来加载 JDK 内部的核心类库（ `%JAVA_HOME%/lib`目录下的 `rt.jar`、`resources.jar`、`charsets.jar`等 jar 包和类）以及被 `-Xbootclasspath`参数指定的路径下的所有类。

**扩展类加载器**(Extension Class Loader)：它用于加载Java的扩展类库，这些类库位于JRE的	`lib/ext`日录下。扩展类加载器是由启动类加载器加载的Java类

**应用程序类加载器**(Application Class Loader)：也称为系统类加载器，是加载应用程序类的默认类加载器。它负责加载应用程序`classpath`下的类，包括用户自定义的类和第三方库

**双亲委派模型：**

双亲委派模型是Java的类加载机制，用于保证Java类的稳定性和安全性。当类加载器收到类加载请求时，它首先会将这个请求委派给父类加载器去完成，只有当父类加载器无法完成加载时，才会由当前类加载器自己去加载，这个模型的工作流程如下

- 在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载（每个父类加载器都会走一遍这个流程）。

- 类加载器在进行类加载的时候，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成（调用父加载器 `loadClass()`方法来加载类）。这样的话，所有的请求最终都会传送到顶层的启动类加载器 `BootstrapClassLoader` 中。

- 只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载（调用自己的 `findClass()` 方法来加载类）。

- 如果子类加载器也无法加载这个类，那么它会抛出一个 `ClassNotFoundException` 异常。

双亲委派模型可以**防止类的重复加载**，保证了类加载的唯一性和一致性，同时也能够防止恶意的代码注入和破坏。

**如何打破双亲委派模型**

自定义类加载器：通过自定义类加载器，我们可以自定义类的加载顺序和逻辑，从而实现对双亲委派模型的改变。

自定义加载器的话，需要继承 `ClassLoader` 。如果我们不想打破双亲委派模型，就重写 `ClassLoader` 类中的 `findClass()` 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要**重写 `loadClass()` 方法**

### JVM线上问题排查和性能调优(排查OOM问题 排查频繁GC问题)

**如何定位排查OOM问题？**

当Java虚拟机(JVM)出现`OutOfMemoryError`(OOM)时，通常意味着JVM没有足够的内存空间来分配新对象。OOM可能由多种原因引起，包括但不限于内存泄漏、过大的垃圾回收开销、不恰当的堆大小设置等。为了排查和定位OOM问题，我们可以按照以下步骤进行：

1. 收集错误信息
   - 当OOM发生时，JVM通常会输出堆栈跟踪信息和堆转储(heap dump)。这些信息对于后续的问题分析至关重要。
   - 如果没有自动生成这些信息，可以通过添加JVM参数来启用。例如，使用`-XX:+HeapDumpOn0utofMemoryError` 参数可以在OOM时自动输出堆转储文件。
2. 初步分析
   - 查看OOM的错误类型，如`Java heap space`、`GC overhead limit exceeded`、`Permgen space`等，这有助于缩小问题范围。
   - 根据错误类型，可以初步判断OOM的原因。例如，`Java heap space`通常意味着堆内存不足，可能是内存泄漏或堆大小设置不当导致的。
3. 使用工具进行深入分析
   - `jVisualVM`和`jConsole`：这两个工具可以实时监控JVM的性能指标，包括内存使用情况、线程状态和垃圾回收情况等。它们有助于发现内存泄漏和性能瓶颈。
   - `MAT(Memory Analyzer Tool)`：通过分析堆转储文件，MAT可以帮助我们找到潜在的内存泄漏和占用大量内存的对象。
   - `GCViewer`或`GCEasy`：这些工具可以分析GC日志，查看垃圾收集器的行为和效率，从而判断是否存在过度的垃圾回收或内存泄漏等问题。
4. 定位和排查问题
   - 内存泄漏定位：使用MAT等工具打开堆转储文件，查找占用内存最大的对象。通过分析这些对象的引用链可以确定是否有对象不应该被保留但却无法被GC清理，即内存泄漏。
   - 检查代码：审查代码中是否存在不必要的大对象分配或内存泄漏的情况。特别注意那些创建了大量对象或使用了大量内存的代码段。
   - 调整配置：根据应用的需求和初步分析的结果，调整JVM的配置，如堆大小、垃圾回收策略等。有时候，简单的配置调整就能解决OOM问题。
5. 解决方案实施与验证
   - 在定位到具体问题后，采取相应的解决措施，如修复内存泄漏、优化代码、调整JVM配置等
   - 实施解决方案后，需要进行充分的测试以验证问题是否已得到解决。这包括功能测试、性能测试以及长时间的稳定性测试。

**如何定位排查频繁GC的问题?**

首先，我们要明确GC的频繁发生可能会导致应用程序性能下降，甚至出现停顿现象，严重影响用户体验。因此，及时定位和解决这个问题至关重要。

1. 第一步，确认GC问题的存在。
   - 通过监控工具或日志分析，观察GC的频率和持续时间是否异常
   - 注意应用程序的性能是否出现明显下降，如响应时间延长、吞吐量降低等。
2. 收集诊断信息。
   - 开启并收集JVM的GC日志。这些日志会详细记录每次GC发生的时间、类型、持续时间和回收的对象数量等信息。
   - 命令：`jstat`、`jmap`、`jstack`
   - 使用jstat、jmap等工具获取JVM的运行时状态，包括堆内存的使用情况、各个内存池的大小和使用率等
3. 分析诊断信息
   - 通过分析GC日志，找出GC频繁发生的原因。可能是堆内存不足、内存泄漏、代码中存在大量临时对象的创建等。
   - 使用jmap等工具生成的堆转储文件(heap dump)来分析内存中的对象分布，找出占用内存大的对象或疑似内存泄漏的对象。
4. 调整和优化
   - 根据分析结果，可以尝试调整JVM的参数配置，如堆大小、新生代与老年代的比例等，以优化GC性能。
   - 检查代码中是否存在不必要的对象创建或长时间持有的对象，导致内存占用过高。如果有，考虑优化代码逻辑，减少内存消耗。

### 使用jmap

用ps命令查看目标进程的PID

使用jmap可以来查看堆内存使用情况`jmap -heap<pid>`、生成堆转储文件`jmap -dump:format=b,file=<file_name>.hprof <pid>`

### Tomcat打破了双亲委派

Tomcat 作为一个 servlet 容器，需要能够动态地加载和卸载 web 应用程序中的类，这与 Java 的双亲委派模型有一些冲突。

在 Tomcat 中，每个 web 应用都有自己的类加载器（`WebappClassLoader`），它们之间互相隔离，保证一个 web 应用的类不会影响到另一个 web 应用。Tomcat 的 `WebappClassLoader` 先尝试自己加载类，如果加载失败才委派给父类加载器。

实现了**隔离性**：确保每个 web 应用都能使用自己版本的库，不受其他应用影响。

## Java：SSM

### spring Ioc?

**IoC 是一个原则，一种解耦的设计思想**

Spring的IOC，也就是控制反转，是Spring框架的核心特性之一。要理解IOC，我们首先需要了解两个概念：**控制反转和依赖注入**。

控制反转，即Inversion of Control，简称IoC。它的主要思想是将原本由代码直接操控的对象的调用权交给第三方，比如一个容器来控制，以解耦代码。在传统的程序设计中，我们直接在对象内部通过 new 关键字来创建依赖的对象，这种方式会导致代码之间高度耦合，不利于测试和维护。而IoC的思想是，将这些对象创建和绑定的控制权交由Spring容器来管理，从而降低系统的耦合性。在 Spring 中， IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个 Map（key，value），Map 中存放的是各种对象。

具体来说，在Spring中实现IoC主要有两种方式：基于XML的配置和基于注解的配置。在XML配置中，我们可以在Spring的配置文件中，通过bean元素来定义需要Spring管理的对象，并通过property元素来注入依赖。而在注解配置中，可以使用如`@Autowired`、`@Resource`等注解来自动注入依赖。

Spring loC容器负责管理Bean的生命周期、依赖注入等。我们只需要配置好相关Bean就可以轻松实现对象之间的解耦和依赖管理。总的来说，IoC的思想和Spring的IoC容器极大地提高了代码的灵活性和可维护性，降低了系统的复杂度

另外，Spring IoC还提供了很多高级特性，如Bean的作用域控制、生命周期回调、事件传播等，这些特性可以帮助我们更好地管理和控制Bean的行为。

总的来说，我对Spring IoC的理解是，它是一种强大的解耦和管理对象依赖关系的工具通过把对象的创建和依赖关系的控制权交给Spring容器来管理，从而提高了代码的灵活性和可维护性。

### Spring主要设计思想

Spring 是一款开源的轻量级 Java 开发框架，旨在提高开发人员的开发效率以及系统的可维护性。Spring 最核心的思想就是不重新造轮子，开箱即用，提高开发效率。

Spring 提供的核心功能主要是 IoC 和 AOP。

控制反转（IoC）：Spring 通过依赖注入实现 IoC。

面向切片编程（AOP）

### spring事务？底层原理？

Spring 支持两种方式的事务管理，编程式事务管理，声明式事务管理

**底层原理**

**事务管理器**：负责处理事务的提交、回滚等操作

**事务代理**：Spring 使用 **AOP**（面向切面编程）创建一个事务代理，来拦截标记为事务的方法。在方法执行前，事务代理会开启一个事务，在方法执行后，代理会提交或回滚事务。

**事务属性**：在声明式事务管理中，事务属性定义了事务的行为，包括传播行为、隔离级别、超时设置等

### Spring事务标签？生效/失效场景？

Spring 的事务管理标签主要用于在 XML 配置文件中配置声明式事务管理。最常用的标签是 `<tx:advice>` 和 `<tx:annotation-driven>`。

- `<tx:advice>` 标签用于定义事务通知，并将其应用于特定的切点（pointcut）。通常与 AOP 配合使用。

  ```XML
  <tx:advice id="txAdvice" transaction-manager="transactionManager">
      <tx:attributes>
          <tx:method name="get*" read-only="true"/>
          <tx:method name="save*" propagation="REQUIRED"/>
          <tx:method name="*" propagation="REQUIRED"/>
      </tx:attributes>
  </tx:advice>
  
  <aop:config>
      <aop:pointcut id="serviceOperation" expression="execution(* com.example.service.*.*(..))"/>
      <aop:advisor advice-ref="txAdvice" pointcut-ref="serviceOperation"/>
  </aop:config>
  ```

- `<tx:annotation-driven>` 标签适用于简化的声明式事务管理，通过注解方式管理事务，适用于大部分场景     

         ```XML
         <tx:annotation-driven transaction-manager="transactionManager"/>
         ```

  ```JAVA
  @Service
  public class MyService {
  
      @Transactional(readOnly = true)
      public void readOnlyMethod() {
          // 只读事务方法
      }
  
      @Transactional(propagation = Propagation.REQUIRED)
      public void requiredMethod() {
          // 必须事务方法
      }
  }
  ```

**生效条件**

1. **在 Spring 容器中配置了事务管理器**：需要定义一个事务管理器（例如 `DataSourceTransactionManager`）。
2. **启用了事务管理支持**：通过 `<tx:annotation-driven>` 或 `<tx:advice>` 标签启用。

### Spring事务注解（Transactional注解）用过吗？什么场景会生效/失效？SpringAOP自调用？

我用过`@Transactional(rollbackFor = Exception.class)`注解

`@Transactional`的工作机制是基于 AOP 实现的，AOP 又是使用动态代理实现的。如果目标对象实现了接口，默认情况下会采用 JDK 的动态代理，如果目标对象没有实现了接口,会使用 CGLIB 动态代理

当 `@Transactional` 注解作用于类上时，该类的所有 public 方法将都具有该类型的事务属性

`@Transactional` 注解默认回滚策略是只有在遇到`RuntimeException`(运行时异常) 或者 `Error` 时才会回滚事务，而不会回滚 `Checked Exception`（受检查异常）。这是因为 Spring 认为`RuntimeException`和 Error 是不可预期的错误，而受检异常是可预期的错误，可以通过业务逻辑来处理

如果想要修改默认的回滚策略，可以使用 `@Transactional` 注解的 `rollbackFor` 和 `noRollbackFor` 属性来指定哪些异常需要回滚，哪些异常不需要回滚。

**生效场景**

- `@Transactional` 注解只有作用到 public 方法上事务才生效，不推荐在接口上使用
- **避免同一个类中调用 `@Transactional` 注解的方法，这样会导致事务失效**
- 正确的设置 `@Transactional` 的 `rollbackFor` 和 `propagation` 属性，否则事务可能会回滚失败
- 被 `@Transactional` 注解的方法所在的类必须被 Spring 管理，否则不生效
- 底层使用的数据库必须支持事务机制，否则不生效

**Spring AOP 自调用问题**

当一个方法被标记了`@Transactional` 注解的时候，Spring 事务管理器只会在被其他类方法调用的时候生效，而不会在一个类中方法调用生效。

这是因为 Spring AOP 工作原理决定的。因为 Spring AOP 使用动态代理来实现事务的管理，它会在运行的时候为带有 `@Transactional` 注解的方法

生成代理对象

，并在方法调用的前后应用事物逻辑。如果该方法被其他类调用我们的代理对象就会拦截方法调用并处理事务。但是在一个类中的其他方法内部调用的时候，我们代理对象就无法拦截到这个内部调用，因此事务也就失效了

### 设计SpringIoC

简单来理解IoC就是：

1. 找到bean

   找到bean在什么地方，是对BeanDefinition的资源定位，是由ResourceLoader通过统一的Resource接口来完成，这个接口对各中形式的Resource都提供了统一接口，比如Xml，比如annotation。而这些都是由ResourceLoader来完成的

   **扫描这个项目，将所有类的文件名.Class存在l链表classNames中**

2. 载入并注册bean

   找到bean后，将bean注册到我们的IOC容器中。Spring是通过一些ApplicationContext来完成的，比如FileSystemXmlApplicationContext, ClassPathXmlApplicationContext以及我们最常见的XmlWebApplicationContext，读取之后将bean注册到IOC容器中，简单来说，就是把读取的bean都放到一个map中。

   然后根据classNams中的文件名反射得到@Controller和@Service 所注释的类的Class对象，然后将对象放入了instanceMap中，这些Class对象所对应的key就是@注释中的值。其实说白了就是就是将Controller层和Service层中的类的Class对象存在了在Map中，然后用@注释名来当Map的key

3. 注入bean

   循环遍历出每个instanceMap中Class对象的变量，判断这个变量上面有没有@Qualifer注解，有的化则拿到@qualifer的注解值，然后通过这个值在instanceMap中对应的Class对象更新（注入）到参数上去。

### Spring扩展点？比如beanfactory，init方法这些？

Spring 框架提供了许多扩展点，允许开发者在不同的生命周期阶段进行自定义操作。这些扩展点涵盖了 Bean 的创建、初始化、销毁等多个环节。

比如

1. 实现 `InitializingBean` 接口的 Bean 会在其属性设置完毕后调用 `afterPropertiesSet` 方法。可以在该方法中执行一些初始化操作。

2. 实现 `DisposableBean` 接口的 Bean 会在销毁时调用 `destroy` 方法。可以在该方法中执行一些清理操作。

3. 实现 `FactoryBean` 接口的类用于创建复杂的 Bean 实例。可以自定义 Bean 的创建逻辑。

4. 可以使用 `@PostConstruct` 注解在 Bean 初始化完成后执行方法，使用 `@PreDestroy` 注解在 Bean 销毁前执行方法。


### Bean生命周期（实例化-属性赋值-初始化-销毁）

Spring Bean的生命周期大体上可以分为四个阶段：实例化、属性赋值、初始化、销毁

**实例化阶段：**

这个阶段主要是通过反射技术创建Bean的实例。Spring会根据配置文件中Bean的定义利用`Java Reflection`（反射）技术创建对应的实例。

**属性赋值阶段：**

实例化后的对象被封装在`BeanWrapper`对象中，此时对象仍然是一个原生状态，并没有进行依赖注入。Spring根据`BeanDefinition`中的信息进行依赖注入，通过`populateBean`方法完成属性的注入。此外，如果Bean实现了`xxxAware`接口，Spring会调用相应的set方法注入相应的内容，如`BeanNameAware`、`BeanClassLoaderAware`、`BeanFactoryAware`和`ApplicationContextAware`等

**初始化阶段：**

初始化阶段是Bean生命周期中非常关键的一个环节，它包含了多个扩展点。

1. **执行Aware接口的方法**：如果Bean实现了相应的Aware接口，Spring会调用这些接口定义的方法，如`setBeanName`、`setBeanClassLoader`、`setBeanFactory`和`setApplicationContext`等。
2. **执行`BeanPostProcessor`的前置处理：**如果Bean实现了`BeanPostProcessor`接口，Spring会在初始化方法调用前，先调用其`postProcessBeforeInitialization`方法，这是用户自定义处理逻辑的一个重要扩展点。
3. **执行`InitializingBean`接口的方法**：如果Bean实现了`InitializingBean`接口，Spring会调用其`afterPropertiesSet`方法，用户可以在这个方法中进行一些初始化操作。
4. **执行自定义的`init-method`**：如果在Spring的配置文件中为Bean指定了`init-method`属
   性，Spring会自动调用这个指定的初始化方法。
5. **执行`BeanPostProcessor`的后置处理**：在初始化方法调用后，如果Bean实现了`BeanPostProcessor`接口，Spring还会调用其`postProcessAfterInitialization`方法，这是另一个用户自定义处理逻辑的扩展点。

**销毁阶段：**
当Bean不再需要时，如果Bean实现了`DisposableBean`接口，Spring会调用其`destroy`方法进行销毁前的清理工作。同时，如果在配置文件中指定了`destroy-method`属性，Spring也会在销毁前调用这个方法。

综上所述，Spring Bean的生命周期包括实例化、属性赋值、初始化和销毁四个阶段，其中每个阶段都提供了不同的扩展点，允许用户自定义处理逻辑。这些扩展点增强了Spring框架的灵活性和可扩展性，使得开发者能够更加灵活地控制Bean的生命周期行为。

### SpringBoot原理(Springboot启动机制)

Spring Boot 是基于 Spring 框架的一个项目

1. **自动配置**

2. 约定优于配置

   Spring Boot 提供了许多默认配置和行为，开发者可以通过属性文件（如 `application.properties` 或 `application.yml`）来覆盖这些默认配置。

简化了 Spring 应用程序的开发、配置和部署。它的核心理念是通过合理的默认配置减少开发者的工作量

详细说一下自动装配：

首先，自动装配是指Spring Boot通过分析项目的依赖和配置，自动配置Spring应用程序所需的组件，而开发者无需手动配置大量的XML配置文件或Java代码。这大大简化了Spring应用的开发和配置过程。

那么，Spring Boot是如何实现自动装配的呢?这主要得益于以下几个关键部分：

1. `@SpringBootApplication`注解：这是启动Spring Boot应用的核心注解，它实际上是一个复合注解，包括了`@Configuration`、`@EnableAutoConfiguration`和`@Componentscan`等。其中，`@EnableAutoConfiguration`是实现自动装配的关键。
2. `@EnableAutoConfiguration`注解：这个注解告诉Spring Boot基于你所添加的 jar 依赖项自动配置你的Spring应用。例如，如果你在 `classpath`下有`HSQLDB`，那么SpringBoot就会自动配置一个内存数据库。这个注解内部使用了`@Import(AutoConfigurationImportSelector.class)`，这是实现自动装配的核心类。
3. `AutoConfigurationImportSelecto`r类：这个类负责加载`META-INF/spring.factories`文件中配置的自动装配类。Spring Boot在启动时，会实例化这个类，并加载配置文件中指定的自动装配类。这些类通常包含了一些Bean的定义和配置。
4. 条件装配：Spring Boot的自动装配还支持基于条件的装配。这意味着只有在某些条件满足时，特定的自动装配才会发生。这是通过`@Conditional`注解实现的。例如，如果`classpath`下有某个特定的类库，那么与之相关的自动装配就会被触发。
5. `SpringFactoriesLoader`：这是一个工具类，用于加载`META-INF/spring.factories`文件中的配置。这个文件包含了各种自动装配类的全限定名，供`AutoConfiqurationImportSelector`类加载和使用。

综上所述，Spring Boot的自动装配原理可以归纳为以下几个步骤：

- 通过`@SpringBootApplication`注解启动Spring Boot应用
- `@EnableAutoConfiguration`注解触发自动装配机制
- `AutoConfigurationImportSelector`类加载`META-INF/spring.factories`中配置的自动装配类
- 根据条件装配的规则，决定是否进行特定的自动装配
- 最后，通过`SpringFactoriesLoader`工具类加载并实例化自动装配类中的Bean

这就是Spring Boot自动装配的基本原理。通过这种方式，Spring Boot能够极大地简化Spring应用的开发和配置过程，提高开发效率。

### Spring、SpringBoot、SpringFramework设计模式

1. 单例模式

   Spring 容器默认会为每个 Bean 创建单例实例，Spring 通过 `ConcurrentHashMap` 实现单例注册表的特殊方式实现单例模式

2. 工厂模式

   Spring 使用工厂模式可以通过 `BeanFactory` 或 `ApplicationContext` 创建 bean 对象。

3. 代理模式

   Spring AOP（面向切面编程）和事务管理使用代理模式来增强对象的方法。Spring 提供了动态代理和 CGLIB 代理来实现 AOP

4. 模板方法模式

   Spring 提供了一些模板类，这些类封装了固定的流程和算法，只需实现特定步骤即可

5. 观察者模式

   Spring 事件机制使用观察者模式来实现事件的发布和订阅

6. 适配器模式

   Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller

7.  依赖注入模式

   DI(依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去

### Ioc注入bean（autowired/resource）？Bean生命周期管理？

注入bean实际上就是告诉IoC容器所管理的对象，有3种， **XML 文件、注解或者 Java 配置类**

**XML 配置**

XML 配置是 Spring 早期版本主要使用的方式

```xml
<!-- beans.xml -->
<!-- Constructor-arg with 'value' attribute -->
<bean id="..." class="...">
   <constructor-arg value="..."/>
</bean>
```

**注解配置**

- `@Component`：通用的注解，可标注任意类为 `Spring` 组件。如果一个 Bean 不知道属于哪个层，可以使用`@Component` 注解标注。

- `@Repository` : 对应持久层即 Dao 层，主要用于数据库相关操作。

- `@Service` : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。

- `@Controller` : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 `Service` 层返回数据给前端页面
- `@Autowired`：`@Autowired` 是 Spring 提供的注解，默认的注入方式为`byType`（根据类型进行匹配），可以通过 `@Qualifier` 注解来显式指定名称
- `@Resource`：`@Resource` 是 JDK 提供的注解，`@Resource`默认注入方式为 `byName`（根据名称进行匹配），可以通过 `name` 属性来显式指定名称。
- `@Inject`

**Java配置类**

Java 配置类是使用 Java 代码来进行配置

使用 `@Configuration` 和 `@Bean` 注解来定义和配置 Bean

`@Bean` 注解通常是我们在标有该注解的方法中定义产生这个 bean

`@Bean` 注解比 `@Component` 注解的自定义性更强，而且很多地方我们只能通过 `@Bean` 注解来注册 bean。比如当我们引用第三方库中的类需要装配到 `Spring`容器时，则只能通过 `@Bean`来实现。

Bean 的生命周期由 Spring 管理，由以下几个角色来实现

- **`BeanFactory` 和 `ApplicationContext`**：负责创建和管理 Bean 的生命周期。

- **`BeanPostProcessor`**：在 Bean 初始化前后进行自定义逻辑处理。

- **`BeanFactoryPostProcessor`**：在容器初始化之后修改 Bean 工厂的配置。

- **`InitializingBean` 和 `DisposableBean`**：允许在 Bean 初始化和销毁时进行自定义逻辑处理。

- **`@PostConstruct` 和 `@PreDestroy`**：用于定义初始化和销毁时的回调方法。

### Bean作用域？

- **singleton** : IoC 容器中只有唯一的 bean 实例。Spring 中的 **bean 默认都是单例的**，是对单例设计模式的应用。

- **prototype** : 每次获取都会创建一个新的 bean 实例。也就是说，连续 `getBean()` 两次，得到的是不同的 Bean 实例。

- **request** （仅 Web 应用可用）: 每一次 HTTP 请求都会产生一个新的 bean（请求 bean），该 bean 仅在当前 HTTP request 内有效。

- **session** （仅 Web 应用可用） : 每一次来自新 session 的 HTTP 请求都会产生一个新的 bean（会话 bean），该 bean 仅在当前 HTTP session 内有效。

- **application/global-session** （仅 Web 应用可用）：每个 Web 应用在启动时创建一个 Bean（应用 Bean），该 bean 仅在当前应用启动时间内有效。

- **websocket** （仅 Web 应用可用）：每一次 WebSocket 会话产生一个新的 bean。

### Spring优势

1. IoC控制反转，依赖注入
2. 面向切面编程（AOP）
3. 通过注解简化了配置，通过**注解**，如 `@Component`、`@Autowired`、`@Configuration` 等，减少了 XML 配置的复杂性
4. 提供了强大的 Web 开发支持，包括 Spring MVC、Spring WebFlux 和 Spring Boot，简化了 Web 应用和微服务的开发
5. 轻量级

### 注解是怎么生效的？

Spring Boot 注解的底层生效机制涉及到 Spring 框架的核心功能，包括注解处理、类路径扫描和条件配置等

1. `@SpringBootApplication` 是一个组合注解，包括 `@Configuration`、`@EnableAutoConfiguration` 和 `@ComponentScan`。这三个注解通过不同的机制共同作用，来启动和配置 Spring Boot 应用
   - `@Configuration`：表示这个类是一个配置类，等同于传统的 XML 配置文件
   - `@EnableAutoConfiguration`：启用 Spring Boot 的自动配置机制，尝试根据类路径下的依赖来自动配置 Spring 应用
   - `@ComponentScan`：启用组件扫描，在指定的包及其子包中寻找带有 `@Component`、`@Service`、`@Repository`、`@Controller` 等注解的类，并将它们注册为 Spring Bean
2. `@EnableAutoConfiguration`使 Spring Boot 根据应用的类路径下的依赖自动配置 Spring 应用上下文。这是 Spring Boot 自动配置的核心注解。它使用 `spring.factories` 文件来加载自动配置类，该文件列出了所有需要自动配置的类
3. 然后就是一些注入bean的注解（这里参考Q9详细回答），比如`@bean`、`@Component`
4. 然后是使用注解注入，比如`@Autowired`

### Spring AOP(连接点 目标 通知 代理 切入点)

AOP就是**面向切面编程**，它可以理解为一种思想，Spring AOP是一种常见的AOP实现，它是Spring框架中的一个重要内容，也是OOP(面向对象编程)的一种延续。与OOP关注对象不同AOP关注的是在程序运行中的某个切入点进行特定的操作，比如日志记录、事务管理等

关于Spring AOP，我认为可以从以下几个方面来理解：

一、**AOP的作用和优势**

AOP的主要作用是在不修改源代码的情况下，对方法进行功能增强。这使得我们可以在不改变原有业务逻辑的基础上，动态地添加一些额外的功能，如日志记录、性能监控等。其优势主要体现在减少重复代码，提高开发效率，同时便于后期维护。

二、**AOP的关键概念**

在AOP中，有几个关键的概念需要理解。首先是连接点(Joinpoint)，它指的是程序执行过程中的某个明确点，如方法的调用或异常的抛出。其次是目标(Target)，即被通知或被代理的对象，它完成具体的业务逻辑。通知(Advice)是在某个特定的连接点上执行的动作，它包含了具体的程序代码实现。代理(Proxy)是将通知应用到目标对象后创建的对象，它结合了目标和通知的功能。最后是切入点(Pointcut)，它定义了通知应该应用到哪些连接点。

三、**AOP的实现方式**

在Spring中，AOP的实现主要依赖于动态代理技术。Spring提供了两种动态代理技术：JDK代理和CGLIB代理。JDK代理是基于接口的动态代理技术，它要求目标对象必须实现一个接口。而CGLIB代理是基于父类的动态代理技术，它可以对没有实现接口的目标对象进行代理。在运行时，Spring会根据目标对象的情况自动选择使用哪种代理技术。

四、**AOP的适用场景**

Spring AOP广泛应用于日志记录、事务管理、安全性检査、性能监控、异常处理、缓存管理以及参数校验等场景。这些场景中的功能都可以通过AOP在不修改原有业务逻辑的情况下动态添加进来。

总的来说，Spring AOP提供了一种灵活且强大的方式来增强程序的功能，同时降低了代码的耦合度，提高了程序的可重用性和开发效率。在实际开发中，我们应该根据具体的业务需求和系统架构来合理利用Spring AOP的功能。

### Spring循环依赖怎么解决？（三级缓存）

首先，循环依赖指的是在Spring框架中，两个或多个Bean之间相互依赖，形成一个环状依赖的情况。例如，Bean A依赖于Bean B，而Bean B又依赖于Bean A，这样就形成了一个循环依赖。这种情况在复杂的系统中比较容易出现，特别是当模块之间的耦合度较高时

Spring容器通过其依赖注入机制来处理循环依赖问题。对于Singleton Bean，Spring容器会先实例化一个Bean，但并不立即进行依赖注入。当发现循环依赖时，Spring会利用一个三级缓存机制来解决这个问题，**三级缓存**其实就是三个 Map。首先，也就是如果发生循环依赖的话，就去 **三级缓存 `singletonFactories`** 中拿到三级缓存中存储的 `ObjectFactory` 并调用它的 `getObject()` 方法来获取这个循环依赖对象的前期暴露对象（虽然还没初始化完成，但是可以拿到该对象在堆中的存储地址了），并且将这个前期暴露对象放到二级缓存中，这样在循环依赖时，就不会重复初始化了！

然而，对于构造函数注入的循环依赖，Spring无法解决，因为这需要在实例化Bean之前满足所有的依赖关系。因此，如果存在构造函数注入的循环依赖，项目在启动时会报错。

**三、如何避免循环依赖**

为了避免循环依赖，可以采取以下措施：首先，尽量使用Setter注入而不是构造函数注入，因为Setter注入允许在Bean实例化后再设置依赖关系，从而更容易解决循环依赖问题。其次，合理设计系统架构和模块之间的依赖关系，降低模块之间的合度。例如，可以使用接口或抽象类来定义模块之间的交互方式，从而减少直接依赖。最后，利用Spring的`@Lazy`注解来延迟Bean的初始化，这样也可以在一定程度上避免循环依赖问题的发成。

### 解决循环依赖的三级缓存？

**一级缓存（singletonObjects）**：存放最终形态的 Bean（已经实例化、属性填充、初始化）。一般情况我们获取 Bean 都是从这里获取的

**二级缓存（earlySingletonObjects）**：存放过渡 Bean（半成品，尚未属性填充），也就是三级缓存中`ObjectFactory`产生的对象，与三级缓存配合使用的，可以防止 AOP 的情况下，每次调用`ObjectFactory#getObject()`都是会产生新的代理对象的。

**三级缓存（singletonFactories）**：存放`ObjectFactory`，`ObjectFactory`的`getObject()`方法（最终调用的是`getEarlyBeanReference()`方法）可以生成原始 Bean 对象或者代理对象

1. 先去 **一级缓存 `singletonObjects`** 中获取，存在就返回；

2. 如果不存在或者对象正在创建中，于是去 **二级缓存 `earlySingletonObjects`** 中获取；

3. 如果还没有获取到，就去 **三级缓存 `singletonFactories`** 中获取，通过执行 `ObjectFacotry` 的 `getObject()` 就可以获取该对象，获取成功之后，从三级缓存移除，并将该对象加入到二级缓存中

### SpringMVC处理流程

![img](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/de6d2b213f112297298f3e223bf08f28.png)

一、**用户请求与前端控制器的接收**

1. 当用户发送一个请求时，这个请求首先会被SpringMVC的**前端控制器**`DispatcherServlet`捕获
2. `DispatcherServlet`会对请求的**URL进行解析**，获取请求的URI，并根据这个URI来决定后续的处理流程

二、**处理器映射与处理器执行链的获取**

1. DispatcherServlet会将**解析得到的URI发送给**`HandlerMapping`(**处理器映射器**)，由其找到**对应的`Handler`**(控制器中的具体方法)以及相关的拦截器，并将它们封装成一个`HandlerExecutionChain`(处理器执行链)返回。
2. 这个过程中，`HandlerMapping`会根据请求的URL和HTTP方法(如GET、POST等)来查找并匹配对应的Handler。

三、**处理器适配与执行**

1. **前端控制器**在获取到`HandlerExecutionChain`后，会**选择一个合适的`HandlerAdapter`(处理器适配器)来执行这个链中的`Handler`**。
2. **处理器适配器**会提取`Request`中的模型数据，填充`Handler`的入参，并**开始执行`Handler`**。在这个过程中SpringMVC还会进行一系列的数据转换、数据格式化和数据验证等工作。

四、**处理器执行结果与模型视图的返回**

1. **`Handler`执行完毕后，会返回一个`ModelAndView`对象给处理器适配器**。这个对象包含了处理的结果数据(模型)以及要展示的视图名称或视图对象。
2. 处理器适配器接收到`ModelAndView`后，会将其返回给前端控制器

五、**视图解析与渲染**

1. 前端控制器在接收到`ModelAndView`后，会请求对应的`ViewResolver`(**视图解析器**)进行解析。`ViewResolver`会根据`ModelAndView`中指定的视图名称找到对应的`View`(视图)对象，并返回给`DispatcherServlet`
2. 最后，由`View`对象负责将模型数据渲染到视图中，生成最终的响应页面，并返回给客户端用户。

### 动态代理怎么实现，AOP中动态代理怎么实现？如何改变类的字节码？（JDK动态代理/CGLIB动态代理）

动态代理是 Java 中的一种设计模式，允许在运行时创建代理类，代理类能够动态地拦截和处理方法调用。Java 中有两种主要的动态代理机制：

1. **JDK** 动态代理：`Proxy` 类会在运行时生成一个代理类，该类实现了目标类实现的所有接口，并且其方法调用被转发到 `InvocationHandler` 的 `invoke` 方法。
2. **CGLIB** 动态代理：通过继承目标类并重写其方法来实现代理。CGLIB 使用 ASM 库来生成和修改字节码。使用 `Enhancer` 类创建代理对象。

**AOP动态代理**

Spring AOP 根据目标对象实现了一个或多个接口，会使用JDK动态代理，否则使用CGLIB动态代理

1. **JDK 动态代理**：
   - Spring 使用 `ProxyFactory` 和 `JdkDynamicAopProxy` 创建代理对象。
   - 代理对象调用方法时，通过 `JdkDynamicAopProxy` 的 `invoke` 方法将调用委托给 `AdvisedSupport`，处理所有的拦截器和通知。

2. **CGLIB 动态代理**：
   - Spring 使用 `CglibAopProxy` 创建代理对象。
   - 代理对象调用方法时，通过 `CglibAopProxy` 的 `intercept` 方法将调用委托给 `AdvisedSupport`，处理所有的拦截器和通知。

改变类的字节码，可以使用 ASM 或 Javassist 库

**ASM**：

1. **读取类字节码**：
   - 使用 `ClassReader` 读取类的字节码。
2. **修改字节码**：
   - 使用 `ClassVisitor` 和 `MethodVisitor` 修改类的字节码。
   - 重写 `visitMethod` 方法，插入新的字节码指令。
3. **生成新字节码**：
   - 使用 `ClassWriter` 生成新的类字节码

**Javassist**：

1. **创建 ClassPool**：
   - `ClassPool` 管理所有的类。
2. **获取和修改类**：
   - 使用 `ClassPool.get` 获取目标类。
   - 使用 `CtMethod` 修改目标方法，插入新的代码。
3. **生成新字节码**：
   - 使用 `toClass` 方法生成新的类字节码。

### 动态代理/静态代理？代理模式/适配器模式？

静态代理在编译时就确定代理类，代理类与目标类实现相同的接口，然后调用实现类

动态代理在运行时生成代理类，代理类不需要实现接口，使用 `InvocationHandler` 或 `MethodInterceptor` 处理代理逻辑

**代理模式：**代理对象控制对原对象的访问。可以在访问原对象前后添加额外的操作。

**适配器模式：**将一个类的接口转换成另一个接口。主要解决接口不兼容问题。 

### AnnotaionConfigApplicationContext启动流程

1. 实例化 `AnnotationConfigApplicationContext`

   ```java
   AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext();
   ```

2. 注册配置类

   ```java
   context.register(AppConfig.class);
   ```

3. 刷新上下文

   ```java
   context.refresh();
   ```

   通过 `refresh()` 方法，Spring 框架执行了完整的上下文初始化流程，完成了配置类的解析、Bean 的注册和初始化、事件的广播等工作，最终启动了 Spring 应用。

### Spring在多线程保证线程安全的？

在 Spring 框架中，单例（`singleton`）作用域的 Bean 在多线程环境中是共享的，因此需要确保线程安全。

1. **创建 Bean 的同步**：Spring 会确保单例 Bean 的实例化是线程安全的。即使多个线程同时请求一个单例 Bean，Spring 也会确保只会有一个线程创建 Bean 实例，其他线程将等待创建完成，并返回同一个实例

2. 在创建和管理单例 Bean 时，Spring 使用同步块来确保创建过程的线程安全

3. 初始化和销毁

   **初始化**：Spring 容器在启动时会初始化所有的单例 Bean。容器的启动过程是线程安全的，确保在所有线程访问之前，所有的单例 Bean 都已经创建和初始化完成。

   **销毁**：在容器关闭时，Spring 会执行销毁回调（如 `@PreDestroy` 注解的方法）来清理单例 Bean 的资源。销毁过程也是线程安全的，确保不会出现并发问题。

### Mybatis设计模式

1. 工厂模式

   在Mybatis中比如`SqlSessionFactory`使用的是工厂模式

2. 单例模式

   在Mybatis中有两个地方用到单例模式，ErrorContext和LogFactory，其中ErrorContext是用在线程范围内的单例，记录该线程的执行环境错误信息，而LogFactory则是提供给整个Mybatis使用的日志工厂，用于获得针对项目配置好的日志对象。

3. 代理模式

   代理模式可以认为是Mybatis的核心使用的模式，正是由于这个模式，我们只需要编写`Mapper.java`接口，不需要实现，由Mybatis后台帮我们完成具体SQL的执行，Mybatis会在解析全局配置文件中的mapper标签时提前帮我们创建好所有mapper接口的代理对象

4. 适配器模式

   将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作

   MyBatis通过适配器模式定义了一套统一的日志接口供上层使用，在Mybatsi的logging包中，有一个Log接口，定义了四种日志级别，相比较其他的日志框架的多种日志级别显得非常的精简

5. 装饰器模式

   在mybatis中，缓存的功能由根接口`Cache（org.apache.ibatis.cache.Cache）`定义。整个体系采用装饰器设计模式，通过一系列的装饰器来对`PerpetualCache`永久缓存进行缓存策略等方面的控制

### MyBatis优点?JDBC步骤？

MyBatis 是一个基于 java 的持久层框架，它**内部封装了 jdbc**。

开发人员**只需要关注 sql 语句**，不需要处理加载驱动、创建连接等繁琐的过程。

MyBatis 通过 xml 或注解两种方式配置 sql 语句，然后执行 sql 并将结果映射为 java 对象并返回

**JDBC**

1. 加载JDBC驱动程序

   通过 `Class.forName()` ，加载并注册 JDBC 驱动程序类

2. 建立数据库连接

   使用 `DriverManager.getConnection()` 方法建立与数据库的连接。需要提供数据库的 URL、用户名和密码

3. 创建 Statement  对象

   使用 `Connection` 对象`createStatement()`方法创建 `Statement` 对象

4. 执行 SQL 查询或更新、

   使用 `Statement` 对象执行 SQL 语句

   ```java
   ResultSet resultSet = statement.executeQuery("SELECT * FROM users");
   ```

5. 关闭资源

   使用完数据库连接后，必须关闭 `ResultSet`、`Statement` 和 `Connection` 对象，以释放数据库资源。

### MyBatis的执行流程

1、获取sqlSessionFactory对象:

解析文件的每一个信息保存在Configuration中，返回包含Configuration的DefaultSqlSession；

注意：【MappedStatement】：代表一个增删改查的详细信息

2、获取sqlSession对象

返回一个DefaultSQlSession对象，包含Executor和Configuration;

这一步会创建Executor对象；

3、获取接口的代理对象（MapperProxy）

getMapper，使用MapperProxyFactory创建一个MapperProxy的代理对象

代理对象里面包含了，DefaultSqlSession（Executor）

4、执行增删改查方法

### lombok底层

Lombok就是能够通过添加注解自动生成一些方法，像最常使用的@Data注解，在没有写 get、set 方法的时候也能够使用其 get、set 方法

定义注解处理器的话，那么就需要继承 AbstractProcessor 类，继承父类的两个方法，

- init 方法：主要是获得编译时期的一些环境信息。
- process 方法：在编译时，编译器执行的方法。也就是我们写具体逻辑的地方。

而 Lombok 本质就是在 process 方法中实现了依据属性生成对应方法字节码的操作，底层就是用到了编译时注解的功能

# Redis

## Redis：架构

### Redis是单线程？单线程为什么快？（Redis为什么快）

**redis是单线程还是多线程?**

首先，关于Redis是单线程还是多线程的问题，这实际上取决于Redis的版本。在Redis的早期版本，比如3.x版本，它确实是单线程的。这里的单线程主要是指Redis处理客户端请求的主线程是单线程的，即网络IO和键值对读写是由一个线程来完成的。这种设计使得Redis能够避免多线程切换带来的开销，提高了执行效率，同时也避免了线程安全问题。

Redis的性能瓶颈不在于CPU，主要在内存和网络。

然而，在后续的版本中，Redis开始逐渐引入多线程的概念。例如，在Redis 4.x版本中虽然处理客户端请求的主线程仍然是单线程的，但一些辅助功能如异步删除等已经开始使用多线程。到了Redis 6.0版本，引入多线程主要是为了提高网络IO读写性能。这种多线程的设计主要是为了提高Redis的吞吐量，更好地利用现代多核CPU的性能，以及减少在写操作时的延迟，但是执行命令依旧是单线程的，所以不会出现线程安全问题。

**单线程为什么还这么快?**

**基于内存操作**：Redis的所有数据都存储在内存中，这使得数据的读写速度非常快，内存操作的延迟远远低于磁盘操作，因此Redis能够迅速响应客户端的请求

**简单的数据结构**：Redis的数据结构是专门设计的，这些简单的数据结构的查找和操作的时间复杂度大部分都是O(1)，这意味着无论数据量多大，操作的时间都是恒定的。

**IO多路复用和非阻塞IO**：Redis使用IO多路复用功能来监听多个socket连接客户端。这样，它就可以使用一个线程连接处理多个请求，从而减少了线程切换带来的开销并避免了IO阻塞操作。

**避免上下文切换**：由于Redis采用单线程模型(在早期版本中)，因此可以避免不必要的上下文切换和多线程竞争。这样可以省去多线程切换带来的时间和性能上的消耗，同时单线程也不会导致死锁问题的发生。

### Redis过期策略/内存淘汰策略（内存被打满）(过期删除策略)？

**过期删除策略**

Redis的过期删除策略主要涉及两种机制：惰性删除和定期删除。这两种策略各有特点，且被Redis结合起来使用，以实现内存的有效管理和CPU资源的合理利用。

首先是**惰性删除**策略。这种策略在Redis中的实现是，当某个键被访问时，Redis会检查该键是否设置了过期时间以及是否已过期。如果键已过期，Redis会删除该键并返回不存在给客户端。这种策略的优点在于它节约CPU性能，只在必要时进行删除操作。然而，它的缺点也很明显，就是如果某些过期键长时间没有被访问，它们会一直占用内存，造成资源浪费。

为了弥补惰性删除的不足，Redis还采用了**定期删除**策略。这种策略是Redis每隔一段时间主动对数据库进行一次检查，删除里面的过期键。具体实现上，Redis会随机抽取一定数量的键进行检查，并删除其中的过期键。这个过程的运行频率和检测数量都是可以通过配置进行调整的。定期删除策略的优点在于它可以有效地释放过期键占用的内存，减少内存浪费。同时，由于它的执行是定期的，也可以在一定程度上平滑CPU的使用率

总的来说，Redis通过结合惰性删除和定期删除两种策略，既保证了过期键的及时删除又避免了CPU资源的过度占用。这种设计使得Redis在处理大量数据时仍能保持良好的性能。

**内存淘汰策略**
Redis的内存淘汰策略，主要是用来处理内存不足的情况。当Redis使用的内存快要达到我们设定的最大值时，就需要用这些策略来决定哪些数据应该被删除，以释放出内存空间。

首先，有一种叫做`noeviction`的策略，这是Redis的默认设置。这种策略在内存满时不会淘汰任何数据，而是直接拒绝新的写入操作。这适合那些不能丢失任何数据的情况

然后，我们有`allkeys-lru`策略，它会淘汰最近最少使用的数据，不管这些数据有没有设置过期时间。如果你有很多数据，并且希望保留最常用的数据，这个策略就很有用。

还有`allkeys-lfu`策略，这个策略会淘汰访问频率最低的数据，它也是全局的，不考虑数据是否过期。

接下来是`volatile-lru`和`volatile-lfu`策略，它们只会淘汰那些设置了过期时间的数据。如果你用Redis来做缓存，并且只希望淘汰那些有过期时间的数据，这两种策略就很合适。

`volatile-ttl`策略则会淘汰设置了过期时间并且即将过期的数据，这样可以最大化利用缓存空间。

除了这些，Redis还有`allkeys-random`和`volatile-random`策略。`alkeys-random`会随机淘汰任意数据，而`volatile-random`则是随机淘汰设置了过期时间的数据。这两种策略在数据选择上没有明确的优先级，适合在数据访问模式没有明显规律或者需要保持数据随机性的场景下使用。

总的来说，选择哪种策略要看你的具体需求，比如数据的重要性、使用频率，以及你是否给数据设置了过期时间等。

### Redis网络模型

**单线程**：Redis 使用单线程来处理所有的客户端请求，这种设计简化了多线程编程的复杂性，避免了线程切换的开销。

**非阻塞 I/O**：通过非阻塞 I/O 和事件循环机制，Redis 能够同时处理多个客户端的请求，而不会因为某一个客户端的操作阻塞其他客户端。

### LRU/LFU

**LRU（最近最少使用）** 算法是缓存淘汰策略中的一种，核心思想是优先删除最近最少使用的数据，LRU 维护一个最近使用的顺序，当缓存满时，删除最久未被访问的元素，Redis 实现 LRU 的方式是通过**双向链表和哈希表**来跟踪每个键的使用情况，确保能够快速地找到和删除最少使用的键。

**LFU（最少使用频率）** 算法是另一种缓存淘汰策略，优先删除使用频率最低的数据，Redis 实现 LFU 的方式是为每个键维护一个访问计数器，确保能够快速地找到和删除使用频率最低的键。

## Redis：数据类型

### Redis常见数据类型(String hash set zset list)

**5种常用数据类型**

字符串 **String**是Redis最基本的数据类型，它可以存储任意类型的数据，包括文本、数字和二进制数据，且其值最大可以达到512MB。例如，在缓存场景中，我们可以将热点数据存储在Redis的字符串类型中，以提高数据的访问速度；在计数器场景中，我们可以利用Redis的原子性操作来实现高效、准确的计数功能。

哈希类型 **Hash**是一种键值对的集合，它适合存储对象的多个属性。在哈希类型中，每个键都与一个值相关联，这使得我们可以方便地获取或修改对象的某个属性。哈希类型的值最多可以包含2^32-1个键值对，这使得它能够存储大量的数据。哈希类型在存诸用户信息、配置信息等场景下非常有用。例如，我们可以将用户的个人信息存储在一个哈希值中，通过键值对的方式来访问和更新这些信息。

列表类型 **List** 是一种有序的字符串集合，它可以存储多个字符串值。列表类型支持在头部或尾部插入元素，以及获取指定范围内的元素等操作。这使得列表类型非常适合用于实现队列、栈等数据结构。例如，在消息队列场景中，我们可以利用列表类型来存储待处理的消息，并通过LPUSH和RPOP等命令来实现消息的入队和出队操作。

集合类型 **Set** 是一种无序、唯一的字符串集合。它支持添加元素、删除元素以及判断元素是否存在等操作。此外，集合类型还支持求交集、并集和差集等操作。这使得集合类型非常适合用于存储不重复的元素，并进行快速的集合运算。例如，在标签系统中，我们可以利用集合类型来存储用户的标签信息，并通过集合运算来查找具有相同标签的用户或者计算用户之间的相似度等。

有序集合类型 **ZSet** 是一种有序的字符串集合，每个元素都关联着一个分数用于排序。有序集合支持添加元素、删除元素以及获取指定范围内的元素等操作，并且还支持根据分数进行范围查找和计算元素的排名等操作。这使得有序集合非常适合用于排行榜、优先级队列等场景。例如，在排行榜场景中，我们可以利用有序集合来存储用户的得分信息，并通过ZRANK等命令来获取用户的排名情况。

**后增加4种**

首先是 **Bitmap** 数据类型，它是一种基于String类型实现的特殊数据结构，用于存储大量进制位(0或1)的数据。这些位可以代表不同的状态或标志。Bitmap非常适合用于签到打卡等场景，因为这类场景通常只需要记录用户是否进行了某个操作，例如签到(1)或未签到(0)。使用Bitmap可以极大地节省存储空间，并提高处理速度。

接下来是 **HyperLogLog** 数据类型，它是Redis2.8.9版本引入的，用于做数据基数计算 HyperLogLog 的优势在于它占用的内存非常少，并且不保存原始数据，而是通过一定的算法估计数据的基数。尽管它有一定的误差(标准误差为0.81%)，但在对数据准确度要不是特别高的场景下，如统计网站注册IP数、每日访问IP数或页面实时UV/PV数等，HyperLogLog是一个非常好的选择。

再来说说 **GEO** 数据类型，这是Redis 3.2版本新增的特性，用于存储经纬度格式的地理坐标，并对这些坐标执行距离计算、范围查找等操作。GEO数据类型非常适合用于地理位相关的业务场景，如各类社交软件中的" 附近的人 "功能、外卖配送服务中的距离计算和最优配送员分配等。

最后是 **Stream** 数据类型，它类似于消息队列或日志系统，可以记录一系列的事件或消息，并按照时间顺序进行存储。stream数据类型非常适合用于实时监控、大数据处理数据流转、实时数据分析和视频流处理等场景。例如，在实时监控中，可以使用Stream来记录服务器的日志或网络流量数据；在大数据处理中，可以使用Stream来对一批数据进行筛选、排序和统计等操作;在实时数据分析中，可以使用Stream来处理网站访问记录并了解用户行为。

### redis数据类型底层？

**String**类型。在Redis中，String 类型的底层的数据结构实现是 **SDS**，SDS 不仅可以保存文本数据，还可以保存二进制数据，SDS 获取字符串长度的时间复杂度是 O(1)，Redis 的 SDS API 是安全的，拼接字符串不会造成缓冲区溢出。根据字符串的具体情况（长度和类型），会选择 Embstr、Raw 或 Integer 来进行优化存储，如果字符串的内容可以解析为**整数**，Redis会将其编码为**int**类型，直接存储整数值。对于长度**较短**的字符串，Redis会使用 **embstr** 编码，这是一种紧凑的字符串表示方式。而对于长度**较长**的字符串，Redis则会使用 **raw** 编码，即简单的动态字符串。

**Hash**类型。在Redis 7中，使用了**`listpack`或`hashtable`**来实现。在这之前，Hash 类型的底层数据结构是由压缩列表或哈希表实现。哈希表是一种根据键直接访问在内存存储位置的数据结构，它通过计算哈希值来快速定位数据。如果哈希类型元素个数小于 512 个，所有值小于 64 字节的话，Redis 会使用压缩列表作为Hash 类型的底层数据结构。

**List**类型，Redis的底层实现主要使用了**QuickList**。在Redis 3.2版本之前，主要使用压缩列表和双向链表来实现，如果列表的元素个数小于 512 个，用**压缩列表，否则使用双向链表**。但是，在3.2版本之后，Redis统一只采用QuickList来实现List，它结合了LinkedList和ZipList的优点，提供了更高的存储上限和更好的性能。

Set类型，Set 类型的底层数据结构是由**哈希表**或**整数集合**实现的，如果集合中的元素都是整数且元素个数小于 512 ，会使用整数集合，否则使用哈希表

**ZSet**类型，底层数据结构是由**压缩列表或跳表和哈希表**实现的，如果有序集合的元素个数小于 128 个，并且每个元素的值小于 64 字节时，Redis 会使用压缩列表作为 Zset 类型的底层数据结构，否则使用跳表。Redis 7 之后压缩列表就被跳表替代了

### 压缩列表

压缩列表是 Redis 为了节约内存而开发的，它是由连续内存块组成的顺序型数据结构，有点类似于数组

表的头三个字段分别是：压缩列表**占用的内存**，**「尾部」节点距离起始地址**有多少字节，**节点数量**；所以如果我们要查找定位第一个元素和最后一个元素，可以通过表头三个字段（zllen）的长度直接定位，复杂度是 O(1)，其他就是O(N)

结点包含：**「前一个节点」的长度、当前节点实际数据的「类型和长度」、当前节点的实际数据**

当我们往压缩列表中插入数据时，压缩列表就会根据数据类型是字符串还是整数，以及数据的大小，会使用不同空间大小的 prevlen 和 encoding 这两个元素里保存的信息

但是他有一个缺陷就是会发生**连锁更新**，新增某个元素或修改某个元素时，如果空间不不够，压缩列表占用的内存空间就需要重新分配。而当新插入的元素较大时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起「连锁更新」问题，导致每个元素的空间都要重新分配，造成访问压缩列表性能的下降。

### zSET与跳表？跳表插入过程，随机层高？

**ZSET和跳表**

ZSET，即有序集合，是Redis提供的一种数据结构。它类似于Set，但其成员是唯一的且每个成员都关联着一个分数。这个分数用于对集合中的元素进行排序。ZSET在实现上采用了跳表(SkipList)和哈希表(HashTable)两种数据结构，其中跳表用于维护元素的排序，而哈希表则提供快速的元素查找。

跳表(SkipList)是一种随机化的数据结构，实质上就是一种可以进行二分查找的有序链表。跳表在原有的有序链表上面增加了多级索引，通过索引来实现快速查找。它不仅提高了搜索性能，同时也提高了插入和删除操作的性能。查询的时间复杂度是O(log n)，空间复杂度O(N)

**跳表的插入过程**

一、创建新节点并设置值

1. 根据待插入元素创建一个新的节点
2. 将新节点的值设置为待插入元素的值。
3. 生成一个随机的层数

二、查找插入位置

1. 从跳表的最高层开始，从左到右依次检查每个节点的下一个节点。
2. 在每一层中，沿着前进指针（forward pointer）向右移动，直到找到第一个大于等于 `score` 的节点
3. 将每一层中找到的前驱节点保存到 `update` 数组中

三、更新前驱节点的指针

​	在每一层中，将 `update[i]` 的前进指针指向新节点，新节点的前进指针指向 `update[i]` 原本指向的节点

四、更新跳表层数

​	如果新节点的层数 `lvl` 比当前跳表的最大层数大，则更新跳表的最大层数

这个过程确保了跳表在插入新元素后仍然保持有序，并且通过随机化的层高设计，使得跳表在查询、插入和删除操作时都能保持高效的性能。

总的来说，Redis中跳表的插入过程是一个结合了有序链表和二分查找思想的优化算法它通过增加多级索引和随机化的层高设计来提高操作效率。

**跳表节点层数设置**

首先，跳表节点的层数设置具有随机性。在Redis的跳表中，每个节点的层数是随机生成的，这种随机性有助于保持跳表的平衡，防止某一层级的节点过多，从而影响查询效率。具体来说，每个节点在初始化时都会被赋予一个基本的层数，通常是1层。然后，通过一个随机数生成器，决定该节点是否增加额外的层级。

其次，层数的增加遵循一定的概率分布规则。在Redis中，这个概率通常是由一个固定的概率因子P(25%)来控制的。每当需要增加一个新的层级时，系统会生成一个介于0和1之间的随机数p，并将其与概率因子P进行比较。如果p小于P，则节点的层数会增加一层。这个过程会重复进行，直到生成的随机数p大于或等于P为止。

值得注意的是，虽然每个节点的层数是随机设置的，但整体上跳表的层数分布是稳定的。这是由于大量的节点在层数设置上遵循相同的概率规则，因此从统计学的角度来看，跳表的层级结构会趋于稳定。

综上所述，跳表节点的层数设置是通过随机化和概率控制来实现的。

### 哈希表rehash（渐进式hash）

**rehash介绍**

当哈希表中的键值对数量超过一定阈值时，为了保证哈希表的性能和稳定性，Redis会自动进行rehash操作。具体来说，rehash就是将哈希表中的所有键值对重新散列到新的哈希表中。

在rehash过程中，Redis会创建一个新的哈希表，其大小通常是原哈希表大小的两倍(扩容时)。然后，Redis会将原哈希表中的所有键值对重新散列到新哈希表中。这个过程中，Redis会使用新的哈希表大小来计算每个键值对的新哈希值，并将其插入到新哈希表的相应位置。

**渐进式Rehash**

为了避免一次性rehash导致的长时间阻塞，Redis采用了渐进式rehash的策略。这种策略将rehash操作分散到多个指令的执行过程中，从而降低了对系统性能的影响。

在渐进式rehash过程中，Redis会维护两个哈希表：**原哈希表和新哈希表**。当触发rehash操作时，新的键值对会直接存储到新哈希表中，而原有的键值对则会在后续的指令执行过程中逐渐被迁移到新哈希表中。这个过程是逐步完成的，因此称为渐进式rehash。

**Rehash触发条件**

扩容条件：当哈希表中的键值对数量超过一定阈值时，会触发扩容rehash。具体来说，在Redis中，这个阈值通常是哈希表大小的某个比例(如负载因子超过某个值)。当达到这个阈值时，为了保证查询效率和性能，Redis会进行扩容操作。

### Redis命令：获取哈希表字段？判断hash中key是否存在？

获得哈希表字段`HGET`

```java
HGET myhash field1
# 返回值：Hello
```

判断key是否存在`HEXISTS`

```java
EXISTS myhash
# 返回值：1
```

### zset为什么要用跳表，而不用平衡树、红黑树或者 B+树？

**平衡树 vs 跳表**

跳表使用概率平衡而不是严格强制的平衡，因此，跳表中的插入和删除算法比平衡树的等效算法简单得多，速度也快得多

**红黑树 vs 跳表**

相比较于红黑树来说，跳表的实现也更简单一些。并且，按照区间来查找数据这个操作，红黑树的效率没有跳表高

**b+树 vs 跳表**

B+树更适合作为数据库和文件系统中常用的索引结构之一，它的核心思想是，通过可能少的 IO 定位到尽可能多的索引来获得查询数据，对于 Redis 这种内存数据库来说，它对这些并不感冒，因为 Redis 作为内存数据库它不可能存储大量的数据，跳表实现 zset 时相较前者来说更简单一些，也不需要像 B+树那样插入时发现失衡时还需要对节点**分裂与合并**

**使用跳表的原因**

1. 不是很占用内存，比 B+ 树更节省内存
2. 经常 `ZRANGE` 或 `ZREVRANGE` 操作的目标，以链表的方式遍历跳表，获取特定范围的元素效率高
3. 跳表的简单性，更好调试增强代码

### Redis中的跳表有多少层?

一般是建议不超过16层

跳表的层数是动态变化的，而不是固定的。跳表的层数通过一种随机算法来决定，每次决定是否增加新层时，使用了一个固定的概率（通常是 0.25），来决定是否增加当前层，然后继续使用相同的概率来决定是否增加下一层。这意味着，某一层的每个节点在上一层上随机增加的概率是 25%，因此对于每个节点，它增加到某一层的总概率是递减的。

## Redis：持久化

### Redis持久化（RDB AOF）

首先是**RDB**持久化，是Redis**默认**的持久化方式，也就是创建快照来获得存储在内存里面的数据在某个时间点上的副本。这种方式是通过在指定的时间间隔内，将Redis内存中的数据集**生成快照**并写入到一个**二进制文件**中，通常是一个名为dump.rdb的文件。RDB的优点是生成的文件较小，且恢复速度快。比如，我们可以通过配置如“save 60 1000“这样的参数，告诉Redis在60秒内有1000个键被修改时，就自动保存一次数据集。当然，我们也可以手动执行save或bgsave命令来生成快照。但RDB的缺点是它可能会丢失一些数据，因为它是基于时间点的快照，而不是实时的。

接下来是**AOF**持久化，即Append Only File。每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入到 **AOF 缓冲区**，然后再写入到 **AOF 文件**中，最后再根据持久化方式的配置来决定何时将系统内核缓存区的数据**同步到硬盘**中的。当Redis重启时，它会重新执行AOF文件中的命令来恢复数据。AOF的优点是数据安全性更高，因为它记录了所有的写操作。但缺点是文件体积可能会很大且恢复速度相对较慢。

除了单独的RDB和AOF持久化外，Redis还支持混合持久化。这种方式结合了RDB和AOF的优点。它首先使用RDB方式创建一个快照，然后将后续的写命令追加到AOF文件中，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样，在恢复数据时，Redis可以先加载快照以快速恢复到某个时间点，然后再通过执行AOF文件中的命令来恢复到最新状态。

### AOF先处理文件请求再写入文件？

- 避免额外的检查开销，AOF 记录日志不会对命令进行语法检查；
- 在命令执行完之后再记录，不会阻塞当前的命令执行

也带来了风险：

- 如果刚执行完命令 Redis 就宕机会导致对应的修改丢失；
- 可能会阻塞后续其他命令的执行（AOF 记录日志是在 Redis 主线程中进行的）。

### AOF刷盘策略（持久化方式）？

3中刷盘策略

`always`：每个写命令都会立即同步到磁盘。这种方式虽然最安全，但性能开销也最大，因为每次写操作都需要进行磁盘I/O。

`everysec`：每秒同步一次。这是默认且推荐的策略，它在数据安全性和性能之间达到了一个平衡。

`no`：由操作系统决定何时同步到磁盘。这种方式性能最好，但数据安全性最低。

### AOF瘦身（AOF重写）？

随着时间的推移，AOF文件会变得越来越大，这可能会影响到Redis的启动速度和恢复时间。为了解决这个问题，Redis提供了AOF重写(瘦身)的功能。AOF 文件重写期间，Redis 还会维护一个 **AOF 重写缓冲区**，该缓冲区会在子进程创建新 AOF 文件期间，记录服务器执行的所有写命令。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新的 AOF 文件保存的数据库状态与现有的数据库状态一致。最后，服务器用新的 AOF 文件替换旧的 AOF 文件，以此来完成 AOF 文件重写操作。这个过程可以在不中断服务的情况下进行，并且通常会显著减小AOF文件的大小。AOF重写可以通过手动触发(使用bgrewriteaof命令)或根据配置的自动触发条件(如文件大小增长比例和时间间隔)来执行。

### RDB/AOF存的是什么数据

RDB 文件存储的是 Redis 在某个时间点（快照）的全量数据。这包括所有的数据库、键、值、以及相关的元数据（如过期时间等）。

AOF 文件存储的是对 Redis 数据库所执行的所有写命令。这包括每一条对数据进行修改的命令，这些命令以追加的方式写入到 AOF 文件中。

### RDB恢复比AOF快

RDB：直接将 RDB 文件中的二进制数据读入内存，反序列化数据到内存中，恢复整个数据集。

AOF：逐条读取 AOF 文件中的命令，逐条解析和执行命令，将数据恢复到内存中。

## Redis：高可用

### Redis高可用?（集群 | 主从 | 哨兵 | 故障转移）

Redis实现服务的高可用性主要依赖于几种关键机制：集群、主从模式、哨兵机制和故障转移过程。下面我将逐一详细介绍这些机制。

首先是**集群**。Redis Cluster是Redis的分布式解决方案，它通过分片将数据分布在多个Redis节点上。每个节点都存储着一部分数据，并且节点之间可以相互通信。这种分布式架构不仅提高了数据的可扩展性，还增强了服务的可用性。当一个节点出现故障时，其他节点仍然可以继续提供服务，从而确保整体服务的可用性。

其次是**主从模式**。在这种模式下，我们有一个主节点和多个从节点。主节点负责处理写入操作，而从节点则复制主节点的数据并处理读取请求。这种读写分离的设计可以提高系统的吞吐量和可用性。如果主节点出现故障，我们可以手动或通过自动化工具将一个从节点提升为主节点，以确保服务的连续性。

接下来是**哨兵机制**。Redis Sentinel是一个高可用的解决方案，它用于监控Redis节点的状态并在主节点出现故障时进行故障转移。哨兵会定期检查Redis节点的状态，并在检测到主节点故障时触发故障转移过程。这个过程包括选举一个新的主节点、通知其他从节点更新配置以及将客户端重定向到新的主节点等步骤。哨兵机制能够自动处理故障转移，从而大大提高了Redis服务的可用性。

综上所述，Redis通过集群、主从模式、哨兵机制实现了服务的高可用性。这些机制能够确保在Redis节点出现故障时，服务仍然能够正常运行，并且数据不会丢失

### 高可用的理解

高可用性（High Availability, HA）是指系统在运行过程中，能够在规定的时间内持续提供服务的能力。高可用性的目标是确保系统尽可能长时间地正常运行，尽量减少因故障导致的停机时间。

集群：当主节点故障时，从节点可以自动提升为主节点，继续提供服务

主从复制：主节点故障时可以通过手动或自动提升从节点为主节点，继续提供服务

哨兵：监控 Redis 实例，自动执行故障转移，选举一个新的主节点

### 热key | 大key | 防护方案

**问题介绍**

在Redis中，大Key指的是那些占用内存较大或者包含大量元素的Key，而热Key则是指访问频率非常高的Key。这两种情况都可能对Redis的性能和稳定性造成影响。大Key可能导致内存占用过高、网络传输效率低下以及备份恢复困难等问题；而热Key则可能引发读写倾斜，导致某些节点负载过高，甚至引发缓存击穿等风险。

**如何发现大Key和热Key**

对于大Key的发现，我们可以采用以下几种方法：

1. 使用Redis自带的命令，如`--bigkeys`选项，对Redis进行扫描，找出各种数据类型中最大的Key。
2. 利用 `MEMORY USAGE` 命令查看特定Key的内存使用情况，从而判断其是否为大Key。
3. 借助第三方工具进行扫描和分析，如`redis-memory-analyzer`等

对于热Key的发现，可以考虑以下途径：

1. 通过Redis的`MONITOR`命令监控实时命令流，分析哪些Key被频繁访问
2. 使用第三方工具或自定义脚本统计每个Key的访问频率
3. 根据业务特点和历史数据，预判哪些Key可能会成为热Key

**如何解决大Key和热Key问题**

针对大Key问题，我们可以采取以下措施：

1. 拆分大Key：将一个大Key拆分成多个小Key，分散存储和访问压力。
2. 优化数据结构：选择更合适的数据结构来减少内存占用和提高访问效率。
3. 设置过期时间：为大Key设置合理的TTL，以便在一段时间后自动清理。

对于热Key问题，可以尝试以下方法：

1. 利用二级缓存：将热Key的数据缓存到离用户更近的地方，如使用本地缓存或CDN来减少对Redis的访问压力。
2. 数据分片：通过一定的策略将热Key分散到多个Redis节点上，实现负载均衡。
3. 优化查询逻辑：减少不必要的查询和重复查询，降低对热Key的访问频率。

### redis集群，计算key在哪个机器上？

Redis 集群使用哈希槽（hash slots）机制来管理数据分布，Redis Cluster 通常有 **16384** 个哈希槽 ，要计算给定 key 应该分布到哪个哈希槽中，需要先对每个 key 计算 **CRC-16**（XMODEM） 校验码，然后再对这个校验码对 16384(哈希槽的总数) 取模，得到的值即是 key 对应的哈希槽。

每个节点负责一部分哈希槽，集群配置文件中保存了每个节点负责的哈希槽范围

要先根据 key 通过上⾯的计算对应的哈希槽，然后再查询哈希槽和节点的映射关系，即可找到⽬标节点。



### Redis集群?数据怎么分片？高可用?怎么部署?

数据分片是通过哈希槽机制实现的，就是Redis 集群将整个键空间分为 16384 个哈希槽。每个键通过 CRC16 哈希函数计算哈希值，并对 16384 取模，得到哈希槽编号。集群中的每个节点负责一定范围的哈希槽。

集群高可用是通过主从复制和自动故障转移

**主从复制**：

- 每个主节点（master）有一个或多个从节点（slave）作为备份。
- 从节点实时复制主节点的数据，保证数据冗余。
- 从节点也可以处理读请求，减轻主节点的负担

**自动故障转移**：

- 集群中的节点相互通信，监控彼此的状态。
- 如果一个主节点故障，集群会通过选举机制将该主节点的一个从节点提升为新的主节点

**部署**

1. 准备节点：

   - 部署多个 Redis 实例，每个实例作为集群中的一个节点。

   - 推荐奇数个节点（至少 3 个主节点）以便于选举

2. 在配置文件里启用集群模式，指定集群的配置文件和端口
3. 启动所有 Redis 实例
4. 使用 `redis-cli` 命令创建集群，指定所有节点的地址和端口

### 哨兵机制检测/集群检测

一个哨兵节点在指定时间内（例如几秒钟）没有收到主节点的响应，会将主节点标记为主观下线，如果大多数哨兵节点都认为主节点无响应，则会将其标记为客观下线。

在 Redis 集群中，节点间会相互监控，每个 Redis 节点都会定期向其他节点发送 PING 消息，检测节点是否正常运行。多个节点确认某个节点为 PFAIL 后，会将其标记为失败。

### 主节点宕机，哨兵机制，集群机制（集群中宕机 leader挂掉）

**从节点切换主节点的切换过程**

**哨兵机制**

1. 选举领头哨兵
   - 当主节点被标记为 ODOWN 后，哨兵节点会通过投票选举出一个领头哨兵节点。
   - 领头哨兵负责执行故障转移操作
2. 选举新的主节点
   - 领头哨兵节点会从剩余的从节点中选举出一个新的主节点。
     - 选择数据最接近主节点的从节点
     - 从节点可以设置不同的优先级，优先级高的从节点更容易被选为新的主节点
   - 哨兵节点发送 SLAVEOF NO ONE 命令给被选中的从节点，将其提升为新的主节点。
3. 重新配置其他从节点
   - 新的主节点上任后，哨兵节点会发送命令给其他从节点，其他从节点会被重新配置，指向新的主节点进行数据同步
4. 通知客户端
   - 哨兵节点会将新的主节点地址通知给所有连接的客户端

**集群中**

1. 节点之间通信检测
   - 集群中的每个节点都会定期向其他节点发送 PING 消息，检测其状态
   - 如果一个节点在指定时间内没有响应其他节点的 PING 消息，该节点会被标记为 PFAIL（可能失败）
   - 多个节点确认该节点无响应后，会将其标记为 FAIL（失败）
2. 选举新节点
   - 集群会选举一个新的主节点
     - 选择数据最接近主节点的从节
     - 从节点可以设置不同的优先级
   - 集群发送 SLAVEOF NO ONE 命令给被选中的从节点，将其提升为新的主节点。
   - 新的主节点接管原主节点的哈希槽，继续提供服务
3. 配置从节点
   - 集群发送命令给其他从节点，让它们开始同步新的主节点
4. 更新集群元数据
   - 集群中的所有节点更新它们的路由表，确保数据请求能够正确路由到新的主节点
   - 客户端连接的节点会接收新的集群状态信息，并更新它们的路由表

### 集群有什么问题?（脑裂 数据不一致）

**脑裂**

在 Redis 主从架构中，部署方式一般是「一主多从」，主节点提供写操作，从节点提供读操作。 如果主节点的网络突然发生了问题，它与所有的从节点又举出一个 leader 作为主节点

当主节点发现从节点下线或者通信超时的总数量小于阈值时，那么禁止主节点进行写数据，直接把错误返回给客户端。
在 Redis 的配置文件中有两个参数我们可以设置：

- min-slaves-to-write x，主节点必须要有至少 x 个从节点连接，如果小于这个数，主节点会禁止写数据。
- min-slaves-max-lag x，主从数据复制和同步的延迟不能超过 x 秒，如果超过，主节点会禁止写数据。

**数据一致性**

Redis 集群采用了以下机制来确保数据一致性，并处理潜在的一致性问题

1. 数据分片
2. 主从复制
   - **异步复制**：主节点和从节点之间的数据同步是异步的。从节点会定期从主节点同步数据，主节点的写操作不会等待从节点的确认，这可以提高性能，但可能会导致短暂的数据不一致。
   - **故障转移**：当主节点故障时，从节点可以提升为新的主节点。这是通过哨兵机制或 Redis 集群的内部故障转移机制来实现的。
3. 使用 RDB 和 AOF 持久化机制来保护数据，确保即使主节点故障也能从持久化存储中恢复数据。

### 多个哨兵选举（多个哨兵同时选举）?

**多个哨兵同时选举**

可能出现多个哨兵节点同时尝试选举新主节点的情况

如果多个哨兵节点同时发现主节点故障，它们会同时发起选举。这种情况下，哨兵节点会互相广播选举请求，尝试成为领头哨兵。

哨兵节点会**使用一致性算法（如 Raft 算法或类似机制）**来达成共识，确保只有一个领头哨兵节点负责执行故障转移

### 哈希槽/一致性哈希

**哈希槽**

Redis 集群通过将数据划分到不同的哈希槽中来实现数据的分片，哈希槽是一种逻辑上的分区机制，用于将数据分布到集群中的多个节点上。Redis 集群使用 16384 个哈希槽，根据哈希函数计算得到一个哈希值，然后通过取模运算将键映射到这些哈希槽之一。

**一致性哈希**

一致性哈希主要用于分布式系统中的数据分片和负载均衡。一致性哈希的主要思想是将数据和节点映射到一个**哈希环**上，通常这个哈希环被表示为一个固定长度的闭环（0到2^32-1的整数环）。哈希值按顺时针的方向排列，每个节点和数据通过哈希函数分别映射到这个环上

### Raft算法（一致性算法 哨兵机制）

共识算法就是让分布式系统中的节点就⼀个问题达成共识。在 选举 leader 这个场景下，这些 从节点要达成的共识就是谁才是 leader

大概说一下就是，节点在选举过程中变为候选者，并请求其他节点的投票。节点获得超过半数的投票后，成为领导者，领导者将日志条目复制到所有跟随者，并等待确认，一旦日志条目被大多数节点确认，领导者将这些条目提交到日志中，并通知跟随者

### 热key过期防护方案？

**解决办法**

- **使用 `TTL` 命令监控**：可以使用 `TTL <key>` 命令监控键的过期时间，并采取措施。
- **缓存预热**：为关键数据的热键实现缓存预热策略，以避免在键过期时对系统造成冲击。
- **限流和缓存穿透防护**：使用限流机制和缓存穿透防护（如 Bloom Filter）来减少对 Redis 的过度访问。

### 搭建集群配置分片参数？

至少 3 个主节点，每个主节点至少有 1 个从节点

哈希槽数量就默认 16384 就好

### 限流导致的热key问题

**问题**

- 如果某个热键在限流策略应用之前已经成为热点，限流可能无法有效地分散请求，导致大量请求集中在少数热键上
- 当热键的访问量超过了限流的阈值，超出限流的请求可能会被丢弃或排队，导致系统的请求积压，降低用户体验或导致服务不可用。
- 限流可能导致请求不均匀地分配到不同的节点上，特别是在 Redis 集群中，某些节点可能会处理更多的请求。

**解决办法**

- 使用动态限流策略，根据实时负载和请求模式调整限流阈值。**基于令牌桶算法**：使用令牌桶算法进行限流，可以平滑处理请求并防止突发流量对系统的影响
- 使用负载均衡技术，将请求均匀分配到不同的 Redis 节点，在 Redis 集群中，将数据合理分片，减少单个节点的负载压力，实现负载均衡

## Redis：应用

### 缓存雪崩/缓存击穿/缓存穿透?

**缓存雪崩**

缓存雪崩是指缓存由于某些原因(如**大量数据同时过期或Redis 故障宕机**)整体崩溃，导致大量请求到达后端数据库，从而导致数据库崩溃

*解决方案*

大量数据同时过期：

1. 设置不同的过期时间，避免缓存集中失效。
2. 设置互斥锁，如果发现访问的数据不在 Redis 里，就加个互斥锁，保证同一时间内只有一个请求来构建缓存（从数据库读取数据，再将数据更新到 Redis 里），当缓存构建完成后，再释放锁
3. 缓存不过期，使用缓存预热和自动刷新机制。

redis故障宕机：

1. 通过主从节点的方式构建 Redis 缓存高可靠集群
2. 应用程序限流，避免过多请求同时到达数据库，只将少部分请求发送到数据库进行处理，再多的请求就在入口直接拒绝服务

**缓存击穿**

缓存击穿是指一个**热点数据失效后**，大量请求同时涌入后端存储，导致后端存储负载增大、响应时间变慢，甚至瘫痪。

可以发现缓存击穿跟缓存雪崩很相似，可以认为缓存击穿是缓存雪崩的一个子集。

*解决办法*

1. 设置互斥锁，如果发现访问的数据不在 Redis 里，就加个互斥锁，保证同一时间内只有一个请求来构建缓存（从数据库读取数据，再将数据更新到 Redis 里），当缓存构建完成后，再释放锁
2. 缓存不过期，使用缓存预热和自动刷新机制。

**缓存穿透**

缓存穿透是指**查询一个数据库一定不存在的数据**。正常的使用缓存流程大致是，数据查询先进行缓存查询，如果key不存在或者key已经过期，再对数据库进行查询，并把查询到的对象，放进缓存。如果数据库查询对象为空，则不放进缓存。持续查询不存在的数据会导致所有请求都落到后端存储上，导致系统瘫痪。

*解决办法*

1. 非法请求限制

   在 API 入口处我们要判断求请求参数是否合理，请求参数是否含有非法值、请求字段是否存在，如果判断出是恶意请求就直接返回错误，避免进一步访问缓存和数据库

2. 缓存空值或者默认值

   发现缓存穿透的现象时，可以针对查询的数据，在缓存中设置一个空值或者默认值，这样后续请求就可以从缓存中读取到空值或者默认值，返回给应用，而不会继续查询数据库

3. 使用布隆过滤器

   在写入数据库数据时，使用布隆过滤器做个标记，然后在用户请求到来时，业务线程确认缓存失效后，可以通过查询布隆过滤器快速判断数据是否存在，如果不存在，就不用通过查询数据库来判断数据是否存在

### 延迟双删

针对「先删除缓存，再更新数据库」方案在「读 + 写」并发请求而造成缓存不一致的解决办法是「延迟双删」。

```
#删除缓存
redis.delKey(X)
#更新数据库
db.update(X)
#睡眠
Thread.sleep(N)
#再删除缓存
redis.delKey(X)
```

### 布隆过滤器原理

布隆过滤器由「初始值都为 0 的位图数组」和「 N 个哈希函数」两部分组成，在写入数据库数据时，在布隆过滤器里做个标记，这样下次查询数据是否在数据库时，只需要查询布隆过滤器，如果查询到数据没有被标记，说明不在数据库中

分3个操作：

1. 使用 N 个哈希函数分别对数据做哈希计算，得到 N 个哈希值
2. 将第一步得到的 N 个哈希值对位图数组的长度取模，得到每个哈希值在位图数组的对应位
3. 将每个哈希值在位图数组的对应位置的值设置为 1

比如：假设有一个位图数组长度为 8，哈希函数 3 个的布隆过滤器

![图片](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/86b0046c2622b2c4bda697f9bc0f5b28.png)

数据 x 会被 3 个哈希函数分别计算出 3 个哈希值，取模之后是1，4，6，把位图数组的第 1、4、6 位置的值设置为 1，查询时，看1，4，6，如果有0就是不存在。

存在哈希冲突的可能性，数据 x 和数据 y 可能都落在第 1、4、6 位置，而事实上，可能数据库中并不存在数据 y。

所以查询布隆过滤器说数据存在，并不一定证明数据库中存在这个数据，但是查询到数据不存在，数据库中一定就不存在这个数据。

### 数据库和缓存如何保证一致性？（数据缓存一致性）

**先更新数据库再更新缓存**

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/8febac10b14bed16cb96d1d944cd08da.png" alt="图片" style="zoom:50%;" />

**先更新缓存再更新数据库**

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/454a8228a6549176ad7e0484fba3c92b.png" alt="图片" style="zoom:50%;" />

**先删除缓存再更新数据库**

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/cc208c2931b4e889d1a58cb655537767.png" alt="图片" style="zoom:50%;" />

**先更新数据库再删除缓存**

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/1cc7401143e79383ead96582ac11b615.png" alt="图片" style="zoom:50%;" />

**解决方案**

首先根据上面分析选择 先更新数据库，再删缓存 比较好，但是要保证这两个操作都能执行成功，有2个办法

1. 结合消息队列和重试机制
   - 当更新数据库成功后，可以将删除缓存的操作放入消息队列中，如果删除缓存失败，可以利用消息队列的重试机制来重新尝试删除操作。
   - 优点：这种方法能够确保即使初次删除缓存失败，也能通过重试机制最终达到数据一致性。
   - 注意事项：需要合理配置消息队列和重试策略，以避免对系统造成过大的压力。
2. 订阅MySQL binlog
   - 更新数据库成功，就会产生一条变更日志，记录在 binlog 里，使用 binlog 订阅工具，实时捕获数据库变更，并同步更新删除缓存，从而保证数据库和缓存的一致性。
   - 即使 Redis 第一次删除缓存失败，根据 binlog 事件内容来判断，然后回重试

### redis实现分布式锁？除了redis还有哪些可以做分布式锁？

分布式系统下，不同的服务/客户端通常运行在独立的 JVM 进程上。如果多个 JVM 进程共享同一份资源的话，使用本地锁就没办法实现资源的互斥访问了。于是，**分布式锁** 就诞生了

**Redis实现分布式锁**

1. 互斥

   Redis的 `SETNX` 命令是实现分布式锁的基础。当一个客户端尝试获取锁时，它会使用 SETNX 来设置一个键。如果键不存在， `SETNX` 会设置该键并返回1表示成功获取了锁。如果键已经存在，SETNX不会做任何事情并返回0，表示获取锁失败。

2. 锁操作原子性

    Lua 脚本是为了保证解锁操作的原子性。因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，从而保证了锁释放操作的原子性。

3. 设置过期时间

   但是，仅仅使用 `SETNX` 是不够的。如果持有锁的客户端在处理过程中崩溃或网络断开那么锁可能永远不会被释放，导致其他客户端永远无法获取锁，这就是所谓的死锁。为了防止这种情况，我们需要为锁设置一个过期时间。在Redis中，这可以通过 `EXPIRE` 命令来实现。

4. 看门狗实现自动续期

   Redisson 中的分布式锁自带自动续期机制，提供了一个专门用来监控和续期锁的 **Watch Dog（ 看门狗）**，如果操作共享资源的线程还未执行完成的话，Watch Dog 会不断地延长锁的过期时间，进而保证锁不会因为超时而被释放。

5. 集群情况下

   `Redlock`算法，让客户端向 Redis 集群中的多个独立的 Redis 实例依次请求申请加锁，如果客户端能够和半数以上的实例成功地完成加锁操作

**其他方式实现分布式锁**

1. 基于数据库的分布式锁
   我们可以利用数据库来实现分布式锁。例如，在MySQL中，可以使用 `SELECT...FOR UPDATE` 语句来锁定某一行数据。当一个事务在对数据执行修改操作前，先使用此语句锁定该行，其他事务在此期间无法对该行进行修改，从而实现了锁的效果。但需要注意的是，数据库锁可能会引起性能瓶颈，特别是在高并发场景下。
2. 基于Zookeeper的分布式锁
   Zookeeper是一个开源的分布式协调服务，它可以帮助我们实现分布式锁。在Zookeeper中，我们可以创建一个临时的顺序节点来表示锁。当客户端需要获取锁时，它会在指定的目录下创建一个顺序节点。然后，客户端会检查自己创建的节点是否是最小的节点，如果是，则获取到锁；如果不是，则监听比自己序号小的前一个节点。当前一个节点被删除时，客户端会收到通知并重新检查自己是否是当前最小的节点。这种方式可以有效地避免死锁，并且具有较好的性能。

### Redisson实现分布锁优势

1. `SETNX` 和 `EXPIRE` 是两个独立的命令，因此在网络延迟或服务器故障的情况下，`SETNX` 成功但 `EXPIRE` 失败，会导致死锁，因为锁没有过期时间
2. Redis 原生的分布式锁并不支持重入。即，如果同一个线程或进程多次获取锁，可能会陷入死锁
3. Redisson 内部实现的分布式锁使用 Lua 脚本将 `SETNX` 和 `EXPIRE` 两个操作原子化
4. Redisson 提供了**可重入锁**，即同一线程可以多次获取同一个锁，而不会导致死锁。这是通过内部记录每个锁的持有线程及重入次数实现的
5. Redisson 提供了**看门狗机制**，自动续期功能保证在持有锁的过程中不会因为锁的过期时间导致锁意外释放。

### Redisson源码分析

分布式锁的核心功能其实就三个：加锁、解锁、设置锁超时

使用hash数据结构

Redisson使用了redis的发布订阅功能

加锁：Redisson的加锁方法有两个，**tryLock**和**lock**

订阅分布式锁, 解锁时进行通知

- 先用`exists key`命令判断是否锁是否被占据了，没有的话就用`hset`命令写入，key为锁的名称，field为“客户端唯一ID:线程ID”，value为1；
- 锁被占据了，判断是否是当前线程占据的，是的话value值加1；（实现可重入的效果）
- 锁不是被当前线程占据，返回锁剩下的过期时长

看门狗：

看门狗在你没有指定锁过期时间时自动启用。

没传**自动释放锁时间**的话，就会给一个默认值，这个默认值就是getLockWatchdogTimeout()，也就是看门狗超时时间，是30秒

每隔固定时间（默认每隔 10 秒）检查一次锁的状态。如果发现锁仍然被持有，Redisson 会向 Redis 发送命令，延长锁的过期时间，将其重置为 30 秒。这个过程会持续到锁被主动释放为止

当客户端主动调用 `unlock()` 方法时，Redisson 会取消看门狗的调度，释放锁，同时删除 Redis 中的锁键。

![image-20240913153905768](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240913153905768.png)

### Redisson有哪些锁(redis分布式锁)

**可重入锁**（Reentrant Lock）：允许同一线程多次获取锁。

**公平锁**（Fair Lock）：按照请求锁的顺序公平获取锁。

**读写锁**（ReadWrite Lock）：支持多个读线程同时访问，但写线程独占。

**联锁**（MultiLock）：可以同时锁定多个资源，所有资源都锁定成功才算获取到锁。

**红锁**（RedLock）：Redisson 的红锁实现基于 Redis 分布式系统的高可用性，按照 Redis 作者提出的算法实现分布式锁。

### Redis消息队列？

**可以是可以，但不建议使用 Redis 来做消息队列。和专业的消息队列相比，还是有很多欠缺的地方**。

Redis2.0之前用List实现简单的消息队列，2.0引入了发布订阅功能，使用channel（频道）来实现

主要说一下Redis5.0增加的Stream做消息队列

- 发布 / 订阅模式
- 按照消费者组进行消费（确保每条消息只被一个消费者处理一次）
- 消息持久化（ RDB 和 AOF）
- ACK 机制（通过确认机制来告知已经成功处理了消息）
- 阻塞式获取消息

大致实现就是`XREAD`命令从Stream 中读取消息，`XGROUP CREATE`创建消费者组，`XREADGROUP`从消费者组中读消息，`XACK`确认处理

### Redis事务?支持原子性吗？支持回滚吗？

支持的，通过`MULTI`、`EXEC`、`DISCARD` 和 `WATCH` 命令来实现

**`MULTI`**：开始一个事务块，之后的命令会被加入到事务队列中。

**`EXEC`**：提交事务

**`DISCARD`**：取消事务块，丢弃事务队列中的所有命令

**`WATCH`**：用于实现乐观锁机制，监视一个或多个键。当这些键在事务执行期间发生变化时，事务会被中止

在事务开始（`MULTI`）之后但在事务提交（`EXEC`）之前发生宕机：不会被执行

在事务提交（`EXEC`）之后但在事务执行期间发生宕机：会看到部分事务命令已经生效

在 `MULTI` 和 `EXEC` 之间的所有命令都会被排入事务队列，在 `EXEC` 被调用时，事务队列中的命令会按顺序逐一执行，而不会被其他命令中断，是**支持原子性**的

如果事务执行过程中出现错误，Redis 会终止事务，并且所有未执行的命令不会被执行，但已成功执行的命令**不会被回滚**。

### Redis怎么统计网站pv、uv？

PV 表示页面浏览量，UV 是指不同用户访问页面的数量

pv可以使用`INCR`命令，每次被访问就+1

使用 Redis 的 `SET` 数据结构，每次页面被访问时，将用户 ID 加入集合，获取集合中的数量，命令就是`SADD`和`SCARD`

### Redis集群跨区域（网络延迟）

设置主从复制可以将数据复制到多个地区的从节点。这种方式可以减少跨城市网络延迟对读取操作的影响，因为读取操作可以在离用户更近的从节点上完成。写操作仍然需要在主节点上执行，但从节点可以在本地提供读请求。

# 计网

## 计网：基础

### OSI七层模型

OSI七层模型。这个模型从下到上依次为：物理层、数据链路层、网络层、传输层、会话层、表示层和应用层。

**物理层**，它主要定义物理设备标准，比如网线的接口类型和传输介质的传输速率等。物理层的主要作用就是透明地传输比特流，也就是传输最原始的数据信号。

数据链路层**，这一层主要将从物理层接收的数据进行MAC地址的封装与解封装数据在这一层被称为帧。数据链路层还负责处理流控制、物理地址寻找以及数据的重发等。

**网络层**，它的主要功能是选择合适的网间路由和交换节点，确保数据及时传送。网络层将从下层接收到的数据进行IP地址的封装和解封装，数据在这一层被称为数据包。

**传输层**，它主要负责建立连接、处理数据包错误以及数据包次序。传输层将从下层收到的数据进行分段和传输，达到目的地址后进行重组。常见的传输层协议有TCP和UDP

**会话层**，它主要负责在网络中的两个节点之间建立、维持和终止通信。会话层还负责解决节点链接的协调和管理，包括同步两个节点之间的对话，并决定通信是否被中断以及通信中断时从何处重新发送。

**表示层**，它主要是对接收的数据进行解释、压缩与解压等处理，确保数据能够以合适的格式进行传输和呈现。

**应用层**，它是OSI七层模型中的最高层，主要负责为网络用户或应用程序提供各种服务。应用层确定通信对象，提供访问网络服务的接口。常见的应用层协议有TeInet、FTP、HTTP和SNMP等

### TCP/IP 网络模型?（四层网络模型）

TCP/IP网络模型由四个层次构成，从下到上分别是网络接口层、网络层、传输层和应用层

**网络接口层**，也被称为链路层，它主要负责数据的物理传输，包括将数据包从一个节点传输到另一个节点。这一层处理的是如何在物理链路上传输数据，涉及到数据的成帧、差错控制等问题

**网络层**，它的主要任务是负责数据包在网络中的路由选择。网络层通过IP协议为数据包选择最佳的路径，确保数据能够准确、高效地到达目的地。同时，网络层还处理数据包的分片与重组，以适应不同网络环境的传输需求。

**传输层**，它提供了端到端的数据传输服务。传输层通过TCP或UDP等协议，确保数据在发送端和接收端之间可靠、有序地传输。其中，TCP协议通过建立连接、确认、重传等机制，保证了数据传输的可靠性和顺序性；而UDP协议则提供了一种无连接、不可靠的数据报服务，适用于对实时性要求较高、可以容忍一定数据丢失的应用场景。

**应用层**，它位于TCP/IP模型的最顶层，直接为用户提供各种网络服务。应用层包含了多种应用协议，如HTTP、FTP、SMTP等，这些协议定义了不同应用程序之间的通信规则和数据格式。通过应用层协议，用户可以轻松地实现各种网络应用，如浏览网页、上传下载文件、发送电子邮件等。

### 输入URL到页面展示发生了什么？

首先，我们输入的是网址，也就是URL，这是HTTP协议的使用入口。浏览器会首先检查是否有**缓存**，如果没有，需要通过**DNS协议**将网址解析为服务器对应的IP地址。

DNS协议工作在应用层，浏览器会向DNS服务器发送查询请求，获取网址对应的IP地址。
一旦获取到IP地址，就可以准备建立与服务器的连接了。

浏览器会与服务器的指定IP地址和端口号进行**TCP三次握手**，建立连接。TCP位于传输层，它负责建立和管理连接，确保数据的可靠传输。

在TCP连接建立后，浏览器就可以通过HTTP协议发送具体的网页请求了。HTTP请求会被封装在TCP数据包中发送出去。

当数据包从客户端发送到网络中时，首先会经过**交换机**。交换机根据数据包的**MAC**地址来决定数据包应该转发到哪个端口。然后数据包会经过**路由器**，路由器会根据**IP**地址来决定数据包应该转发到哪个网络。这样，数据包就能够逐级跳转，最终到达目标服务器。

服务器收到数据包后，会进行拆包处理。首先拆除的是MAC层的数据包，然后是IP层的数据包，接着是TCP层的数据包，最后是HTTP层的数据。服务器会根据HTTP请求的内容来准备响应数据，并将响应数据按照相反的顺序封装成数据包发送回客户端。‘

客户端收到服务器的响应数据包后，也会进行类似的拆包处理，最终提取出HTTP响应的内容。浏览器会根据HTTP响应的内容来渲染网页，展示给用户。

这就是整个从浏览器键入网址到网页显示的过程中所涉及的网络层协议和主要过程

### DNS解析？

DNS是应用层协议，基于UDP协议之上，端口53，主要工作是域名和IP的映射

首先会由本地计算机的DNS客户端发起一个DNS查询请求。这个请求首先会到达本地DNS服务器。

**本地DNS**服务器在收到请求后，会首先查看自身的缓存中是否已经有该域名的记录。如果有，则直接返回对应的IP地址；如果没有，则会向**根域名服务器**发出查询请求。根域名服务器会返回顶级域名服务器的IP地址，如.com、.net等域的服务器地址。

然后，本地DNS服务器会再向这些**顶级域名服务器**发出查询请求，顶级域名服务器会返回负责管理该域名的权威DNS服务器的IP地址。

最后，本地DNS服务器会向**权威DNS服务器**发出查询请求，权威DNS服务器会返回该域名对应的IP地址。本地DNS服务器在得到IP地址后，会将其缓存起来，并返回给DNS客户端。这样，用户就可以通过这个IP地址访问对应的网站了。

总的来说，DNS的解析过程是一个分布式的、层次化的查询过程，它通过多个DNS服务器之间的协作，将用户输入的域名解析为对应的IP地址。这个过程确保了用户在访问网站时能够准确地找到目标服务器，从而实现了网络的互联互通。

### DNS结构？

根 DNS 服务器、顶级域 DNS 服务器、权威 DNS 服务器、本地 DNS 服务器

### 子网掩码

**子网掩码**是用于区分 **IP 地址** 中的 **网络部分** 和 **主机部分**

**网络部分**：标识网络的地址，决定某个 IP 地址属于哪个网络。

**主机部分**：标识具体网络中的某台设备（主机）的地址。

通过子网掩码，路由器等网络设备可以判断两个 IP 地址是否属于同一个子网，从而决定如何路由数据包

**A 类地址**: 默认子网掩码 `255.0.0.0`，网络部分为 8 位，主机部分为 24 位。

**B 类地址**: 默认子网掩码 `255.255.0.0`，网络部分为 16 位，主机部分为 16 位。

**C 类地址**: 默认子网掩码 `255.255.255.0`，网络部分为 24 位，主机部分为 8 位。

## 计网：TCP&UDP

### TCP/UDP

这两种协议都是传输层协议

首先，从连接方式上来看，TCP是一个**面向连接**的协议。在数据传输开始之前，发送方和接收方必须先进行"三次握手"以建立连接。这个过程确保了双方都已准备好进行数据传输，并提供了一种同步机制。一旦连接建立，数据就可以在一个可靠的、有序的通道中进行传输。

与此相反，UDP是一个**无连接**的协议。UDP在发送数据之前不需要与接收方建立连接。发送方可以直接将数据包发送到网络上，而无需等待接收方的确认或同步。这种无连接的方式使得UDP具有较低的延迟和较高的传输效率，但同时也意味着数据包可能会在传输过程中丢失或乱序。

其次，TCP提供了**可靠**的数据传输服务。它通过**数据校验、序列号、确认应答、重传机制**等一系列措施来确保数据的完整性和准确性。如果数据包在传输过程中丢失或损坏，TCP会要求发送方重新发送数据包，直到接收方正确接收到数据为止。

而UDP则不提供这样的可靠性保证。它仅仅是将数据包发送到网络上，而不关心数据包是否能够成功到达接收方。如果数据包在传输过程中丢失或损坏，UDP并不会进行重传或修复操作。这种传输方式使得UDP具有较低的延迟和较高的效率，但同时也需要应用层来处理可能的数据包丢失或乱序问题。

TCP是一个基于**字节流**的协议，它将数据划分为较小的数据包进行传输，并在接收端将这些数据包重新组装成原始数据流。这种方式使得TCP能够灵活地处理各种大小的数据传输需求。

而UDP则是一个基于**数据报**的协议，它将应用层传来的数据直接打包成数据报进行传输。这种方式使得UDP能够更加高效地处理单个数据包，但也可能导致数据包的丢失或乱序问题。

TCP还具有**流量控制和拥塞控制**机制，能够根据网络状况动态调整发送速率，避免网络拥堵和数据丢失。而UDP则没有这些控制机制，需要应用层自行处理相关问题。

总的来说，TCP和UDP在连接方式、可靠性、数据传输方式以及控制机制等方面都有着显著的不同。这些差异使得它们适用于不同的应用场景和需求。例如，对于需要稳定、可靠数据传输的文件传输、电子邮件等应用，我们通常会选择TCP；而对于需要低延迟、高效率的在线视频、实时游戏等应用，我们可能会选择UDP。当然，在实际应用中，我们也需要根据具体情况进行权衡和选择

### TCP数据包/UDP数据包（TCP包头格式 UDP包头格式）

TCP：源端口号、目标端口号、序列号、确认应答号、控制位（ACK、RST、SYN、FIN）、校验和、窗口大小

UDP：源端口、目的端口、数据包的长度以及校验和

### TCP和UDP可以共用端口吗

可以的 TCP 和 UDP 可以同时绑定到相同的端口上，因为它们在内核中是作为两个完全独立的软件模块来处理的。当一个数据包到达时，操作系统会检查 IP 包头的协议号字段来确定该数据包是属于 TCP 还是 UDP。根据这个信息，数据包会被送到相应的协议模块进行处理。然后，模块会根据端口号将数据包转发给相应的应用程序。

由于 TCP 和 UDP 是独立的协议，它们可以并行运行而不会相互干扰，即使它们都绑定到同一个端口上。这种设计允许在同一台计算机上的不同应用程序可以同时使用 TCP 和UDP 协议，而不会发生端口冲突。

### TCP三次握手

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4/网络/TCP三次握手.drawio.png" alt="TCP 三次握手" style="zoom:50%;" />

TCP三次握手是TCP协议中建立连接的重要过程，它涉及客户端和服务器的状态变化以及发送请求的具体内容。

一开始，客户端和服务端都处于**`CLOSE`**状态。当服务端准备好接受连接时，它会进入**`LISTEN`**状态，等待客户端的连接请求。

第一次握手时，客户端会初始化一个随机序号，并发送一个`SYN`请求连接报文给服务端这个报文中包含了客户端的初始序列号。此时，客户端的状态变为**`SYN-SENT`**，等待服务端的确认。

服务端收到`SYN`请求后，会进行第二次握手。服务端也会随机初始化一个序号，并将客户端的序列号加1作为确认应答报文`ACK`，同时发送自己的`SYN`报文。这样，服务端发送的就是一个`SYN+ACK`的报文，此时服务端的状态变为**`SYN-RECEIVED`**。

客户端收到服务端的`SYN+ACK`报文后，会进行第三次握手。客户端会再次发送一个`ACK`报文给服务端，确认服务端的`SYN`报文，并将服务端的`ACK`序列号加1。此时，客户端的状态变为`ESTABLISHED`，表示连接已建立。服务端收到客户端的`ACK`报文后，状态也变为`ESTABLISHED`

这个三次握手的过程确保了客户端和服务端都能准确地知道对方已准备好进行数据传输是TCP协议中保证数据可靠传输的重要机制。

只有第三次握手可以携带数据，前两次不能携带数据

### 为什么是3次握手，能否改成2次握手或者4次?(为什么是三次握手？)

首先，三次握手能确保双方都具有接收和发送数据的能力。在第一次握手时，客户端发送`SYN`报文，证明客户端发送、服务端接收能力正常；第二次握手时，服务端发送`SYN+ACK`报文，证明服务端发送、客户端接收能力正常；第三次握手时，客户端发送ACK报文，证明客户端发送、服务端接收能力也都正常。这样，通过三次握手，可以全面地验证双方的通信能力。

三次握手还可以防止已失效的连接请求报文突然又传送到了服务器，从而产生错误连接。如果使用两次握手，在第一次握手时客户端发送SYN报文，服务端接收到并发送SYN+ACK报文后，就进入了已连接状态。如果此时客户端的SYN报文在网络中滞留了一段时间才到达服务端，服务端会误认为是新的连接请求，从而建立错误的连接。**在两次握手的情况下，服务端没有中间状态给客户端来阻止历史连接，导致服务端可能建立一个历史连接，造成资源浪费。**而三次握手可以避免这种情况的发生，因为在第三次握手时，客户端会发送ACK报文来确认服务端的SYN报文，这样即使网络中有滞留的SYN报文，也不会导致错误的连接建立。所以，TCP 使用三次握手建立连接的最**主要原因是防止「历史连接」初始化了连接。**

通过三次握手，客户端和服务端可以**同步彼此的初始序列号**。具体来说，在第一次握手中，客户端发送SYN报文，其中包含自己的初始序列号；在第二次握手中，服务端回复SYN+ACK报文，确认客户端的序列号，并发送自己的初始序列号；在第三次握手中，客户端发送ACK报文，确认服务端的序列号。这样，通过三次握手，双方就完成了初始序列号的同步。

三次握手就已经理论上最少可靠连接建立，所以不需要使用更多的通信次数。

### 粘包？

TCP 是面向**字节流**的协议，当用户消息通过 TCP 协议传输时，消息可能会被操作系统分组成多个的 TCP 报文，当两个消息的某个部分内容被分到同一个 TCP 报文时，就是我们常说的 TCP 粘包问题，这时接收方不知道消息的边界的话，是无法读出有效的消息。

**解决粘包**

- 固定长度的消息

  每个用户消息都是固定长度的，比如规定一个消息的长度是 64 个字节，当接收方接满 64 个字节，就认为这个内容是一个完整且有效的消息。

- 特殊字符作为边界

  在两个消息之间插入一个特殊的字符串，这样接收方在接收数据时，读到了这个特殊字符，就把认为已经读完一个完整的消息。

  HTTP 是一个非常好的例子，通过设置回车符、换行符作为边界

- 自定义消息结构

  比如，由包头和数据组成，其中包头包是固定大小的，而且包头里有一个字段来说明紧随其后的数据有多大。

### 握手丢失（握手失败）

第一次握手丢失：客户端重传SYN

第二次握手丢失：客户端重传SYN，服务端重传SYN+ACK

第三次握手丢失：**ACK 报文是不会有重传的，当 ACK 丢失了，就由对方重传对应的报文**，服务端重传SYN+ACK

### 半连接队列/全连接队列

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/3.jpg" alt="半连接队列与全连接队列" style="zoom:50%;" />

服务端收到客户端发起的 SYN 请求后，内核会把该连接存储到半连接队列，并向客户端响应 SYN+ACK，接着客户端会返回 ACK，服务端收到第三次握手的 ACK 后，内核会把连接从半连接队列移除，然后创建新的完全的连接，并将其添加到 accept 队列也就是全连接队列，等待进程调用 accept 函数时把连接取出来。

### UDP的socket/TCP的socket

TCP：服务端：socket-bind-listern-accept-write-read

​		   客户端：socket-connec-write-read

UDP：服务端/客户端：socket -> send/receive

### 服务端和客户端socket实现有哪些步骤？

服务端：socket-bind-listen-accept-write-read

客户端：socket-connect-write-read

客户端在创建好 Socket 后，调用 `connect()` 函数发起连接，该函数的参数要指明服务端的 IP 地址和端口号，然后 TCP 三次握手就开始了。

连接建立后，客户端和服务端就开始相互传输数据了，双方都可以通过 `read()` 和 `write()` 函数来读写数据

### 半连接状态？

客户端已经发送了SYN包，服务器也发送了SYN-ACK包，但客户端的ACK包还没有到达服务器，在这种状态下，连接还没有完全建立，处于等待客户端确认的阶段。

### 服务端没有`listen`能建立TCP连接吗？

服务端如果只bind了IP和端口，没有调用listen的话，客户端发起连接请求，服务端会**回复RST报文**。

**没有`listen`也可以建立TCP连接，比如说TCP自连接**，客户端给自己发请求，其实形成连接的前提就是，要有地方存放，方便握手的时候根据IP和端口号找到对应的scoket，就比如服务端`listen`方法，会创建半连接队列和全连接队列；在自连接的情况下，客户端会将连接信息放入全局hash表中，然后将信息发出，信息在经过回环地址重新回到TCP传输层时，就会根据IP和端口信息，就会从全局hash取出信息，就成功建立连接了。

### TCP数据就一定不会丢吗？（TCP一定安全吗）

不一定，TCP只保证传输层的消息是可靠的，并不能保证应用层的是可靠的，比如说

- 建立连接时丢包：半、全连接队列溢出
- 流量控制丢包：发送数据过快，流控队列长度不够大
- 网卡丢白：RingBuffer过小导致丢包，网卡性能不足
- 接收缓冲区丢包：缓冲区满了，还有数据发进来

### TCP四次挥手

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240729193759811.png" alt="image-20240729193759811" style="zoom:50%;" />

当一方想要关闭连接时，它需要通知另一方，并确保双方的数据传输都已完成，然后才能完全关闭连接。这就是四次挥手的主要目的。

第一次挥手是由客户端发起的。当客户端没有更多的数据要发送时，它会发送一个`FIN`报文段给服务端。这个`FIN`报文段的作用是告诉服务端："我已经没有数据要发送了，你可以开始关闭你那边到我这边的连接了"。同时，客户端会进入`FIN_WAIT_1`状态，等待服务端的确认。

第二次挥手发生在服务端收到FIN报文段后。服务端会发送一个`ACK`报文段给客户端，作为对`FIN`报文段的确认。这个ACK报文段的作用是告诉客户端："我已经收到你的关闭请求了，我这边会准备关闭连接，但是在我发送完所有的数据之前，连接还不会完全关闭"，此时，服务端进入`CLOSE_WAIT`状态，等待所有数据发送完毕。而客户端在收到`ACK`后会进入`FIN_WAIT_2`状态，等待服务端的FIN报文段。

第三次挥手发生在服务端发送完所有数据后。此时，服务端会发送一个`FIN`报文段给客户端，告知客户端它也没有数据要发送了，可以关闭连接了。这个`FIN`报文段的作用是告诉客户端	"我这边也没有数据要发送了，我们可以关闭连接了"。服务端在发送`FIN`报文段后，会进入`LAST_ACK`状态，等待客户端的确认。

第四次挥手是客户端在收到服务端的`FIN`报文段后发起的。客户端会发送一个`ACK`报文段给服务端，作为对`FIN`报文段的确认。这个`ACK`报文段的作用是告诉服务端："我已经收到你的关闭请求了，我会关闭连接"。在发送完`ACK`报文段后，客户端会进入`TIME_WAIT`状态。这个状态会持续一段时间(通常是`2MSL`，即数据包在网络中的最大生存时间)，以确保关闭请求和确认报文段能够被对方正确接收。如果在这段时间内没有收到对方的重新发送请求，那么客户端会最终关闭连接，进入`CLOSED`状态。而服务端在收到`ACK`报文段后，会立即关闭连接，进入`CLOSED`状态。

这就是TCP四次挥手的全过程。通过这个过程，TCP连接能够在确保双方数据传输完成的前提下安全地关闭。这个过程的每一步都是必要的，以确保数据的完整性和可靠性。

### TIME_WAIT（2MSL）

当TCP连接中主动关闭连接的一方发送最后一个`ACK`确认包后，它会进入`TIME_WAIT`状态。这个状态的主要作用是等待一段时间，**以确保最后的ACK确认包能够被对方正确接收**。

`MSL`是**一个TCP数据包（段）在网络中可以存在的最长时间**，一般`1MSL`是2分钟，所以`2MSL`是4分钟

首先，设置`2MSL`的等待时间可以确保最后的`ACK`报文能够到达对方并被正确接收。网络传输中可能存在延迟或丢包的情况，所以给予足够的时间窗口，让对方能够接收到确认消息，从而保证连接的可靠关闭。

其次，这个时间的设置也**有助于避免新旧连接之间的混淆**。在网络中，可能存在多个相同源IP地址和目的IP地址的连接。等待`2MSL`的时间可以确保之前连接的所有报文都已经在网络中消失，这样新的连接就不会错误地接收到之前连接的残留数据。

总的来说，`TIME_WAIT`等待的时间是2MSL，是为了确保网络连接的可靠关闭，并防止新旧连接之间的混淆。

**服务器什么情况下会产生大量的`TIME_WAIT`**

首先要知道 TIME WAIT 状态是主动关闭连接方才会出现的状态，所以如果服务器出现大量的 TIME WAIT 状态的 TCP 连接，就是说明服务器主动断开了很多 TCP 连接。而服务器主动断开链接的场景有：

1. HTTP 没有使用长连接
2. HTTP 长连接超时
3. HTTP 长连接的请求数量达到上限

**怎么排查**

1. 查看TIME_WAIT连接数：

   ```sh
   netstat -an | grep TIME_WAIT | wc -l
   ```

2. 检查应用程序是否频繁使用短连接，或者大量长连接超时

3. 检查`Keep-Alive`设置是否合理

4. 检查设置的`TIME_WAIT`状态的持续时间

5. 查看日志和监控工具

**如果`TIME_WAIT`太多会有什么问题**

首先端口资源耗尽是一个主要的问题。由于每个处于`TIME_WAIT`状态的连接都会占用一个端口，当这种状态的连接数量过多时，可能会导致服务端可用端口不足。一旦所有可月端口都被占满，新的连接就无法建立，从而导致服务拒绝新的客户端请求。这里的端口资源主要是指客户端的本地端口资源。

大量的`TIME_WAIT`状态可能会占用大量的内存和系统资源。服务器需要维护每个处于`TIME_WAIT`状态的连接的数据结构，这可能导致内存消耗增加。

**怎么解决大量`TIME_WAIT`问题**

首先，考虑到`TIME_WAIT`状态主要是为了确保关闭连接的请求和最后的`ACK`能够被对方正确接收，我会尝试优化TCP参数配置。具体来说，可以调整TCP的`SO_REUSEADDR`选项，允许端口复用，这样即使端口处于`TIME_WAIT`状态，也可以被新的连接所使用，从而避免了端口资源耗尽的问题。

其次，我会从应用程序的设计层面进行优化。例如，对于频繁进行短连接操作的场景，我会考虑使用长连接或者连接池技术来减少连接的频繁建立和关闭。这样不仅可以减少`TIME_WAIT`状态的数量，还能提高系统的整体性能。

另外，我也会关注服务端和客户端之间的网络通信质量。如果网络通信不稳定或者存在延迟，可能会导致连接在关闭后仍然长时间处于`TIME_WAIT`状态。因此，我会尝试优化网络环境，减少网络延迟和丢包率，从而降低`TIME_WAIT`状态的数量。

最后，如果服务端确实需要频繁地关闭和打开连接，并且无法避免大量的`TIME_WAIT`状态，我会考虑增加服务端的端口范围，以确保即使有大量`TIME_WAIT`状态存在，也不会耗尽所有可用端口。

### TCP可靠性?滑动窗口？流量控制？拥塞控制?

TCP，即传输控制协议，是一种面向连接的、可靠的、基于字节流的传输层通信协议。TCP的可靠性主要是通过一系列机制来实现的，这些机制共同确保了数据包能够准确、有序地从发送端传输到接收端。具体来说，TCP的可靠性主要通过以下几个方面来保证和实现：

1. **重传机制**：TCP通过序列号和确认应答机制来实现数据的可靠传输。当发送端发送数据后，会等待接收端的确认应答。如果在一定时间内没有收到确认应答，发送端会重传数据，直到收到确认应答为止。这种重传机制确保了数据的可靠传输，避免了数据丢失的问题。
2. **滑动窗口机制**：TCP使用滑动窗口机制来实现流量控制。发送端维护一个发送窗口接收端也维护一个接收窗口。发送端根据接收端的接收能力来动态调整发送窗口的大小，从而控制数据的发送速率，避免发送端发送数据过快而导致接收端来不及处理。这种机制确保了数据的稳定、有序传输。
3. **流量控制**：TCP的流量控制是基于滑动窗口实现的。接收端会在确认应答中告知发送端自己当前的接收窗口大小，发送端根据这个信息来调整自己的发送速率。这样，当接收端的处理能力下降时，发送端会相应地降低发送速率，从而避免数据丢失或乱序。
4. **拥塞控制**：TCP还通过拥塞控制机制来避免网络拥塞。具体来说，TCP使用了开始、拥塞避免、快重传和快恢复等算法来调整发送速率。这些算法可以根据网络的实时状况动态地调整发送端的发送速率，从而避免网络拥塞和数据丢失。

**TCP怎么进行流量控制的**

TCP进行流量控制主要通过滑动窗口机制来实现。流量控制是为了防止发送方发送数据过快，导致接收方来不及处理而丢失数据。为了实现这一目标，TCP使用了基于滑动窗口的流量控制方法。

在TCP连接建立后，接收方会根据自己的接收缓冲区大小，通过确认报文段(ACK)中的窗口字段来告知发送方自己当前能够接收的最大数据量，即接收窗口的大小。发送方在收到这个信息后，就会根据接收窗口的大小来调整自己的发送速率，确保发送的数据量不会超过接收方的处理能力。

随着数据传输的进行，接收方的接收窗口大小可能会动态变化。比如，当接收方的应用程序从接收缓冲区中读取数据时，接收窗口的大小就会相应增加；而当接收缓冲区中的数据积累过多时，接收窗口的大小就会减小。接收方会实时更新窗口大小，并通过ACK报文通知发送方。

发送方在收到新的窗口大小信息后，会相应地调整自己的发送窗口，以确保发送的数据量始终在接收方的处理能力范围之内。这样，TCP就能够通过滑动窗口机制实现有效的流量控制，防止数据丢失和网络拥塞的发生。

**TCP的拥塞控制怎么做的？拥塞控制算法有哪些？**

TCP的拥塞控制是为了防止因过多的数据注入网络而产生的网络拥塞，从而避免网络性能下降和数据丢失。拥塞控制主要是通过一系列算法来动态调整数据的发送速率，以达到网络资源的合理利用。

TCP的拥塞控制算法主要包括：**慢开始、拥塞避免、快重传和快恢复**。

首先是慢开始算法。在TCP连接刚建立时，发送方会初始化一个较小的拥塞窗口，并随着每个确认报文的到来逐渐增大这个窗口，通常是每次翻倍。这个阶段的目的是逐渐探测网络的拥塞程度，避免一开始就发送大量数据导致网络拥塞。

当拥塞窗口增大到一定程度后，TCP会进入拥塞避免阶段。在这个阶段，发送方每收到一个确认报文，拥塞窗口只增加一点点，而不是继续翻倍，以避免发送速率增长过快导致网络拥塞。

如果发送方连续收到三个相同的确认报文(即三个重复的ACK)，它会认为有数据包丢失，此时会触发快重传算法。发送方会立即重传被认为丢失的数据包，而不必等待超时。

在快重传之后，TCP会进入快恢复阶段。在这个阶段，发送方会将拥塞窗口减半，以降低发送速率，并尝试恢复正常的数据传输。然后，发送方会采用与拥塞避免算法相同的策略来增加拥塞窗口。

### TCP滑动窗口大小变化的策略

TCP滑动窗口大小的变化是通过流量控制和拥塞控制两种机制共同决定的。流量控制基于接收方的能力，而拥塞控制则通过动态调整拥塞窗口来避免网络拥塞并优化数据传输效率。

`send_window = min(rwnd, cwnd)`

接收窗口和拥塞窗口的最小值

### 服务端满了再去调用write()函数，会怎么样?（阻塞模式/非阻塞模式）

在阻塞模式下，如果服务端的缓冲区满了，客户端继续调用`write()`会阻塞，到服务端缓冲区有空闲空间可以接收更多的数据为止。在此期间，`write()`调用不会返回，程序会暂停在这个位置。

在非阻塞模式下，如果服务端的缓冲区满了，客户端继续调用`write()`函数会立即返回，并且返回值会是`-1`，同时设置`errno`为`EAGAIN`或`EWOULDBLOCK`，表示当前没有空闲缓冲区空间可用，写操作未成功。

### 服务端针对连接需要做什么？服务端收到数据时，怎样处理？（read一次性读完吗）

服务端如果只`bind`了IP和端口，或者没bind的话，没有调用`listen`的话，客户端发起连接请求，服务端会回复RST报文。

客户端发起一个连接，会先进入全连接队列里，等待服务器`accept()`

使用`read()`函数从客户端套接字中读取数据，直到读取完客户端发送的所有数据，根据应用协议的格式解析数据。例如，在HTTP协议中，需要解析请求头和请求体，然后执行操作，比如，如果请求涉及数据查询，可能需要从数据库中获取数据，然后生成响应，通过`write()`发回给客户端。

服务器端的 `read()` 函数读取数据时，并不总是能够一次性将所有数据读完。`read()`函数每次调用会从套接字中读取最多指定数量的字节

服务器端判断长时间不在线的连接有赖于TCP `Keep-Alive`，保活机制

### 设计TCP可靠性连接?

1. 建立和终止(TCP三次握手,四次挥手)
2. 序列号和确认机制,重传机制
3. 流量控制,拥塞控制

### 命令查看TCP处于什么状态

`ss`命令

```SH
ss -t -a
```

`-t`：显示TCP连接。

`-a`：显示所有连接和监听端口。

```
State      Recv-Q Send-Q Local Address:Port   Peer Address:Port
LISTEN     0      128     127.0.0.1:3306       *:*
ESTAB      0      0       192.168.0.10:22      192.168.0.1:53784
ESTAB      0      0       192.168.0.10:53784   192.168.0.1:22
LISTEN     0      128     *:80                 *:*
```

### TCP缺陷

1. TCP建立连接有延迟

   基于 TCP 实现的应用协议，都是需要先建立三次握手才能进行数据传输，基于 TCP 实现的应用协议，都是需要先建立三次握手才能进行数据传输

2. TCP存在队头阻塞的问题

   TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且有序的，如果序列号较低的 TCP 段在网络传输中丢失了，即使序列号较高的 TCP 段已经被接收了，应用层也无法从内核中读取到这部分数据，必须等待丢失的数据重传。

3. 网络迁移需要重新建立连接

   当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立 TCP 连接

### UDP实现可靠传输？（QUIC）

现在基于UDP实现的可靠的传输方案就是：QUIC

QUIC怎么实现可靠传输？

UDP不可靠的原因主要有：无连接、没有确认和重传机制、数据包的顺序不保证、没有流量和拥塞控制

QUIC是有连接的，通过减少握手次数来加速连接建立，**一次握手**，客户端发送第一个握手消息，服务器在收到后立即回复，双方完成加密密钥协商。初次连接时，它结合了TLS和传输层握手，可以在一次往返中建立加密连接。对于复用已有的连接ID的后续连接，可以在0-RTT内建立连接。而且QUIC使用连接ID而不是IP地址和端口号来标识连接，允许连接在网络环境（如IP地址变化）改变时迁移而不中断。

QUIC内部包含了TLS，他在自己的帧会携带TLS里的记录，速度更快；HTTP/1和HTTP/2，TCP和TLS是分层的，分别属于内核实现的传输层，表示层，无法合并

QUIC实现了**ACK（确认）机制**，接收方会对成功接收到的数据包发送ACK，发送方如果在一定时间内未收到ACK，会重传数据包。QUIC 使用的 Packet Number 单调递增的设计，可以让数据包不再像 TCP 那样必须有序确认，QUIC 支持乱序确认，当数据包Packet N 丢失后，只要有新的已接收数据包确认，当前窗口就会继续向右滑动。有序列号，接收方也可以按正确顺序重组数据

QUIC**没有队头阻塞**，给每一个 Stream 都分配了一个独立的滑动窗口，这样使得一个连接上的多个 Stream 之间没有依赖关系，都是相互独立的，各自控制的滑动窗口。

QUIC的流量控制有两个，Stream流量控制、Connection流量控制

QUIC默认用的也是TCP 的 Cubic 拥塞控制算法（我们熟知的慢开始、拥塞避免、快重传、快恢复策略），而且QUIC处于应用层（TCP是传输层的），应用层就能实现拥塞控制算法，不需要内核，速度更快

### 网络拥塞原因?网线剪断？

高延迟、丢包率增加、带宽利用率下降等

**网络拥塞的原因：**

由于各种原因导致网络设备和链路无法处理超出其能力的数据流量，最终引发网络性能问题。

1. 突发流量：突然的流量激增（如大规模下载、视频流、DoS攻击）会迅速耗尽网络带宽，导致拥塞
2. 持久高流量：持续的高流量负载（如视频会议、云备份）会长时间占用带宽，导致其他数据流受影响

**拥塞控制：**

拥塞控制机制不能完全保证网络一直保持通畅，但可以在很大程度上缓解和管理网络拥塞，提高网络的整体性能和稳定性。

**网线剪断：**

物理连接被断开，TCP会

1. **数据传输中断**：数据包无法传递。
2. **重传和超时**：发送方会尝试重传数据，直至超时。
3. **连接关闭**：多次重传失败后，TCP最终会认为连接断开并关闭连接。（TCP Keep-Alive）
4. **应用层处理**：应用层检测到连接断开后，可能会尝试重连或报错。

## 计网：HTTP

### HTTP/HTTPS?

首先，安全性是这两者之间最显著的区别。HTTP是超文本传输协议，它是以明文方式传输数据的，这意味着在数据传输过程中，如果遭到拦截，信息将以明文形式暴露，存在安全风险。而HTTPS则是HTTP的安全版，它在HTTP的基础上加入了SSL/TLS协议，可以对数据进行加密传输和身份认证，从而保证了数据传输的安全性。

其次，HTTP和HTTPS使用的端口也不同。HTTP默认使用80端口，而HTTPS则使用443端口。

再者，HTTPS将加密，解密，验证，解压缩等流程都交给了SSL/TLS来处理，因此相对于HTTP来说，HTTPS在处理速度上会有所降低，但由于目前硬件性能的大幅提升，这种影响在实际使用中几乎可以忽略不计。

最后，HTTPS需要向认证机构(CA)申请证书，一般需要一定的费用，而HTTP则不需要

### HTTPS建立连接的过程（HTTPS握手过程 TLS四次握手）

首先是TCP三次握手，接着就是TLS四次握手

TLS第一次握手：首先，由客户端向服务器发起加密通信请求，也就是 ClientHello 请求，他会发送TLS 协议版本、客户端产生的随机数（用于生成密钥）、密码套件列表

第二次握手：服务器收到客户端请求后，向客户端发出响应，也就是 SeverHello，确认 TLS 协议版本，服务器生产的随机数，密码套件列表，服务器的数字证书

第三次握手：客户端收到服务器的回应之后，首先通过浏览器或者操作系统中的 CA 公钥，确认服务器的数字证书的真实性，如果证书没有问题，客户端会从数字证书中取出服务器的公钥，然后使用它加密报文，向服务器发送随机数

第四次握手：服务器收到客户端的第三个随机数之后，通过协商的加密算法，计算出本次通信的「会话秘钥」，表示随后的信息都将用「会话秘钥」加密通信

### HTTPS采用对称加密和非对称加密？为什么后续换成对称密钥？

HTTPS确实采用了对称加密和非对称加密结合的方式。在HTTPS连接建立的过程中，非对称加密主要用于密钥交换和安全认证，而对称加密则用于实际的数据传输。

HTTPS之所以在密钥交换阶段使用**非对称加密**，是因为非对称加密具有很高的安全性。非对称加密算法使用一对密钥，即公钥和私钥。公钥可以公开，任何人都可以用公钥对数据进行加密，但只有持有相应私钥的人才能解密。这种特性使得非对称加密在密钥交换过程中非常有用，因为它可以确保只有服务器能解密客户端发送的数据，从而保证了通信的安全性。

然而，非对称加密的计算复杂度较高，加密和解密速度相对较慢。因此，在大量的数据传输中，如果一直使用非对称加密，会导致性能下降，影响用户体验。为了提高通信效率，HTTPS在密钥交换完成后，会生成一个对称密钥。这个对称密钥将用于后续的数据传输加密。对称加密的特点是加密和解密使用相同的密钥，其加密速度快适合用于大量数据的加密传输。

所以，HTTPS在密钥交换阶段使用非对称加密来确保安全性，而在**后续的数据传输阶段则切换到对称加密**以提高效率。这种结合使用的方式既保证了通信的安全性，又兼顾了性能需求。

### HTTP报文结构

首先，HTTP请求报文主要由**请求行、请求头部和请求体**三个部分构成。请求行包含了请求的方法，比如`GET`或`POST`，请求的URL路径，以及HTTP协议的版本。紧接着请求行的是请求头部，它由多个键值对组成，每个键值对代表一种信息或设置，比如`Host`字段标明请求的目标主机，`User-Agent`字段描述发出请求的用户代理信息，`Accedt`字段则列出客户端能够理解的内容类型。请求头部之后，可能跟随一个请求体，它通常用于`POST`或`PUT`请求中，包含客户端提交给服务器的数据。

其次，HTTP响应报文的结构与请求报文相似，由状态行、响应头部和响应体组成。状态行包含HTTP协议版本状态码以及状态描述信息。比如，状态码`200`表示请求成功，`404`则表示资源未找到。响应头部也包含多个键值对，提供有关响应的附加信息，如`Content-Type`描述响应体的内容类型，`Content-Length`指明响应体的长度。响应体则包含服务器返回给客户端的实际数据内容，如HTML文件、图片或其他媒体资源。

### HTTP状态码

- 1xx 类状态码属于提示信息，是协议处理中的一种中间状态，实际用到的比较少。
- 2xx 类状态码表示服务器成功处理了客户端的请求，也是我们最愿意看到的状态。
- 3xx 类状态码表示客户端请求的资源发生了变动，需要客户端用新的 URL 重新发送请求获取资源，也就是重定向。
- 4xx 类状态码表示客户端发送的报文有误，服务器无法处理，也就是错误码的含义。
- 5xx 类状态码表示客户端请求报文正确，但是服务器处理时内部发生了错误，属于服务器端的错误码

**具体常见的有：**

- [200 OK]：请求成功
- [204 No Content]：响应头没有body数据
- [206 Partial Content]：应用于HTTP分块下载或断点续传，表示body不是资源全部



- [301 Moved Permanetly]：永久重定向，说明请求的资源已经不存在了，需改用新的 URL 再次访问
- [302 Found]：临时重定向，说明请求的资源还在，但暂时需要用另一个 URL 来访问
- 响应头里使用字段 Location，指明后续要跳转的 URL，浏览器会自动重定向新的 URL。
- [304 Not Modified]：缓存重定向



- [400 Bad Request]：客户端请求报文有误
- [403 Forbidden]：服务器禁止访问资源
- [404 Not Found]：请求的资源在服务器上不存在或未找到



- [500 Internal Server Error]
- [501 Not Implemented]：客户端请求的功能还不支持（未完待续的意思）
- [502 Bad Gateway]：作为网关或者代理工作的服务器尝试执行请求时，从上游服务器接收到无效的响应
- [504 Gateway Time-out]：作为网关或者代理工作的服务器尝试执行请求时，未能及时从上游服务器收到响应
- 假设 nginx 是代理服务器，收到客户端的请求后，将请求转发到后端服务器（tomcat 等）
  - 当nginx收到了无效的响应时，就返回502。
  - 当nginx超过自己配置的超时时间，还没有收到请求时，就返回504错误。
- [503 Service Unavaliable]：服务器正忙

### header字段（请求头字段），content-type知道吗?

在HTTP请求和响应中，常见的header字段包括：

1. Host：指定请求的目标主机和端口号。
2. User-Agent：标识发出请求的用户代理(如浏览器类型和版本)
3. Accept：告知服务器客户端能够处理的内容类型，如 `text/html`、`application/json`等
4. Accept-Encoding：告诉服务器客户端支持的内容编码方式，如 `gzip`、 `deflate` 等，这有助于减少传输数据的大小。
5. Accept-Language：表明客户端希望接收的语言类型，如`en-US`、`zh-CN` 等
6. Authorization：在需要用户验证的情况下，此字段包含认证信息，如`Bearer Token`或其他认证方式。
7. Content-Length：在请求或响应中，标明正文内容的长度。
8. Content-Type：这个字段非常重要，它定义了请求或响应正文的媒体类型。例如，在POST请求中，如果发送的是JSON数据，那么Content-Type应该设置为 `application/json`。同样，如果响应正文是HTML，则Content-Type可能设置为 `text/html`。

### HTTP请求类型

- GET：用于请求获取指定资源，通常用于获取数据。
- POST：用于向服务器提交数据，通常用于提交表单数据或进行资源的创建。
- PUT：用于向服务器更新指定资源，通常用于更新已存在的资源。
- DELETE：用于请求服务器删除指定资源。
- HEAD：类似于GET请求，但只返回资源的头部信息，用于获取资源的元数据而不获取实际内容

### get/post

1. 根据 RFC 规范，GET方法主要用于从服务器获取资源，而POST方法则用于向服务器提交数据，通常用于表单的提交或文件的上传等操作。
2. 在数据传输方面，GET方法将请求参数附加在URL之后，作为查询字符串传递，这些参数在浏览器地址栏中可见，因此GET请求不适合传输敏感信息。而POST方法则将请求参数放在请求体中传输，对用户来说地址栏是不可见的。(当然因为http来说都是明文传输，抓包之后都可见)
3. GET请求的参数有长度限制，因为浏览器和Web服务器对**URL**长度有限制，而POST请求由于参数放在**请求体**中，没有这样的限制。
4. GET是**幂等的**，多次请求不会改变资源状态。POST不是，每次执行可能产生不同结果或影响资源状态
5. 因为GET幂等，可以背浏览器或中间结点缓存，以提高效率

至于GET方法是否可以用来提交数据，从技术上说，是可以通过查询字符串提交一些简单的数据的。但是，由于GET请求的参数可见性和长度限制，它不适合用于提交大量或敏感的数据。在实际应用中，我们通常使用POST方法来提交数据，以确保数据的安全性和完整性。

### 应用层协议

HTTP、HTTPS、CDN、DNS、FTP

### cookie/session（会话）

Cookie和Session都是用来跟踪浏览器用户身份的会话方式。当客户端第一次发送请求到服务器时，服务器会创建一个Session，并同时创建一个名为JSESSIONID的特殊Cookie发送到客户端。此后，客户端每次发送请求都会携带这个Cookie，服务器则通过识别这个Cookie中的SessionID来找到对应的Session，从而获取用户的会话信息。

1. 存储位置：这是二者最明显的区别。Cookie的数据信息存放在客户端的浏览器上，而Session的数据信息则是存放在服务器端的。
2. 存储容量：单个Cookie保存的数据量是有限的，通常不超过4KB，而且一个站点最多可以保存20个Cookie。相比之下，Session的存储容量没有明确的上限，但出于服务器性能考虑，我们不会在其中存放过多的数据，并会设置适当的Session删除机制
3. 数据类型：Cookie中只能存储ASCII字符串，如果需要存储其他类型的数据，则需要进行编码。而Session则能够存储任何类型的数据，包括字符串、整数、列表、字典等。
4. 安全性：由于Cookie的数据存储在客户端，因此它对于客户端是可见的，这带来了一定的安全风险，如Cookie欺骗等。而Session数据存储在服务器端，对客户端是透明的，因此不存在敏感信息泄漏的风险。
5. 生命周期：Cookie的生命周期可以通过设置其属性来实现长期有效。而Session的生命周期则依赖于名为`JSESSIONID`的Cookie，该Cookie的过期时间默认为-1，意味着只要关闭浏览器窗口，该Session就会失效。
6. 服务器压力：由于Cookie数据保存在客户端，因此不会给服务器带来额外的负担。然而，对于Session来说，每个用户都会产生一个Session，如果并发访问的用户非常多，那么服务器上将会产生大量的Session，这可能会消耗大量的内存资源。
7. **Cookie 一般用来保存用户信息**，我们在 Cookie 中保存已经登录过得用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了。
8. **Session 的主要作用就是通过服务端记录用户的状态。** 典型的场景是购物车，当你要添加商品到购物车的时候，系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。

### HTTP/1.1vsHTTP/1.0

HTTP/1.1引入了**长连接**。在HTTP/1.0中，每次请求都需要建立一个新的TCP连接，请求处理完毕后立即断开连接。这种方式在处理包含大量图片、链接等资源时，会导致大量的连接建立和断开操作，造成资源浪费和时间延迟。而HTTP/1.1则允许在同一个TCP连接上连续发送多个请求，从而减少了连接建立和断开的开销，提高了网络资源的利用率。

HTTP/1.1支持请求**管道化**，这意味着客户端可以同时发送多个请求，而不需要等待前一个请求的响应。这样，服务器就可以按照请求到达的顺序依次进行回应，从而减少了等待时间，提高了整体的响应速度。需要注意的是，虽然请求管道化可以并行发送请求，但服务器仍然需要按照顺序回应。

HTTP/1.1还增加了更多的错误状态码，提供了更详细的错误信息，这有助于开发人员更好地定位问题并进行调试。

HTTP/1.1还引入了更多的请求头和响应头，为开发人员提供了更多的控制和灵活性。

HTTP/1.1还改进了缓存机制，允许服务器发送`Last-Modified`和`ETag`等响应头，以便客户端进行条件性请求，这避免了不必要的资源传输，提高了网络的利用率。

1.0存在浪费带宽的现象，比如只需要某一个对象一部分，但是只能传一整个对象，不支持断点续传。1.1在请求头里引入了range头域，允许请求资源某个部分

### HTTP/2.0vsHTTP/1.1

HTTP/2.0采用了二进制分帧层，这使得客户端和服务器可以将HTTP消息分解为多个互不依赖的帧，并可以乱序发送。这些帧在另一端被重新组合起来，从而实现了低延迟和高吞吐量。

HTTP/2.0引入了**多路复用技术**，允许同一连接上同时传输多个请求和响应，互不干预。这解决了HTTP/1.1中的队头阻塞问题，使得多个数据流可以并行传输，从而更有效地使用TCP连接，提高了网络资源的利用率。（但是也只是解决了HTTP这一层的对头阻塞，TCP的队头阻塞还是存在的）

HTTP/2.0还支持头部压缩，通过HPACK算法对请求头进行压缩传输，1.1只支持body压缩，2.0支持header压缩，节省了消息头占用的网络流量，降低了带宽消耗。

HTTP/2.0还引入了服务器推送技术，在客户端请求一个资源的时候，将其他相关资源一并推送，从而减少客户端的请求次数和延迟

### HTTP/3.0vsHTTP/2.0

HTTP/3.0使用了基于**UDP协议的QUIC**作为传输层协议，而不是传统的TCP协议。这一改变显著减少了连接建立的延迟，因为QUIC协议在建立连接时无需进行多次握手从而加速了数据传输的速度。

HTTP/3.0支持真正的多路复用，这意味着在同一连接上可以并行处理多个请求和响应，而不会像HTTP/2那样存在队头阻塞的问题。这大大提高了网络资源的利用率和传输效率。

在安全性方面，HTTP/3.0默认使用TLS 1.3加密，为数据传输提供了更高的安全性。同时，QUIC协议还提供了更好的错误恢复机制，当数据包丢失或损坏时，能够更快速地恢复连接并进行数据传输。

HTTP/3.0还支持0-RTT连接，即客户端可以在不需要等待服务器响应的情况下直接发送数据给服务器。这种连接方式可以大幅度缩短连接建立时间，但在某些场景下可能存在一定的安全风险，因此在使用时需要谨慎考虑。

### HTTPS传输时，客户端如果接收到了被拦截方(第三方)的公钥，怎么办?

这属于中间人攻击，攻击者拦截和篡改客户端和服务器之间的通信。在HTTPS中，中间人攻击可能通过伪造或替换公钥来实现。

HTTPS协议中，公钥通常嵌入在服务器的数字证书中。客户端在接收到服务器证书时，会进行一系列验证。比如说检查证书签名、证书链验证、检查证书有效期、检查证书域名

### HTTPS中3个随机数，但前两个都是明文传的，为什么不只使用第3个随机数？

1. 客户端和服务器的随机数提供了额外的熵，增加了会话密钥的复杂性和不可预测性。如果只使用一个随机数，可能会降低密钥生成的安全性，使得密钥更容易被攻击者预测或破解也就是说就算被获取到了也没关系
2. 如果在握手阶段对这些随机数进行加密传输，会增加额外的计算复杂度和通信开销

### CA证书是什么

CA证书是由证书颁发机构签发的数字证书，用于确认身份，包括证书所有者身份、公钥、有效期、数字签名、序列号等

服务器证书主要用于验证网站的合法性和建立安全的HTTPS连接

客户端证书用于验证客户端的身份，通常在需要强身份验证的场景下使用

### Tls 的证书，是怎么吊销的你知道吗？怎么让客户端知道这个证书过期了？背后的原理知道吗？

TLS证书主要用于验证服务器的身份，确保客户端（如浏览器）连接到的是合法的网站或服务。

吊销证书的主要方法包括证书吊销列表（CRL）和在线证书状态协议（OCSP）

当证书被吊销时，CA会将其添加到CRL中，客户端在验证证书时会下载并检查该列表，以确保证书未被吊销

客户端向OCSP服务器发送包含证书序列号的请求，查询证书的状态，OCSP服务器查询数据库，返回证书的状态

OCSP Stapling是一种优化的OCSP方法，通过将OCSP响应绑定到TLS握手过程中，以减少客户端的请求次数，服务器定期向OCSP服务器请求其证书的状态，并获得OCSP响应，服务器在TLS握手过程中，将OCSP响应作为证书的一部分发送给客户端

### 协商缓存/强制缓存

协商缓存是指客户端每次请求资源时，都会向服务器发送一个请求，询问缓存的资源是否仍然有效。服务器会根据资源的状态返回相应的响应。若资源没有变化，服务器会返回 304 状态码，告诉客户端可以继续使用本地缓存的资源。如果资源已发生变化，则返回新的资源和 200 状态码

协商缓存通过 HTTP 请求头中的`Last-Modified` 或 `ETag`来实现

强制缓存是指客户端在一定时间内直接从缓存中读取资源，而无需向服务器发送请求来检查资源的有效性。强制缓存通过 HTTP 响应头中的 `Cache-Control` 来指定资源的有效期

### HTTPS校验的哈希算法

SHA-256、SHA-384和SHA-512等哈希算法，客户端验证服务器证书的签名时，会使用证书中指定的哈希算法（如SHA-256），结合CA的公钥进行签名验证，确保证书的真实性。客户端和服务器交换的握手消息通过哈希算法进行哈希计算，以生成消息摘要。

### Cookie在哪一部分?如何控制请求携带一个cookie?

Cookie在请求头中， `Cookie` 头部字段

浏览器自动管理，当服务器在响应中设置了 `Set-Cookie` 头部后，浏览器会自动存储并在后续请求中携带相应的Cookie

### 对称加密/非对称加密算法

**非对称加密**

非对称加密使用一对密钥：公钥和私钥。公钥用于加密，私钥用于解密。

- RSA：基于大整数分解的难题
- ECDSA：基于椭圆曲线离散对数问题

**对称加密**

使用同一个密钥进行加密和解密

- AES
- ChaCha20：基于流密码的对称加密算法

### 实现Session共享

1. 将Session数据存储在数据库中，所有服务器都连接到同一个数据库实例
2. 将Session数据存储在Redis
3. 使用JWT

### NAT？ARP？

NAT协议是网络地址转换协议，内部网到外部网的地址转换，在NET表中会有IP和端口的映射。NET协议主要是为了节省IP地址，每个路由器都可以管理很多主机，如果每个主机都有唯一的IP，那IPv4很快就会到达上限

ARP是地址解析协议，连接了网络层和链路层，ARP表记录了IP和MAC地址的映射关系，广播问询，单播响应

### Ping 命令

Ping命令是一个网络工具，用于测试主机之间的连通性。其工作原理主要依赖于ICMP，当用户在命令行中输入 `ping <目标主机>` 时，Ping程序向目标主机发送一个ICMP Echo请求消息，目标主机收到ICMP Echo请求后，会发送一个ICMP Echo回复消息回给发送方

远端不可达，一般是防火墙配置，或者主机不可达

### 在传输过程中，IP地址会变吗，MAC地址会变吗

IP地址通常是不变的，除了经过NAT设备，IP地址可能会发生变化

MAC地址是一直改变的，因为MAC地址是下一跳的设备，直到找到最终的目标设备

### 路由器/交换机？广播消息？广播消息能过路由器么？

路由器：工作在网络层（第三层）。主要功能是将数据包从**一个网络传送到另一个网络**，即跨网络的通信。

交换机：工作在数据链路层（第二层）。主要功能是将数据帧在**同一个局域网内**的设备之间进行转发。

跨网络访问是通过**路由器**实现的

交换机上的广播：当交换机收到一个广播帧，它会将这个帧转发到除源端口外的所有端口，即整个局域网内的所有设备都能收到这个广播消息。

路由器上的广播：路由器不会转发局域网中的广播消息到其他网络。广播消息只在一个局域网内传播，不会通过路由器进入其他网络

通常情况下，**广播消息不能通过路由器**。路由器会阻止广播消息跨越不同的网络。

### JWT？JWT常见问题？

JWT是跨域认证的解决方案，基于Token的认证授权机制，JWT本身就是Token，使用JWT服务器不需要存储Session信息

**JWT常见问题：**

1. 注销登录等场景下JWT还有小，Session可以直接删除，但JWT一旦派出去，失效前都有效
2. JWT续签问题（可以用两个JWT解决）

**JWT字段：**

Header：令牌类型、签名算法

Payload：要传递的数据（不加密）

Signature：通过Header、Payload、密钥、签名算法生成

### 消息推送（实时推送）

- 短轮询

- 长轮询

  Nacos、Apollo、Kafka、RocketMQ都使用了长轮询，服务器数据没改变，一直hold主请求，数据发送变化才返回

- SSE

  一种服务器到客户端单向的消息推送，ChatGPT就是SSE

- WebSocket

  基于TCP的全双工通信协议，客户端和服务端都可以发送和接收

- MQTT

  基于发布/订阅模式的轻量级通讯协议，通过订阅相应注意来获取消息，一般用于物联网

### IPv4的长度？地址不够用怎么办？

IPv4地址长度是32位（二进制位），通常表示为四个8位的二进制数

地址不够用可以用NET协议，允许多个设备共享一个公共IP地址访问互联网，或者使用IPv6

### HTTP是无状态的？

HTTP被描述为“无状态”的主要原因是**每个HTTP请求都是独立的，服务器并不保存关于客户端的状态信息**，每个请求都需要提供足够的信息来理解请求的意图。

每个请求都是相互独立的。使用Cookie只是在无状态协议下的一种补充机制，用于在客户端存储状态信息以实现状态保持。

### localStorage和Cookie区别？

Cookie 的安全性较低,因为 Cookie 在每次 HTTP 请求中都会自动发送到服务器,存在被窃取或篡改的风险。而 LocalStorage 的数据仅在浏览器端存储,不会自动发送到服务器,相对而言更安全一些

### 为什么有HTTP协议了?还要用RPC?

- RPC 本质上不算是协议，而是一种调用方式，而像 gRPC 和 Thrift 这样的具体实现，才是协议，它们是实现了 RPC 调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。同时 RPC 有很多种实现方式，不一定非得基于 TCP 协议。
- 从发展历史来说，HTTP 主要用于 B/S 架构，而 RPC 更多用于 C/S 架构。但现在其实已经没分那么清了，B/S 和 C/S 在慢慢融合。很多软件同时支持多端，所以对外一般用 HTTP 协议，而内部集群的微服务之间则采用 RPC 协议进行通讯。
- RPC 其实比 HTTP 出现的要早，且比目前主流的 HTTP/1.1 性能要更好，所以大部分公司内部都还在使用 RPC。
- HTTP/2.0在 HTTP/1.1的基础上做了优化，性能可能比很多 RPC 协议都要好，但由于是这几年才出来的，所以也不太可能取代掉 RPC。

### 网页转圈圈，要定位问题需要从哪些角度？

最直接的办法就是抓包，排查的思路大概有：

1. 先确定是服务端的问题，还是客户端的问题。先确认浏览器是否可以访问其他网站，如果不可以，说明客户端网络自身的问题，然后检查客户端网络配置（连接wifi正不正常，有没有插网线）；如果可以正常其他网页，说明客户端网络是可以正常上网的。
2. 如果客户端网络没问题，就抓包确认 DNS 是否解析出了 IP 地址，如果没有解析出来，说明域名写错了，如果解析出了 IP 地址，抓包确认有没有和服务端建立三次握手
3. 如果能成功建立三次握手，并且发出了 HTTP 请求，但是就是没有显示页面，可以查看服务端返回的响应码：
   - 如果是404错误码，检查输入的url是否正确；
   - 如果是500，说明服务器此时有问题；
   - 如果是200，F12看看前端代码有问题导致浏览器没有渲染出页面。

4. 如果客户端网络是正常的，但是访问速度很慢，导致很久才显示出来。这时候要看客户端的网口流量是否太大的了，导致tcp发生丢包之类的问题。

总之就是一层一层有没有插网线，网络配置是否正确、DNS有没有解析出 IP地址、TCP有没有三次握手、HTTP返回的响应码是什么。

### 服务端启动了，客户端请求不到?（客户端请求不到怎么排查）

如果客户端请求的接口没有响应，排查的方式：

- 检查接口IP地址是否正确，ping一下接口地址。
- 检查被测接口端口号是否正确，可以在本机Telnet接口的IP和端口号，检查端口号能否连通
- 检查服务器的防火墙是否关闭，如果是以为安全或者权限问题不能关闭，需要找运维进行策略配置，开放对应的IP和端口。
- 检查你的客户端（浏览器、测试工具 (opens new window)），是否设置了网络代理，网络代理可以造成请求失败。

如果客户端的请求有响应，但是返回了错误状态码，那么根据错误码做对应的排查

### 服务器ping不通但是http能请求成功什么原因造成的?

ping 走的是 icmp 协议，http 走的是 tcp 协议。
有可能服务器的防火墙禁止 icmp 协议，但是 tcp 协议没有禁止，就会出现服务器 ping 不通，但是 http 能请求成果。

### HTTP和RPC区别？

**服务发现**

- 在HTTP中，直到服务域名可以通过DNS服务找IP
- 在RPC中，使用Nacos

**底层连接形式**

- HTTP/1.1默认建立TCP连接之后，会一直保持这个连接，之后请求 响应都会复用这个连接
- RPC会建一个连接池

# 操作系统

## 操作系统：硬件结构

### 软中断、硬中断

硬中断是由外部硬件设备触发，优先级高，需要快速响应

软中断是由软件程序触发的中断，优先级相对较低，响应时间较长

一般来说，Linux 将中断处理程序分为上半部和下半部：

上半部，对应硬中断，由硬件触发中断，用来快速处理中断；
下半部，对应软中断，由内核触发中断，用来异步处理上半部未完成的工作；

### 负数、小数能表示准确吗？怎么表示

负数用补码表示，就是把正数的二进制全部取反再加 1

小数：符号位、指数部分和尾数

小数并不是都可以精确表示的，比如说0.1，因为小数存进计算机的时候，需要转为二进制，小数部分很有可能会是一串无限循环的二进制数，所以只能存近似值

### CPU架构

x86架构：电脑

ARM架构：手机，平板

## 操作系统：内存管理

### 虚拟内存？(分段分页 虚拟内存-物理地址)（TLB）

**虚拟内存**

虚拟内存是计算机系统内存管理的一种重要技术。为了在多进程环境下，使得进程之间的内存地址不受影响，相互隔离，于是操作系统就为每个进程独立分配一套虚拟地址空间，每个程序只关心自己的虚拟地址就可以，实际上大家的虚拟地址都是一样的，但分布到物理地址内存是不一样的。当启用了大量的进程，物理内存必然会很紧张，于是操作系统会通过**内存交换技术**，把不常使用的内存暂时存放到硬盘（换出），在需要的时候再装载回物理内存（换入）。

**为什么要有虚拟内存地址**

1. 虚拟内存扩展了可用的内存容量。通过结合物理内存和硬盘空间，它允许大型应用程序运行，并为多任务操作系统提供了更好的支持。即使物理内存不足，系统也可以通过将未使用的数据移动到磁盘上来继续运行程序。
2. 虚拟内存提供了内存隔离，增强了数据的安全性和系统的稳定性。每个进程都有自己的虚拟地址空间，这样一个进程就无法读取或修改另一个进程的数据。这种隔离保护了操作系统和应用程序免受潜在的安全威胁。
3. 虚拟内存还减少了程序之间的冲突。由于每个程序都在自己的地址空间中运行，因此不同的程序之间不会产生地址冲突。如果两个程序试图使用相同的内存地址，操作系统会检测到这种情况并防止数据损坏。
4. 虚拟内存还通过页面置换技术提高了I/O效率。当物理内存中的某些页面不再活跃时，它们会被移动到硬盘上，从而释放出物理内存空间给更活跃的应用程序使用。这不仅减少了I/O操作的次数，还降低了硬盘访问时间，从而提高了整体的系统性能。
5. 虚拟内存技术还简化了内存管理，减轻了程序员的负担。程序员无需关心实际的物理内存布局和管理细节，可以更加专注于应用程序的开发和实现。

**操作系统如何管理虚拟内存地址**

对于虚拟地址与物理地址的映射关系，可以有**分段**和**分页**的方式，同时两者结合都是可以的

首先，在分段机制下，虚拟地址由两部分组成：**段选择子**和**段内偏移量**。段选择子用于指定要访问的段的起始地址和长度，而段内偏移量则表示在该段内的具体位置。操作系统会维护一个**段表**，记录了每个段的起始地址和长度等信息。当程序访问一个虚拟地址时，操作系统会通过段选择子从段表中找到对应的段描述符，进而计算出物理地址。

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/1720434118047-14f427b4-9a01-4c53-abc9-0538e8a678ac.png" alt="img" style="zoom: 50%;" />

接下来是分页。分页是另一种重要的虚拟内存管理技术。分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小，称为"页"。虚拟地址与物理地址之间通过**页表**来映射。当程序需要更多的内存空间时操作系统会将部分数据从物理内存中移到磁盘等辅助存储器上，而将当前需要的数据加载到物理内存中。这种机制使得程序可以透明地使用更大的内存空间，而无需关心实际物理内存的大小。

然而，随着程序规模的扩大，页表也会变得非常庞大，这导致了查找效率的问题。为了解决这个问题，我们引入了**多级页表**的概念。多级页表将页表进行分级，通过分级索引的方式来定位具体的页框号。这种方式可以有效地减少页表所占用的空间，并提高查找效率。

最后，TLB是一种高速缓存结构，用于加速虚拟地址到物理地址的转换过程。它保存了最近使用的虚拟地址和物理地址之间的映射关系。当CPU发出虚拟地址时，TLB会首先检查其中是否包含所需的物理地址。如果命中，则直接返回对应的物理地址；如果未命中，则需要访问操作系统的页表以获取映射关系，并将这些信息加载到TLB中以便下次快速访问。通过这种方式，TLB可以显著提高内存访问的性能。

### 操作系统-一页大小？虚拟内存空间大小？进程可以访问到所有的地址吗?

一页一般是4KB，如果是大页面可以是2MB；

应用程序的虚拟内存空间大小在32位系统中通常限制为4GB，在64位系统中理论上可以达到2^64但是实际受硬件的限制

虚拟地址空间范围是操作系统为每个进程分配的地址范围，每个进程在同一个系统中具有相同的虚拟地址空间范围，例如，在32位系统中为0到4GB），但是实际使用的虚拟内存量不同，比如程序A（一个简单的文本编辑器），虚拟地址空间范围：0到4GB，实际使用的虚拟内存：50MB。程序B（一个复杂的图像处理软件），虚拟地址空间范围：0到4GB，实际使用的虚拟内存：1.5GB。

不可以，每个进程在同一系统中具有相同范围的虚拟地址空间，但它们之间是相互独立和隔离的。即便两个进程的某些虚拟地址相同，它们所指向的物理内存是不同的

### 操作系统内存占用，有哪些内存的部分？（内核空间/用户空间）

内核空间、用户空间

内核空间里面有：内核代码、内核数据、内核缓冲区

用户空间有 6 种不同的内存段：代码段、数据段、BSS段、堆段、文件映射段、栈段

### malloc分配内存？（brk，mmap）

malloc() 并不是系统调用，而是 C 库里的函数，用于动态分配内存，有两种方式向操作系统申请堆内存

方式一：通过 brk() 系统调用从堆分配内存
方式二：通过 mmap() 系统调用在文件映射区域分配内存

mmap是一个用于在程序中映射文件或设备到内存的系统调用

方式一就是通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间

方式二通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存

应用程序通过 malloc 函数申请内存的时候，实际上申请的是虚拟内存，此时并不会分配物理内存。

当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生缺页中断，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。

缺页中断处理函数会看是否有空闲的物理内存，如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。

如果没有空闲的物理内存，那么内核就会开始进行回收内存的工作。

### 页面置换算法？操作系统内存不足？mmap映射的文件超过内存空间？(先进先出)

1. **先进先出置换算法**（*FIFO*）

   选择在内存驻留时间很长的页面进行中置换

2. 最近最久未使用的置换算法（*LRU*）

   选择最长时间没有被访问的页面进行置换

3. 最佳页面置换算法（*OPT*）

   置换在「未来」最长时间不访问的页面，实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面

4. 时钟页面置换算法（*Lock*）、

5. 最不常用置换算法（*LFU*）

### 内存对齐？

操作系统将内存划分为固定大小的页（通常是4KB），并将内存管理中的各种结构对齐到页边界。

### 操作系统内存清理涉及的内存（清理 用户空间 内核空间）

**用户空间内存**

包括堆和栈内存。应用程序在运行过程中会动态分配和释放内存，通过调用`free`、`delete`等函数来释放未使用的内存。

映射到虚拟内存的文件，可以通过调用`munmap`来解除映射并释放内存

**内核空间内存**

如页缓存（page cache）、缓冲区缓存（buffer cache），这些缓存用于提高文件系统I/O性能，可以通过内核内存回收机制来清理未使用的缓存

当进程结束时，操作系统会清理与该进程相关的所有内存，包括PCB和任务结构

### 内存分配算法？内存碎片，怎么办？内存紧凑算法？（首次适配 最佳适配 最坏适配 下次适配）

**内存分配算法**

1. **首次适配**

   从头开始扫描空闲列表，找到第一个足够大的空闲块进行分配，容易导致内存碎片

2. 最佳适配

   扫描整个空闲列表，找到最小的但足够大的空闲块进行分配，可以减少内存碎片，但会留下很多小的无法使用的空闲块

3. 最坏适配

   扫描整个空闲列表，找到最大的空闲块进行分配，较少生成碎片，因为大块内存会被划分为多个部分

4. 下次适配

   从上次分配结束的位置开始扫描空闲列表，找到第一个足够大的空闲块进行分配，仍然会产生内存碎片

**内部碎片怎么办**

可以通过**内存紧凑算法**，将分散的内存块移动到一起，将空闲内存块合并成一个大的连续内存区域，从而减少外部碎片

**紧凑算法实现**

1. 标记

   算法遍历内存中的所有对象，标记出正在使用的对象

2. 压缩

   将活跃对象移动到内存的低地址部分，保持它们的相对顺序

### swap/内存？top指令怎么评估内存的占用？

**swap（交换空间）是为扩展物理内存而设置的一块磁盘空间，将暂时不活跃的内存页面移到磁盘上，以释放物理内存给当前需要的进程使用**

**判断在swap还是在内存中**

页面表里面的有效位，如果是1，表示页面在物理内存中；如果为0，表示页面不在物理内存中，可能在交换空间中

在Linux系统中，可以通过读取`/proc`文件系统中的特定文件来获取页面状态信息

**top指令怎么评估内存的占用**

`top`命令的输出中，有几个关键部分与内存使用相关

**Mem**：显示物理内存的使用情况，里面会显示总物理内存大小、空闲内存大小、用于缓冲区和缓存的内存总量、已使用的内存

**Swap**：显示交换空间（swap）的使用情况，里面显示总交换空间大小、空闲交换空间大小、已使用的交换空间、被交换出的页面数量

 **Process Area（进程区域）**：所有进程的详细信息，包括进程ID、进程所有者、优先级、进程虚拟内存总量、进程实际物理内存使用量、共享内存使用量

### 需要的内容不在物理内存中，缺页中断？

**缺页中断**，如果页面不在物理内存中，操作系统将检查页面是否在交换空间中。如果页面在交换空间中，操作系统会从交换空间中读取该页面，并将其调入物理内存，如果物理内存已满，使用页面置换算法来交换出去页面

### 打印出hello world，从用户态到内核态具体发生了什么？如何分配空间？（系统处理命令具体步骤）

用户程序的源代码被编译成机器代码，当用户执行程序时，操作系统将其加载到内存中，打印"hello world"通常会调用`printf`函数或类似的库函数，这些库函数会最终触发一个系统调用，涉及从用户态切换到内核态。

触发系统调用--->保存上下文--->切换到内核态

到了内核态，操作系统执行系统调用，处理请求：

系统调用处理（包括分配空间）--->数据写入

在内核态完成系统调用处理后，操作系统将返回用户态：

恢复上下文--->返回用户态

**分配空间**

- 用户程序或内核发起内存分配请求，指定所需的内存大小
- 内存管理系统根据内存分配算法找到合适的内存
- 分配内存

### 什么时候会触发page fault？（缺页中断）

 页面错误，在程序尝试访问一个不在物理内存中的虚拟地址时

`malloc`就会，`malloc` 分配的是虚拟内存地址，访问时会发生缺页中断，然后才分配实际的物理内存

fork之后，父进程和子进程对共享的内存页进行写操作

### 操作系统负责地址翻译？

负责虚拟地址到物理地址的翻译的组件主要是**内存管理单元**（MMU）

### fork()会复制哪些东西？

- fork 阶段会复制父进程的页表（虚拟内存）
- fork 之后，如果发生了写时复制，就会创建一个新的物理页面，将虚拟地址映射到新的物理页面

### 写时复制（copyonwrite）是什么

主进程在执行 fork 的时候，操作系统会把主进程的「页表」复制一份给子进程，这个页表记录着虚拟地址和物理地址映射关系，而不会复制物理内存，也就是说，两者的虚拟空间不同，但其对应的物理空间是同一个

子进程就共享了父进程的物理内存数据了，这样能够节约物理内存资源，页表对应的页表项的属性会标记该物理内存的权限为只读。

当父进程或者子进程在向这个内存发起写操作时，CPU 就会触发写保护中断，这个写保护中断是由于违反权限导致的，然后操作系统会在「写保护中断处理函数」里进行物理内存的复制，并重新设置其内存映射关系，将父子进程的内存读写权限设置为可读写，最后才会对内存进行写操作，这个过程被称为「写时复制(Copy On Write)」

### 写时复制节省了什么资源

节省了物理内存的资源，因为 fork 的时候，子进程不需要复制父进程的物理内存，只需要复制父进程的页表，这时候父子进程的页表指向的都是共享的物理内存。

只需要复制父进程的页表，这时候父子进程的页表指向的都是共享的物理内存。

### fork会返回两次？为什么

`fork` 系统调用在成功时确实会返回两次，这两次返回发生在父进程和子进程中，这是因为 `fork` 的目的是创建一个新的进程（子进程），这个新进程是现有进程（父进程）的副本。

- 父进程：在父进程中，`fork` 返回新创建的子进程的进程 ID（PID）
- 子进程：在子进程中，`fork` 返回 0。这表示当前进程是新创建的子进程

**为什么**

- **不同的返回值**：`fork` 通过返回不同的值来区分父进程和子进程。父进程接收到子进程的 PID，而子进程接收到 0。这样，父进程和子进程可以根据返回值判断自己是哪个角色，并执行相应的逻辑
- **进程的独立性**：`fork` 调用之后，父进程和子进程是两个独立的进程，具有各自的执行流和数据。它们各自从 `fork` 返回的点继续执行，因此 `fork` 会在两个进程中返回。

### `malloc` 1KB和1MB有什么区别？

如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；

如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；

## 操作系统：进程管理

### 进程、线程、协程的区别

1. **进程是操作系统中进行资源分配和调度的基本单位**，它拥有自己的独立内存空间和系统资源。每个进程都有独立的堆和栈，不与其他进程共享。进程间通信需要通过特定的机制，如管道、消息队列、信号量等。由于进程拥有独立的内存空间，因此其稳定性和安全性相对较高，但同时上下文切换的开销也较大，因为需要保存和恢复整个进程的状态。
2. **线程是进程内的一个执行单元**，也是CPU调度和分派的基本单位。与进程不同，线程共享进程的内存空间，包括堆和全局变量。**线程之间通信更加高效，因为它们可以直接读写共享内存**。线程的上下文切换开销较小，因为只需要保存和恢复线程的上下文，而不是整个进程的状态。然而，由于多个线程共享内存空间，因此存在数据竞争和线程安全的问题，需要通过同步和互斥机制来解决。
3. **协程是一种用户态的轻量级线程**，其调度完全由用户程序控制，而不需要内核的参与。协程拥有自己的寄存器上下文和栈，但与其他协程共享堆内存。协程的切换开销非常小，因为只需要保存和恢复协程的上下文，而无需进行内核级的上下文切换。这使得协程在处理大量并发任务时具有非常高的效率。然而，协程需要程序员显式地进行调度和管理，相对于线程和进程来说，其编程模型更为复杂

### 进程上下文切换/线程上下文切换？

首先是**进程**上下文切换。在切换内容方面，进程上下文切换涉及的内容较为广泛。由于进程是操作系统中进行资源分配和调度的基本单位，它拥有自己的独立内存空间和系统资源，因此**进程上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源**，这些信息存储在一个进程控制块（PCB）。这些资源在切换时都需要被保存和恢复，以确保新进程能够在切换后顺利执行。

在发生场景上，进程上下文切换主要发生在以下几种情况：一是进程主动调用某些系统调用时，如等待IO完成或者获取锁，这时进程无法继续运行，操作系统会触发上下文切换，二是进程分配到的时间片用完，或者有更高优先级的进程需要抢占CPU时，也会发生上下文切换。

接下来是**线程**上下文切换。在切换内容方面，线程上下文切换主要涉及线程在执行过程中的运行条件和状态，**如程序计数器、虚拟机栈、本地方法栈寄存器的值等**，这些信息存储在一个线程控制块（TCB）中。**由于线程共享进程的内存空间，因此线程上下文切换不需要像进程上下文切换那样涉及大量的内存和资源管理**。

线程上下文切换发生场景主要包括：线程主动让出CPU，例如调用了Thread.seep()或Object.wait()等方法；当一个线程的时间片用完，需要切换到另一个线程继续执行；或者线程因为阻塞或等待某个事件而无法继续执行时，调度器会切换到另一个线程继续执行。

### 进程间通信方式（管道 消息队列 共享内存 信号量 套接字 信号）

1. **管道**(Pipe)是一种常见的进程间通信方式，它允许一个进程的输出作为另一个进程的输入。管道是半双工的，数据只能**单向流动**，且通常用于具有亲缘关系的进程之间如父子进程。此外，还有命名管道(FIFO)，它与管道类似，但允许无亲缘关系的进程间进行通信。
2. **消息队列**(Message Queue)是另一种重要的IPC机制。进程可以将消息发送到消息队列，其他进程则可以从队列中检索消息。这种方式克服了管道的一些限制，如只能承载无格式的字节流以及缓冲区大小受限等。消息队列允许进程之间发送和接收具有特定格式的消息，且可以异步地进行通信。
3. **共享内存**(Shared Memory)也是一种高效的进程间通信方式。多个进程可以访问同一块内存区域，从而直接读写共享的数据。这种方式速度非常快，因为数据不需要在不同进程之间复制。然而，它也需要更复杂的同步机制来防止数据冲突和不一致性。
4. **信号量**(Semaphore)则是一种用于控制多个进程对共享资源的访问的同步机制。它可以被视为一个计数器，用于实现进程间的互斥和同步操作。信号量常用于保护对共享内存或其他资源的访问，以防止发生竞态条件。
5. **套接字**(Socket)通信则是一种更为通用的进程间通信方式，它不仅适用于同一台计算机上的进程间通信，还适用于网络中的不同计算机上的进程间通信。套接字提供了一种标准的接口来发送和接收数据，支持多种协议(如TCP、UDP等)，并具有跨平台和可靠性高的特点。
6. **信号**(Siqnal)也是一种进程间通信的方式，但它主要用于通知接收进程某个事件已经发生。信号是一种软件中断，可以由操作系统或其他进程发送。接收进程在收到信号后，可以根据信号的类型执行相应的操作。

### 管道的原理？有名管道/匿名管道？

管道(Pipe)是一种特殊的文件类型，具有读写两个端口，数据从一个端口流入，从另个端口流出。管道的原理是基于内核缓冲区实现的，**其本质是一个伪文件**。当进程向管道写入数据时，数据首先被存储在内核缓冲区中，然后其他进程可以从该缓冲区读取数据实现了进程间的数据共享和传输。

有名管道(Named Pipe)和匿名管道(Anonymous Pipe)之间的主要区别在于它们的存在范围和使用方式。

**有名管道**也被称为FIFO，它在文件系统创建一个类型为 p 的设备文件，并且可以被多个进程同时打开和使用。这意味着**不相关的进程**可以通过读写这个共享的文件来交换数据。有名管道是基于磁盘上实际的文件进行操作的，因此即使进程退出，管道依然存在。此外，多个进程可以同时向管道写入数据或从管道读取数据，这种特性使得有名管道成为进程间通信的重要手段之一。它常常**用于解决在异步场景下进程间通信的问题**。

**匿名管道是单向的**，常用于 shell 脚本中，只能在**具有亲缘关系的进程间**使用，例如父子进程。它没有磁盘上对应的文件，只存在于内核中，因此读写速度非常快。匿名管道是基于内存进行操作的，主要用于实现进程间的同步和互斥，避免竞争条件和死锁等问题。当数据被写入匿名管道时，它会被存储在内核的缓冲区中，等待其他进程来读取。如果没有数据可读或管道已满，则相应的读写进程会被阻塞，直到有数据可读或管道中有空间可写为止。

### 信号与信号量的区别

信号(Signal)是一种处理异步事件的方式，用于通知接收进程有某种事件发生。信号是比较复杂的通信方式，**它不仅可以用于进程间的通信，还可以发送给进程本身**。在Unix和Linux系统中，信号被广泛应用于处理各种异步事件，如中断、异常或用户自定义的通知。**信号是一种软件中断**，允许一个进程向另一个进程或自身发送简短消息，通知某些事件发生，例如，一个程序可能因用户按下Ctrl+C而接收到SIGINT信号，表示需要中断执行。

信号量(Semaphore)则是一种**进程间通信处理同步互斥的机制**，常用于多线程或多进程环境中。信号量负责协调各个线程或进程，以确保它们能够正确、合理地使用公共资源或关键代码段。**信号量通常被用作一个计数器**，用于控制对共享资源或临界区域的访问。它提供了两种基本操作：`Wait`(等待，有时也称为P操作)和`Release`(释放，有时也称为V操作)。当某个线程或进程需要使用共享资源时，它会执行`Wait`操作，这将减少信号量的值；当资源使用完毕后，它会执行`Release`操作，增加信号量的值。通过这种方式，信号量能够确保在同一时刻只有一个线程或进程能够访问特定资源，从而防止资源冲突和数据不一致。

### 为什么进程崩溃不会对其他进程产生很大影响（进程隔离性 进程独立性）

- **进程隔离性**：每个进程都有自己独立的内存空间，这种进程间的隔离性保证了一个进程崩溃不会直接影响其他进程的执行
- **进程独立性**：每个进程都是独立运行的，它们之间不会共享资源

### 进程是分配资源的基本单位，那么这个资源指的是什么？

虚拟内存、文件句柄、信号量等资源。

### 为什么要设计线程？

因为比如说一个业务场景，需要首先读取文件数据，再解压数据，最后显示解压的数据，对于单进程，会有读取的时候过了一段时间，才进行数据的解压，才显示，各个步骤之间不是并发执行的，影响资源的使用率

对于多进程，又会有进程之间无法通信，共享数据，进程切换也会造成很大开销

线程就可以解决这些问题，线程之间可以并发运行且共享相同的地址空间

### 多线程太多会有什么问题？

切换开销：线程的创建和切换会消耗系统资源，包括内存和CPU

过多的线程可能会导致竞争条件和死锁

### 进程状态（进程五种状态），如何切换？（创建 就绪 运行 阻塞 结束）

![img](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/1715669823633-dcd21d9d-1bc9-44b0-b708-7afda68c2257.webp)‘

- NULL -> 创建状态：一个新进程被创建时的第一个状态；
- 创建状态 -> 就绪状态：当进程被创建完成并初始化后，一切就绪准备运行时，变为就绪状态，这个过程是很快的
- 就绪态 -> 运行状态：处于就绪状态的进程被操作系统的进程调度器选中后，就分配给 CPU 正式运行该进程
- 运行状态 -> 结束状态：当进程已经运行完成或出错时，会被操作系统作结束状态处理
- 运行状态 -> 就绪状态：处于运行状态的进程在运行过程中，由于分配给它的运行时间片用完，操作系统会把该进程变为就绪态，接着从就绪态选中另外一个进程运行
- 运行状态 -> 阻塞状态：当进程请求某个事件且必须等待时，例如请求 I/O 事件
- 阻塞状态 -> 就绪状态：当进程要等待的事件完成时，它从阻塞状态变到就绪状态

### 线程通讯/通信方式？（互斥锁 读写锁 自旋锁）

Linux系统提供了五种用于线程通信的方式：**互斥锁、读写锁、条件变量、自旋锁和信号量**。

- 互斥锁（Mutex）：互斥量(mutex)从本质上说是一把锁，在访问共享资源前对互斥量进行加锁，在访问完成后释放互斥量上的锁
- 条件变量：条件变量用于线程间的同步和通信。它通常与互斥锁一起使用，线程可以通过条件变量等待某个条件满足，当条件满足时，其他线程可以通过条件变量发送信号通知等待线程
- 自旋锁：自旋锁是一种忙等待锁，线程在获取锁时不会进入阻塞状态，而是循环忙等待直到获取到锁。适用于临界区很小且锁的持有时间很短的场景，避免线程频繁切换带来的开销。
- 信号量：信号量是一种计数器，用于控制对共享资源的访问。它可以用来限制同时访问资源的线程数量，或者用于线程间的同步
- 读写锁：读写锁允许多个线程同时读取共享资源，但只允许一个线程进行写操作。适用于读操作频繁、写操作较少的场景，可以提高并发性能

### 管道数据没有进程读取怎么办（阻塞模式/非阻塞模式）（管道缓冲区满了会怎么样）

**阻塞模式**

数据被写入管道缓冲区，等待被读取；当写入的数据量超过管道缓冲区的剩余空间时，写操作将被阻塞，直到有进程读取数据并释放缓冲区中的空间

**非阻塞模式**

如果管道缓冲区已满，写操作不会阻塞，而是立即返回 `-1`，并将 `errno` 设置为 `EAGAIN` 或 `EWOULDBLOCK`，表示当前无法完成写操作

### 匿名管道举几个例子，什么场景会用到

父进程生成数据，传递给子进程进行处理，或子进程生成数据，传递给父进程进行进一步操作，比如一个进程不断生成日志数据，通过管道传递给另一个进程进行分析和处理

### 共享内存的局限性

多个进程可以访问同一块内存区域，从而直接读写共享的数据。它需要更复杂的同步机制来防止数据冲突和不一致性。

### Linux常见的信号有哪些（信号）

- SIGHUP(1)：挂起信号
- SIGINT(2)：中断信号
- SIGQUIT(3)：退出信号
- SIGILL(4)：非法指令信号
- SIGABRT(6)：中止信号
- SIGKILL(9)：杀死信号
- SIGSTOP(19)：停止（暂停）进程信号

### 进程vs线程（线程轻量级）

- 本质区别：进程是操作系统资源分配的基本单位，而线程是任务调度和执行的基本单位
- 在开销方面：每个**进程都有独立的代码和数据空间**（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，**同一类线程共享代码和数据空间**，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小
- 稳定性方面：进程中某个线程如果崩溃了，可能会导致整个进程都崩溃。而进程中的子进程崩溃，并不会影响其他进程。
- 内存分配方面：系统在运行的时候会为每个进程分配不同的内存空间；而对线程而言，除了CPU外，系统不会为线程分配内存（线程所使用的资源来自其所属进程的资源），线程组之间只能共享资源
- 包含关系：没有线程的进程可以看做是单线程的，如果一个进程内有多个线程，则执行过程不是一条线的，而是多条线

### 操作系统和线程调度的边界在哪？用户的代码和调度的边界在哪？

**操作系统与线程调度的边界**：操作系统内核负责线程/进程的调度和管理，用户空间运行应用程序代码并通过系统调用与内核交互。

**用户代码与调度的边界**：用户代码通过系统调用请求操作系统服务，系统调用触发上下文切换，用户代码运行在用户模式，操作系统内核运行在内核模式。调度由内核调度器决定，基于时钟中断、I/O 中断和同步原语等事件进行上下文切换。

### 多线程/多进程？I/O密集型/CPU密集型用多线程还是多进程？

**多线程**

1. 共享同一块内存空间，适合需要频繁共享数据的任务：多线程数据库访问、实时数据处理等
2. 线程的创建和销毁相对进程来说开销更小：处理高并发的网络请求、Web 服务器处理请求、用户界面程序的响应等
3. **I/O 密集型任务**：I/O 操作（如文件读写、网络通信）通常比 CPU 操作慢，可以使用多线程在等待 I/O 操作完成时进行其他任务，从而提高整体性能。

**多进程**

1. 隔离性和安全性：一个进程的崩溃不会影响其他进程，适合需要高隔离性的任务，Web 浏览器（每个标签页作为一个独立进程）、微服务架构等
2. **CPU 密集型任**务：多进程可以充分利用多核 CPU 的性能，因为每个进程可以在不同的 CPU 核心上运行，避免了全局解释器锁（GIL）等问题。

### 运行一个.java文件，有几个进程？

在编译和运行一个 `.java` 文件的整个过程中，可能会有两个独立的进程：一个是编译器进程（如果需要编译），一个是 JVM 进程

### 线程调度以linux为例

1. 线程创建：使用`pthread_create`函数创建线程

2. 调度：线程创建时，内核会将其加入到就绪队列（ready queue）。调度器会根据调度策略和优先级从就绪队列中选择线程。

3. 触发上下文切换：

   保存当前线程的状态（寄存器、程序计数器、虚拟机栈、本地方法栈）

   加载待执行线程的状态。

   更新线程的调度信息

### 线程上下文切换？线程的状态？运行和就绪区别？

保存当前线程的上下文-----> 加载下一个线程的上下文

当前线程：运行---->就绪

下一个线程：就绪---->运行

**`Running`**：当前正在 CPU 上执行的线程。

**`Ready`**：等待调度程序分配 CPU 时间的线程。

在 Java 线程模型中，`Runnable` 状态包含了线程的运行（`Running`）和就绪（`Ready`）状态。在这种模型下，所有能够被调度的线程都被视为 `Runnable`，这包括当前正在运行的线程和准备运行的线程

### 多个进程如何使用同一个文件

- 有名管道实际上在文件系统中表现为一个文件。多个进程可以通过打开这个有名管道文件来进行读写操作
- 文件的I/O操作实际上也是访问同一个文件

### 管道通信缺点?

- 匿名管道只能单向，如果要双向，要创建两个管道；有名管道虽然是双向，需要额外的同步机制（如互斥锁）来确保进程间的数据一致性和防止数据竞争。
- 缓冲区限制：管道通常具有固定大小的缓冲区，当缓冲区满时，写操作可能被阻塞

### 僵尸进程和孤儿进程？

在多进程模型中，为每个客户端分配一个进程来处理请求，服务器的主进程负责监听客户的连接，一旦与客户端连接完成，accept() 函数就会返回一个「已连接 Socket」，这时就通过 fork() 函数创建一个子进程，子进程不需要关心「监听 Socket」，只需要关心「已连接 Socket」；父进程则相反，将客户服务交给子进程来处理，因此父进程不需要关心「已连接 Socket」，只需要关心「监听 Socket」

当子进程调用 exit()系统调用结束自己的生命时，内核会释放该进程的所有资源，包括打开的文件、占用的内存等，实际上内核里还会保留该进程的一些信息，也是会占用内存的。这些信息只有在父进程调用 wait()或 waitpid()系统调用时才会被释放，以便让父进程得到子进程的状态信息。

**僵尸进程**：子进程已经终止，但是其父进程仍在运行，且父进程没有调用 `wait()`或 `waitpid()`等系统调用来获取子进程的状态信息，释放子进程占用的资源，导致子进程的 PCB 依然存在于系统中，但无法被进一步使用。

**孤儿进程**：一个进程的父进程已经终止或者不存在，但是该进程仍在运行。这种情况下，该进程就是孤儿进程。孤儿进程通常是由于父进程意外终止或未及时调用 wait()或 waitpid()等系统调用来回收子进程导致的

### 银行家算法

银行家算法是**避免死锁**的算法

就是在分配给进程资源前，首先判断这个进程的安全性，也就是预执行，判断分配后是否产生死锁现象。如果系统当前资源能满足其执行，则尝试分配，如果不满足则让该进程等待。

通过不断检查剩余可用资源是否满足某个进程的最大需求，如果可以则加入安全序列，并把该进程当前持有的资源回收；不断重复这个过程，看最后能否实现让所有进程都加入安全序列

### 进程调度算法？多级队列具体怎么实现？会有进程饥饿吗？

**先来先服务算法**（FCFS）

最短作业优先算法（SJF）

高响应比优先算法：（等待时间 + 要求服务时间）/ 要求服务时间

时间片轮转调度算法：每个进程被分配一个时间段，如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程

最高优先级调度算法

多级反馈队列调度算法

- 设置了多个队列，赋予每个队列不同的优先级，每个队列优先级从高到低，同时优先级越高时间片越短
- 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成
- 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行
- 短作业可能可以在第一级队列很快被处理完，对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也会更长了

**多级反馈队列调度算法会有进程饥饿吗**

是会有的，如果高优先级队列中总是有大量高优先级的短作业，低优先级队列中的进程可能长期得不到调度，导致饥饿

### 进程创建/线程创建

进程的创建通常通过系统调用 `fork()` 或 `exec()` 完成

线程的创建通过 `pthread_create()`

每个进程分配独立的资源，包括内存地址空间、文件描述符、I/O设备等。进程之间的资源是隔离的，彼此不能直接访问对方的内存和资源。开销较大

线程共享所在进程的资源，如内存地址空间、文件描述符、全局变量等。线程创建的开销相对较小，因为线程无需重新分配进程级别的资源，只需创建一个新的执行上下文（如栈、寄存器状态等）

### 线程结束方法

通过**标志位**让线程在运行时检测标志位的状态，然后自行退出，` private volatile boolean running = true;`

`public void run() {        `

`while (running)`

通过 `volatile` 修饰的 `running` 标志位来控制线程运行

是使用 `Thread` 类的 `interrupt()` 方法来中断线程

线程是通过 `ExecutorService` 提交的任务，则可以通过 `Future.cancel()` 来取消线程任务

## 操作系统：系统结构

### 内核态和用户态？

内核态和用户态是操作系统中的两种运行级别，它们之间有着明显的区别。简单来说，内核态是操作系统**内核运行的模式**，具有**极高**的权限，可以访问所有硬件资源和执行所有指令，包括特权指令。而用户态则是**用户程序运行的模式**，其权限受到限制，只能执行非特权指令，且不能直接访问操作系统内核数据结构和程序。

这两种状态的主要差别在于，处于用户态执行的程序不能直接访问操作系统内核数据结构和程序，其所处于占有的处理器是可被抢占的，且其能访问的内存空间和对象受到限制。而内核态下运行的程序则能访问所有的内存空间和对象，且所占有的处理器不允许被抢占。

**为什么要划分**

1. 安全性：通过划分内核态和用户态，可以防止用户程序对系统造成不可修复的破坏，因为用户程序在用户态下运行，其操作受到严格的限制，不能直接操作关键系统资源，从而保护了系统的安全性。
2. 稳定性：如果用户程序崩溃或出现错误，由于它运行在用户态，因此不会影响整个系统的稳定性。内核态下运行的操作系统可以继续正常工作，而不受用户程序的干扰
3. 性能：虽然内核态和用户态之间的切换会耗费一定的性能资源，但这种划分也带来了性能上的优势。因为内核态具有更高的权限，可以执行一些需要较高特权级别的操作，如直接访问硬件，这有助于提高系统的性能。同时，通过将关键操作限制在内核态，可以确保这些操作的高效和准确执行。

### 用户态陷入系统调用，用户态切换内核态？哪些命令和函数会陷入系统调用？

**切换过程**

当用户态陷入系统调用，切换到内核态的过程，首先是由用户态的进程主动发起一个系统调用请求。这个请求通常是由于用户态的程序需要执行一些只有操作系统内核才能完成的敏感或关键操作，比如访问硬件资源、进行进程间通信等。

在发起系统调用时，CPU会通过一个特殊的指令，比如x86架构中的`int`指令或者`syscall`指令，来触发一个软中断。这个软中断会导致CPU暂停当前用户态程序的执行，转而跳转到预先设定好的中断处理程序中去执行。

中断处理程序是操作系统内核的一部分，它在内核态中运行。当中断处理程序开始执行时，就意味着操作系统已经从用户态切换到了内核态。在内核态中，操作系统可以访问所有的硬件资源和系统数据，拥有最高的权限。

**哪些命令和函数会陷入系统调用?**

关于哪些命令和函数会陷入系统调用，实际上在Linux等类Unix系统中，很多常见的命令和函数最终都会陷入系统调用。比如，当我们使用`open`函数打开一个文件时，这个函数内部最终会调用系统提供的`open`系统调用来完成实际的文件打开操作。同样地，像`write`、`read`、`close`等文件操作函数，以及`socket`、`bind`、`listen`、`accept` 等网络编程相关的函数，它们底层都会陷入相应的系统调用来完成实际的工作。

### 系统调用是安全的？内核态是安全的？

1. 用户态与内核态的隔离：用户态进程只能访问自己的内存空间，不能直接访问内核空间
2. 上下文切换：软中断，CPU会保存当前的用户态上下文并安全地切换到内核态。这种机制确保了用户态程序不能直接跳转到内核代码执行。
3. 内核代码通常经过严格的代码审计和测试，以发现和修复安全漏洞

### malloc会陷入内核态吗？

会，调用`brk()`和`mmap()`，就是系统调用

## 操作系统：网络系统

### IO多路复用？select，poll，epoll的区别？

**IO多路复用**

IO多路复用是一种高效的IO处理方式，它允许单个进程或线程同时监视多个文件描述符，也就是只使用一个进程来维护多个 Socket ，如网络连接或文件句柄。当这些描述符中的任何一个就绪时，比如有数据可读或可写，多路复用机制就能够通知应用程序进行相应的读写操作。这种机制的核心优势在于它可以在不增加额外线程或进程的情况下，处理大量的并发连接，从而显著地提高系统的并发性和响应能力。

常见的IO多路复用技术包括`select`、`poll`和`epoll`等。这些技术各有特点，但核心思想都是通过一个线程来管理多个连接，减少系统资源的消耗，并提高程序运行的效率。

**`select`，`poll`，`epoll`的区别**

`select`是最早的一种I/O多路复用技术。将已连接的 Socket 都放到一个文件描述符集合，用固定长度的 BitsMap，然后调用 `select `函数将文件描述符集合拷贝到内核里，内核遍历文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写，遍历文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写。

`poll`则是`select`的一种改进方案。它使用动态数组，以链表形式来表示被监视的文件描述符及其事件。与`select`相比，`poll`没有文件描述符数量的限制，因为它基于链表来存储。然而，`poll`仍然需要两次遍历，两次拷贝

`epoll`则是Linux特有的I/O多路复用技术，它在很多方面都优于`select`和`poll`。首先，epoll只在初始时完成一次文件描述符的注册，避免了每次调用时的拷贝开销。其次，`epoll`采用回调函数的方式，只有当一个或多个文件描述符就绪时，才会调用回调函数并通知用户空间，这使得`epoll`在处理大量文件描述符时仍然能保持高效。最后，`epoll`返回时已经明确指出了哪些文件描述符是就绪的，因此无需再像`select`和`poll`那样进行额外的遍历操作。

### `epoll`实现？`epoll`的事件触发？(边缘触发/水平触发)

**`epoll`的实现**

`epoll` 在内核里使用**「红黑树」**来关注进程所有待检测的 Socket，红黑树是个高效的数据结构，增删改一般时间复杂度是 O(logn)，通过对这棵黑红树的管理，不需要像 `select/poll `在每次操作时都传入整个 Socket 集合，减少了内核和用户空间大量的数据拷贝和内存分配。

epoll 使用事件驱动的机制，内核里维护了一个「链表」来记录就绪事件，只将有事件发生的 Socket 集合传递给应用程序，不需要像 select/poll 那样轮询扫描整个集合（包含有和无事件的 Socket ），大大提高了检测的效率。

**`epoll`的事件触发有几种方式**

`epoll`的事件触发主要有两种方式。第一种是边缘触发（ET），也就是说，只有当文件描述符的状态从不就绪变为就绪时，`epoll`才会发出通知。一旦通知过后，除非状态再次发生变化，否则不会再次通知。第二种是水平触发（LT），这种模式下，只要文件描述符处于就绪状态，`epoll`就会持续发出通知，直到数据处理完毕

### 零拷贝技术?（mmap sendfile）（文件读取过程）

零拷贝技术是为了优化磁盘的技术，提高系统的吞吐量

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240806201922209.png" alt="image-20240806201922209" style="zoom:50%;" />

不进行优化的话，正常的文件传输，也就是`read()`和`write()`，需要经过4次用户态的上下文切换（因为是两次系统调用，一次是 `read()` ，一次是 `write()`），4次数据拷贝（如图）

实现零拷贝实际上就是希望减少「上下文切换」和「数据拷贝」的次数，有两种方式

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240806201943869.png" alt="image-20240806201943869" style="zoom:50%;" />

第一种：mmap + write

用 `mmap()` 替换 `read()` ，mmap() 系统调用函数会直接把内核缓冲区里的数据「映射」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作

仍然需要 4 次上下文切换，仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240806202015839.png" alt="image-20240806202015839" style="zoom:50%;" />

第二种：sendfile

它可以替代前面的 read() 和 write() 这两个系统调用，这样就可以减少一次系统调用，也就减少了 2 次上下文切换的开销。

它可以直接把内核缓冲区里的数据拷贝到 socket 缓冲区里，不再拷贝到用户态，再结合SG-DMA控制器，就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中

零拷贝（Zero-copy）技术，因为我们没有在内存层面去拷贝数据，也就是说全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的

### 有哪些IO模型（阻塞IO/非阻塞IO区别）(BIO NIO AIO)

- BIO(阻塞I/O模型)：应用程序发起I/O操作后会被阻塞，直到操作完成才返回结果。
- NIO(非阻塞I/O模型)：应用程序发起I/O操作后立即返回，不会被阻塞，但需要不断轮询或者使用`select/poll/epoll`等系统调用来检查I/O操作是否完成。
- I/O复用模型：通过select、poll、epoll等系统调用，应用程序可以同时等待多个I/O操作，当其中任何一个I/O操作准备就绪时，应用程序会被通知
- AIO(异步I/O模型)：应用程序发起I/O操作后可以立即做其他事情，当I/O操作完成时，应用程序会得到通知。异步I/O模型由操作系统内核完成I/O操作，应用程序只需等待通知即可

###   非阻塞需要轮询，有没有什么方案解决这个问题？

使用 `select`、`poll` 或 `epoll` 等系统调用在文件描述符可用时返回，非阻塞 I/O 操作此时应该能够立即完成，从而避免了不断轮询检查的开销。

当文件描述符通过 I/O 多路复用检测到可用时，不论是使用阻塞 I/O 还是非阻塞 I/O，都可以立即执行 I/O 操作并返回。

### `select`，`poll`，`epoll`分别的适用场景

`select`：支持几乎所有的操作系统，并且简单，但是文件描述符数量有限制，一般为 1024 个，小型 Web 服务器

`poll`：与 `select` 类似，但没有文件描述符数量限制，适用于文件描述符数量可能会动态变化的场景，并发连接数在几百到几千之间的聊天应用

`epoll`：专门为 Linux 操作系统设计的高效 I/O 多路复用机制，适用于大规模、高并发的场景，但是只能在Linux环境下，如大型 Web 服务器、反向代理服务器、大型聊天应用

这三个里面只有`select`可以在Windows环境下使用

### 同步/异步

同步：同步任务在前一个任务完成后才能执行下一个任务，调用者在等待操作完成时会被阻塞，直到任务完成

异步：异步任务可以在前一个任务未完成时启动另一个任务，调用者在等待操作完成时不会被阻塞，可以继续执行其他任务

### `epoll`系统调用有哪些 api ?

- **`epoll_create`** 和 **`epoll_create1`**：用于创建 `epoll` 实例。
- **`epoll_ctl`**：用于控制 `epoll` 实例，添加、修改或删除文件描述符。
- **`epoll_wait`**：用于等待文件描述符的事件，并获取发生的事件列表。

### 内核读取文件(从磁盘读取文件过程)（DMA）

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240806201845713.png" alt="image-20240806201845713" style="zoom:50%;" />

- 用户进程调用 read 方法，向操作系统发出 I/O 请求，请求读取数据到自己的内存缓冲区中，进程进入阻塞状态；
- 操作系统收到请求后，进一步将 I/O 请求发送 DMA，然后让 CPU 执行其他任务；
- DMA 进一步将 I/O 请求发送给磁盘；
- 磁盘收到 DMA 的 I/O 请求，把数据从磁盘读取到磁盘控制器的缓冲区中，当磁盘控制器的缓冲区被读满后，向 DMA 发起中断信号，告知自己缓冲区已满；
- DMA 收到磁盘的信号，将磁盘控制器缓冲区中的数据拷贝到内核缓冲区中，此时不占用 CPU，CPU 可以执行其他任务；
- 当 DMA 读取了足够多的数据，就会发送中断信号给 CPU；
- CPU 收到 DMA 的信号，知道数据已经准备好，于是将数据从内核拷贝到用户空间，系统调用返回；

`pread` 系统调用允许从指定的文件偏移量读取数据，而不改变文件的当前偏移量。

### epoll为什么不用hashmap呢?

- hashmap内存开销比红黑树大
- 红黑树在处理范围查询和有序数据时表现较好

### pageCache（页缓存）是什么？

`page cache`（页缓存）是在内存中的一个机制，它也叫磁盘高速缓存，文件传输过程，其中第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是磁盘高速缓存

读写磁盘相比读写内存的速度慢太多了，所以我们应该想办法把「读写磁盘」替换成「读写内存」。于是，我们会通过 DMA 把磁盘里的数据搬运到内存里，这样就可以用读内存替换读磁盘。

用 **PageCache 来缓存最近被访问的数据**，读磁盘数据的时候，优先在 PageCache 找，如果数据存在则可以直接返回；如果没有，则从磁盘中读取，然后缓存 PageCache 中

### 从 a文件 copy 到另外一个目录，b作为一个从 a目录 copy 到一个b 目录这样的一个文件，操作过程中间包含了哪些系统调用？这里面执行了几次拷贝的动作？

**系统调用**

`open`：两次，分别为源文件和目标文件

`read`：从源文件读取

`write`：向目标文件写入

（中间这两个步骤可以改为：

**`sendfile`** - 从源文件 `a` 直接传输数据到目标文件 `b`。）

`close`：两次，分别为源文件和目标文件

**拷贝动作**

**`read` 和 `write`**：需要多次拷贝（4 次），包括内核空间和用户空间之间的拷贝。

**`sendfile`**：减少拷贝次数（2 次），主要是在内核空间中完成数据传输。

## 操作系统：Linux命令

### Linux常用命令有哪些？

**查看系统资源**

1. `top`命令：这是一个实时显示系统中各个进程的资源占用状况的监视器。它可以展示CPU使用率、内存使用率、进程状态等信息。通过top命令，用户可以快速识别哪些进程占用了过多的系统资源。
2. `ps`命令：用于显示当前系统中活动进程的信息。通过执行`ps-ef`或`ps aux`，用户可以查看所有进程的详细信息，包括进程ID、CPU和内存使用率等。此外，结合`grep`命令，可以方便地查找特定进程。
3. `free`命令：用于显示系统的内存使用情况。执行`free-m`可以以MB为单位显示内存使用情况，包括总内存、已用内存、空闲内存等信息。
4. `df`命令：用于显示磁盘分区上的可用和已使用的磁盘空间。执行`df-h`可以以人类可读的格式(如GB、MB)显示磁盘空间使用情况。
5. `sar`命令：这是一个系统活动报告工具，可以用于收集、报告和保存系统活动信息，通过`sar`命令，用户可以查看CPU使用率、内存使用率、I/O等历史数据。
6. `vmstat`命令：用于显示关于系统虚拟内存、进程、CPU活动等的统计信息。通过`vmstat`命令，用户可以实时监控系统的资源使用情况。

**查看日志**

1. `tail`命令：常用于查看实时变化的日志文件。例如，`tail -f/var/log/syslog`可以实时查看系统日志的最新内容。此外，`tail -n「行数] 文件名`可用于查看文件的最后几行。
2. `cat`命令：用于显示文件内容。结合`grep`命令，可以搜索关键字附近的日志内容。例如，`cat /var/log/syslog|grep'关键字'`可以搜索包含特定关键字的日志行。
3. `less`命令：这是一个强大的文本查看器，允许用户向前和向后翻页查看文件内容。对于大型日志文件，`less`命令提供了方便的导航和搜索功能。
4. `head`命令：与`tail`命令相反，`head`命令用于查看文件的开头部分。例如，`head-n [行数] 文件名`可以查看文件的前几行。
5. `more`命令：以全屏幕的方式按页显示文件内容，适合查看长文件。

**查看网络状态**

1. `ifconfig`：此命令用于显示和配置网络接口的详细信息，包括IP地址、MAC地址、网络掩码等。例如，`ifconfig` 可以列出所有活动网卡的信息。
2. `ip`：`ip` 命令是一个多功能的网络工具，用于显示或操作路由、网络设备、策略路由和隧道。例如，`ip addr` 可以显示网络接口的详细信息，类似于 `ifconfig`；`ip route` 可以显示路由表信息
3. `netstat`： 此命令用于显示网络连接、路由表、接口统计等网络相关信息。例如，`netstat -an` 可以显示所有活动的网络连接和监听的端口。
4. `telnet`：可以用来测试与远程主机的连接
5. `ss`：`ss` 命令是一个比 `netstat `更强大的工具，用于查看系统的 socket 统计信息。它可以提供更详细的信息，并且比`netstat` 更快。例如，`ss-tuln` 可以列出所有TCP和UDP的监听端口。
6. `ping`：此命令用于测试网络的连通性，通过发送ICMP回显请求数据包到目标主机，并等待接收回显回复数据包来判断网络是否连通。例如，`ping www.baidu.com` 可以测试到Baidu的网络连通性。

### Java程序线上CPU使用率100%如何排查？（Linux使用率）

top->jstack

1. 使用`top`命令查看进程CPU占用情况
   - 首先，运行 `top`命令，观察各个进程的CPU占用情况
   - 通过 `top` 命令，可以快速找到CPU占用率最高的Java进程。注意记录下该进程的PID(进程ID)。
2. 使用`top -Hp`命令查看线程CPU占用情况
   - 确定高CPU占用的Java进程后，使用`top-Hp [PID]`命令查看该进程中各个线程。
     的CPU占用情况。
   - 这里的`[PID]`是之前通过 `top` 命令找到的Java进程的PID
   - 通过这个命令，我们可以找到具体是哪个线程占用了过多的CPU资源。
3. 使用`jstack`命令查看线程堆栈信息
   - 确定了高CPU占用的线程后，我们需要获取该线程的堆栈信息。这时可以使用`jstack` 命令。
   - 运行`jstack[PID] > stack.txt` 将]ava进程的线程堆栈信息输出到文本文件中。
   - 在`stack.txt`文件中搜索之前通过 `top-Hp` 命令找到的线程ID(需要将线程ID转换成16进制进行搜索)，以定位到具体线程的堆栈信息。
4. 分析线程堆栈信息定位问题
   - 通过分析堆栈信息，我们可以找到线程中正在执行的方法调用链，从而定位到可能导致CPU占用率上升的代码段。
   - 常见的导致CPU占用率上升的原因包括无限循环、复杂的算法和计算、频繁的IO操作以及线程问题(如竞争条件、死锁等)。

### 怎么杀死进程?

```bash
kill <PID>
```

### 对一个txt文件进行操作，提取第四行第四列的内容

`awk`可以用来提取特定的行和列

```bash
awk 'NR==4 {print $4}' filename.txt
```

`NR==4`：匹配第四行。

`$4`：打印第四列的内容。

`filename.txt`：文件名。

### top结果CPU占用会超过100%吗？如何限制CPU利用率在100%之下？

CPU 占用率的显示可能会超过 100%，`top` 命令显示的 CPU 占用率是所有核心总和的一个百分比。例如，在一个有 4 个核心的系统中，CPU 占用率的最大值可以达到 400%，如果一个进程在 4 核上每个核心使用 100% 的 CPU，那么它的 CPU 占用率显示为 400%。

# Netty

### redis，nginx，netty 是依赖什么做的这么高性能？（多Reactor多进程、单Reactor单进程 Reactor）

主要是依赖**Reactor 模式**，也就是来了一个事件，Reactor 就有相对应的反应/响应

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send

**Redis**

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240808190811162.png" alt="image-20240808190811162" style="zoom: 67%;" />

Redis 是由 C 语言实现的，在 Redis 6.0 版本之前采用的正是「单 Reactor 单进程」的方案

- Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型
- 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件
- 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程

存在缺点：

- 第一个缺点，因为只有一个进程，无法充分利用 多核 CPU 的性能
- 第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，如果业务处理耗时比较长，那么就造成响应的延迟

**Netty**

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240808191057217.png" alt="image-20240808191057217" style="zoom:67%;" />

- 主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept 获取连接，将新的连接分配给某个子线程
- 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件
- 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应
- Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

优势：

- 主线程和子线程分工明确，主线程只负责接收新连接，子线程负责完成后续的业务处理
- 主线程和子线程的交互很简单，主线程只需要把新连接传给子线程，子线程无须返回数据，直接就可以在子线程将处理结果发送给客户端

**nginx** 

nginx是多 Reactor 多进程方案，不过方案与标准的多 Reactor 多进程有些差异

具体差异表现在主进程中仅仅用来初始化 socket，并没有创建 mainReactor 来 accept 连接，而是由子进程的 Reactor 来 accept 连接

### Netty是什么

1. Netty 是⼀个 **基于 NIO** 的 client-server(客户端服务器)框架，使⽤它可以快速简单地开发⽹络应⽤程序。
2. 它极⼤地简化并优化了 TCP 和 UDP 套接字服务器等⽹络编程,并且性能以及安全性等很多⽅⾯甚⾄都要更好。

### BIO,NIO 和 AIO？（阻塞IO/非阻塞IO）

- **BIO：** 同步阻塞 I/O 模式，数据的读取写⼊必须阻塞在⼀个线程内等待其完成
- **NIO**：同步⾮阻塞的 I/O 模型，线程不断地轮询调用 `read` 操作来判断是否有数据，提供了 Channel , Selector ， Buffer等抽象
- **AIO**：异步⾮阻塞的 IO 模型，当后台处理完成，操作系统会通知相应的线程进⾏后续的操作

### 直接用 NIO和用 Netty？（Netty的优势）

对编程功底要求⽐较⾼，⽽且，NIO 在⾯对断连重连、包丢失、粘包等问题时处理过程⾮常复杂。Netty相对来说有这些优势

- 统⼀的 API，⽀持多种传输类型，阻塞和⾮阻塞的
- 简单⽽强⼤的线程模型
- ⾃带编解码器解决 TCP 粘包/拆包问题
- 安全性不错，有完整的 SSL/TLS 以及 StartTLS ⽀持
- ⽐直接使⽤ Java 核⼼ API 有更⾼的吞吐量、更低的延迟、更低的资源消耗和更少的内存复制

### Netty 应用场景？

Netty 主要⽤来做**⽹络通信**：

1. **作为 RPC 框架的⽹络通信⼯具**：⽐如我调⽤另外⼀个节点的⽅法的话，⾄少是要让对知道我调⽤的是哪个类中的哪个⽅法以及相关参数吧
2. 可以聊天类似微信的即时通讯系统
3.  **实现消息推送系统**：比如市面上的像Nacos，RocketMQ、Dubbo

### Netty 的核心组件？

**Bytebuf（字节容器）**

⽹络通信最终都是通过字节流进⾏传输的。 `ByteBuf` 就是 Netty 提供的⼀个字节容器，其内部是⼀个字节数组。 当我们通过 Netty 传输数据的时候，就是通过 `ByteBuf `进⾏的，可以将 `ByteBuf `看作是 Netty 对 `ByteBuffer` 字节容器的封装和抽象

**Bootstrap 和 ServerBootstrap（启动引导类）**

1. `Bootstrap` 通常使⽤ `connect()` ⽅法连接到远程的主机和端⼝，作为⼀个 Netty TCP 协议通信中的客户端
2. `ServerBootstrap` 通常使⽤ `bind()` ⽅法绑定本地的端⼝上，然后等待客户端的连接
3. `Bootstrap` 只需要配置⼀个线程组 `EventLoopGroup` ,⽽ `ServerBootstrap` 需要配置两个线程组`EventLoopGroup` ，⼀个⽤于接收连接，⼀个⽤于具体的 IO 处理。第一个EventLoopGroup通常只有一个EventLoop，通常叫做bossGroup，负责客户端的连接请求

**Channel（⽹络操作抽象类）**

`Channel` 接⼝是 Netty 对⽹络操作抽象类。通过 `Channel` 我们可以进⾏ I/O 操作。

**EventLoop（事件循环）**

`EventLoop`的主要作⽤实际就是责监听⽹络事件并调⽤事件处理器进⾏相关 I/O 操作（读写）的处理

![image-20240808202041747](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240808202041747.png)

**ChannelHandler（消息处理器） 和 ChannelPipeline（ChannelHandler 对象链表）**

`ChannelHandler`是消息的具体处理器，主要负责处理客户端/服务端接收和发送的数据，当`Channel` 被创建时，它会被⾃动地分配到它专属的 `ChannelPipeline`

 当`ChannelHandler` 被添加到的 `ChannelPipeline` 它得到⼀个 `ChannelHandlerContext` ，它代表⼀个` ChannelHandler` 和 `ChannelPipeline` 之间的"绑定"

**ChannelFuture（操作执⾏结果）**

Netty 中所有的 I/O 操作都为异步的，我们不能⽴刻得到操作是否执⾏成功

可以通过 `ChannelFuture` 接⼝的 `addListener()` ⽅法注册⼀个` ChannelFutureListener `，当操作执⾏成功或者失败时，监听就会⾃动触发返回结果

### Netty-`NioEventLoopGroup` 默认的构造函数会起多少线程呢？

`NioEventLoopGroup` 默认的构造函数实际会起的线程数为 `CPU核⼼数*2`

### `Reactor`/`Proactor`(非阻塞同步网络模式/异步网络模式)

<img src="https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240808204333271.png" alt="image-20240808204333271" style="zoom:67%;" />

![image-20240808204406736](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240808204406736.png)

- Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件。
- Proactor 是异步网络模式， 感知的是已完成的读写事件

Reactor 可以理解为「来了事件操作系统通知应用进程，让应用进程来处理」，而 Proactor 可以理解为「来了事件操作系统来处理，处理完再通知应用进程」

### 主从多线程 Reactor

⼀组 NIO 线程负责接受请求，⼀组 NIO 线程处理 IO 操作

### Netty 线程模型？

Netty 主要靠 `NioEventLoopGroup` 线程池来实现具体的线程模型的

实现服务端的时候，⼀般会初始化两个线程组：

- `bossGroup`：接收连接
- `workerGroup`：负责具体的处理，交由对应的 `Handler` 处理

单线程模型：`eventGroup`既用于处理客户端连接，又负责具体的处理

单线程模型：⼀个 Acceptor 线程只负责监听客户端的连接，⼀个 NIO 线程池负责具体处理

主从多线程模型：从⼀个 主线程 NIO 线程池中选择⼀个线程作为 `Acceptor` 线程，绑定监听端⼝，接收客户端连接的连接，其他线程负责后续的接⼊认证等⼯作。连接建⽴完成后`SubNIO `线程池负责具体处理 I/O 读写

### Netty 服务端和客户端的启动过程？（Netty启动过程）

**服务端**

1. 创建了两个 `NioEventLoopGroup` 对象实例： `bossGroup` 和 `workerGroup`
2. 创建了⼀个服务端启动引导类： `ServerBootstrap`
3. 通过 `.group()` ⽅法给引导类 `ServerBootstrap` 配置两⼤线程组，确定了线程模型
4. 通过 `channel()` ⽅法给引导类 `ServerBootstrap` 指定了 IO 模型为 NIO
5. `bind()`绑定端口

**客户端**

1. 创建⼀个 `NioEventLoopGroup `对象实例
2. 创建客户端启动的引导类是 `Bootstrap`
3. 通过 `.group() `⽅法给引导类 `Bootstrap` 配置⼀个线程组
4. 通过` channel()` ⽅法给引导类 `Bootstrap` 指定了 IO 模型为 NIO
5. 通过 `.childHandler()` 给引导类创建⼀个 `ChannelInitializer` ，然后指定了客户端消息的业务处理逻辑 `HelloClientHandler` 对象
6. 调⽤ `Bootstrap` 类的 `connect()` ⽅法设定好ip和端口进⾏连接

### Netty 长连接、心跳机制了解么?

**TCP长连接和短链接**

TCP 在进行读写之前，`server` 与 `client`之间必须提前建立一个连接。建立连接的过程，需要三次握手，释放/关闭连接的话需要四次挥手。这个过程是比较消耗网络资源并且有时间延迟的。

所谓，短连接说的就是 `server` 端 与 `client `端建立连接之后，读写完成之后就关闭掉连接，如果下一次再要互相发送消息，就要重新连接。短连接的优点很明显，就是管理和实现都比较简单，缺点也很明显，每一次的读写都要建立连接必然会带来大量网络资源的消耗，并且连接的建立也需要耗费时间。

长连接说的就是 `client` 向 `server` 双方建立连接之后，即使 `client`与`server` 完成一次读写，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。长连接的可以省去较多的TCP 建立和关闭的操作，降低对网络资源的依赖，节约时间。对于频繁请求资源的客户来说，非常适用长连接。

**为什么需要心跳机制，Netty中心跳机制了解吗**

在 TCP 保持长连接的过程中，可能会出现断网等网络异常出现，异常发生的时候， `client`与`server` 之间如果没有交互的话，它们是无法发现对方已经掉线的。为了解决这个问题，我们就需要引入心跳机制。

心跳机制的工作原理是：在 `client` 与 `server` 之间在一定时间内没有数据交互时，即处于 `idle` 状态时，客户端或服务器就会发送一个特殊的数据包给对方，当接收方收到这个数据报文后，也立即发送一个特殊的数据报文，回应发送方，此即一个 `PING-PONG` 交互。所以，当某一端收到心跳消息后，就知道了对方仍然在线，这就确保 TCP连接的有效性。

TCP 实际上自带的就有长连接选项，本身是也有心跳包机制，也就是TCP的选项：`SO_KEEPALIVE`。但是，TCP 协议层面的长连接灵活性不够。所以，一般情况下我们都是在应用层协议上实现自定义心跳机制的，也就是在 Netty 层面通过编码实现。通过 Netty 实现心跳机制的话，核心类是 `IdleStateHandler`

### Netty-零拷贝

Netty 中的零拷贝与操作系统层面上的不太一样，操作系统通过mmap或者sendfile来实现，Netty的零拷贝完全是在用户态层面的，他更偏向于优化数据操作这样的概念

- Netty 提供了 `CompositeByteBuf` 类，它可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf，避免了各个 ByteBuf 之间的拷贝
- 通过 wrap 操作，我们可以将 byte[] 数组、ByteBuf、ByteBuffer等包装成一个 Netty ByteBuf 对象，进而避免了拷贝操作
- ByteBuf 支持 slice 操作，因此可以将 ByteBuf 分解为多个共享同一个存储区域的 ByteBuf，避免了内存的拷贝
- 通过 `FileRegion` 包装的`FileChannel.tranferTo` 实现文件传输，可以直接将文件缓冲区的数据发送到目标 `Channel`，避免了传统通过循环 write 方式导致的内存拷贝问题

# Nacos

### 什么是Nacos,Nacos 的主要功能有哪些

Nacos 是一种分布式服务发现和配置管理工具，它可以用于服务注册、健康检查、负载均衡、故障恢复、动态配置等方面，其中服务注册和健康检查是 Nacos 最核心的功能

### Nacos 的服务注册？

- Nacos 的服务注册是通过 **Agent** 进程实现的。
- 当一个服务启动时，它会向 Nacos 的 Agent 发送一个注册请求，Agent 会将服务的元数据存储在本地，并将服务的信息发送到 Nacos 的 Server 上。
- 当服务停止时，它会向 Agent 发送一个注销请求，Agent 会将服务的元数据从本地删除，并将服务的信息从 Nacos 的 Server 上删除。

### Nacos 的健康检查？

- 当一个服务注册后，它会向 Nacos 的 Agent 发送一个健康检查请求，Agent 会定期向服务发送健康检查请求，并根据服务的响应结果来判断服务的健康状态。
- 如果服务的健康状态发生变化，Agent 会将服务的状态信息发送到 Nacos 的 Server 上，以便其他服务可以及时发现和处理。

### Nacos 的服务发现方式？

Nacos 支持多种服务发现方式，包括 DNS、HTTP API、RPC API、Service Mesh 等。

### Nacos 的优点？

- 支持多种服务发现方式和多种协议，可以满足不同场景下的需求。
- 支持多种负载均衡算法和故障恢复机制，可以提高系统的可用性和稳定性。
- 支持动态配置，可以帮助开发人员更好地管理和维护配置信息。
- 支持多数据中心，可以帮助开发人员和运维人员更好地管理和维护分布式系统。

### Nacos 如何实现自动刷新配置以及原理

Nacos客户端持续监听配置变化的核心是长轮询机制。具体来说，客户端会周期性地（默认是每30秒）向Nacos服务器发送请求，询问自上一次查询之后，被监听的配置是否有更新。如果服务器响应表示配置发生了变化，客户端则会立即向服务器发出查询配置的请求，获取最新的配置数据。

### 在RPC项目中使用Nacos

- 服务提供者把自己的协议地址注册到Nacos server
- 服务消费者需要从Nacos Server上去查询服务提供者的地址（根据服务名称）
- Nacos Server需要感知到服务提供者的上下线的变化
- 服务消费者需要动态感知到Nacos Server端服务地址的变化

# GIT

### git命令有哪些

**`git init`**：初始化一个新的 Git 仓库

**`git clone <repository>`**：克隆一个远程仓库到本地

**`git add <file>`**：将文件添加到暂存区

**`git commit -m "message"`**：提交暂存区的更改到本地仓库。

**`git log`**：查看提交历史

**`git rm <file>`**：删除工作区和暂存区中的文件

**`git branch`**：列出、创建或删除分支

**`git merge <branch>`**：将指定分支的更改合并到当前分支

**`git remote`**：管理远程仓库

**`git pull`**：从远程仓库获取并合并最新的提交

**`git push`**：将本地的提交推送到远程仓库



# RPC

### RPC了解多少？都有哪些？

- **user**（服务调用方）
- **user-stub**（调用方的本地存根）
- **RPCRuntime**（RPC通信者）
- **server-stub**（服务端的本地存根）
- **server**（服务端）

**远程过程调用**，它是利用**网络**从远程计算机上请求服务，可以理解为把程序的一部分放在其他远程计算机上执行。通过**网络通信**将调用请求发送至远程计算机后，利用远程计算机的系统资源执行这部分程序，最终返回远程计算机上的执行结果。

比如，微服务项目：服务提供者和服务消费者运行在两台不同物理机上的不同进程内，它们之间的调用相比于本地方法调用，可称之为远程方法调用，简称 RPC

RPC框架有：Dubbo、GRPC、

### 介绍一下你的项目（你的RPC是怎么设计的）

![image-20240812151605117](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240812151605117.png)

#### 代理层

代理层的设计是为了简化和透明化客户端与服务端之间的远程调用过程。代理层通常包含客户端代理和服务端代理，它们负责处理调用的序列化、反序列化、网络传输，以及异常处理等操作

客户端侧的代理层，负责将本地方法调用转换为网络请求，并发送给服务端；服务端侧的代理层，负责接收网络请求，将其转换为本地方法调用，然后将结果返回给客户端

**客户端代理层的职责：**

- 方法拦截：代理层通过实现 `InvocationHandler` 接口，拦截客户端对接口方法的调用。

- 请求构建：在方法调用时，代理层会根据方法名、参数、接口名等信息构建 `RpcRequest` 对象。

- 请求传输：Netty 实现的 `NettyRpcClient`将请求发送到服务端，并接收 `RpcResponse`。

- 响应处理：代理层负责检查 `RpcResponse` 的合法性，包括校验请求和响应的匹配、检查响应状态码等。

- 结果返回：将远程调用的结果返回给客户端，就像调用本地方法的返回值一样。

执行流程：

- 如果某个字段标注了 `@RpcReference`，则表示这个字段需要被注入一个 RPC 客户端代理对象，以调用远程服务

-  代理对象生成：通过 `getProxy(Class<T> clazz)` 方法，使用 `Proxy.newProxyInstance` 方法动态生成代理对象
- 方法调用拦截：当客户端调用代理对象的方法时，调用会被拦截并传递到 `invoke` 方法中。
- 请求构建与传输：
  - 在 `invoke` 方法中，根据调用的 `Method` 对象和参数，构建 `RpcRequest` 对象。
  - 然后通过`NettyRpcClient`将请求发送到服务端，则发送请求后会返回一个 `CompletableFuture<RpcResponse<Object>>`，等待异步返回结果
- 响应校验与处理：在接收到响应后，代理层会调用 `check` 方法来验证响应的合法性，如果验证通过，则返回结果数据。

**服务端代理层**

- **服务获取**：`handle` 方法首先通过 `serviceProvider` 获取对应的服务实例。`serviceProvider` 提供了对服务的管理和获取功能。
- 使用 Java 反射机制根据 RPC 请求中的方法名和参数类型找到对应的方法，并调用该方法。

#### 注册中心层

注册中心层的核心功能是管理服务的注册和发现，我使用的是Nacos，让服务能够被正确地注册、管理，并在需要时供客户端查询和调用

这里主要的方法有：添加服务、获取服务、发布服务

当一个服务在应用程序中被标记为需要发布，`NacosServiceProviderImpl` 会自动执行以下步骤：

1. **检查并添加服务**：检查服务是否已经注册，未注册则添加到本地管理中。

2. **服务注册**：将服务的网络信息（如 IP 地址和端口）注册到 Nacos 服务注册中心。
3. **服务可用**：注册完成后，服务即可被客户端通过 Nacos 发现和调用。

#### 路由层

请求的路由和负载均衡

在服务发现功能里面有体现，将服务实例转换为服务 URL 列表。这一步将服务实例的 IP 和端口信息拼接成服务 URL，根据负载均衡策略选择一个服务地址。

#### 异步设计层

首先体现在通信方面，也就是使用netty连接，和发送RPC请求，当客户端通过`bootstrap.connect`方法连接服务器时，连接操作是异步执行的，发送RPC请求的时候使用`CompletableFuture`类来存储异步操作的结果，

### 说一下整个RPC的服务过程

1. 服务端启动的时候先扫描，检测 `@RpcService` 注解，读取 `@RpcService` 注解中的 `group` 和 `version` 信息，结合当前 Bean 实例生成 `RpcServiceConfig` 配置对象，将服务名称及其对应的地址(ip+port)注册到Nacos，这样客户端才能根据服务名称找到对应的服务地址
2. 客户端检测 `@RpcReference` 注解，读取 `@RpcReference` 注解中的 `group` 和 `version` 信息，生成 `RpcServiceConfig` 配置对象
3. 通过 `getProxy(Class<T> clazz)` 方法，使用方法动态生成代理对象
4. 当客户端调用代理对象的方法时，调用会被拦截并传递到 `invoke` 方法中
5. 然后通过`NettyRpcClient`将请求发送到服务端，`RpcRequest` 会被封装到一个 `RpcMessage` 对象中
6. `NettyRpcClient`将 `RpcMessage` 对象通过Kryo序列化为字节数组
7. 编码后的数据通过 Netty 的 `Channel` 发送到服务器端
8. 当客户端发起请求的时候，多台服务器都可以处理这个请求。通过负载均衡选一台服务器。
9. 发送请求后会返回一个 `CompletableFuture<RpcResponse<Object>>`，等待异步返回结果
10. 服务端收到了来自客户端的消息，判断收到的消息类型是 心跳检测 还是普通RPC请求
11. 心跳检测就回应心跳检测，普通的 RPC 请求就使用`rpcRequestHandler`来处理这个请求
12. `RpcRequestHandler` 首先通过 `ServiceProvider` 获取到对应的服务实现对象
13. 然后使用反射机制，调用服务实现对象中与客户端请求匹配的方法，并传递客户端提供的参数
14. 最后，将方法的执行结果返回
15. 服务端将响应消息发送给客户端通过 `Channel` 发送给客户端
16. 客户端接收到服务器的响应后，Netty 会将字节流传递给 `RpcMessageDecode`，解码后得到 `RpcResponse` 对象
17. 在接收到响应后，代理层会调用 `check` 方法来验证响应的合法性，如果验证通过，则返回结果数据



### 为什么用Netty？Netty和Scoket的区别

**socket**

Socket编程主要涉及到客户端和服务端两个方面，首先是在服务器端创建一个服务器套接字(ServerSocket)，并把它附加到一个端口上，服务器从这个端口监听连接。

客户端请求与服务器进行连接的时候，根据服务器的域名或者IP地址，加上端口号，打开一个套接字。当服务器接受连接后，服务器和客户端之间的通信就像输入输出流一样进行操作。

**缺点**

1. 需对传输的数据进行解析，转化成应用级的数据
2. 对开发人员的开发水平要求高
3. 相对于Http协议传输，增加了开发量

**Netty**

写入和 ChannelHandler 绑定的 ChannelHandlerContext 中，消息从 ChannelPipeline 中的下一个ChannelHandler 中移动

### 为什么要做这个项目，RPC框架的优势

**远程过程调用**，比如，微服务项目：服务提供者和服务消费者运行在两台不同物理机上的不同进程内，它们之间的调用相比于本地方法调用，可称之为远程方法调用，内部集群的微服务之间则采用 RPC 协议进行通讯

RPC框架一般使用长链接，不必每次通信都要3次握手，减少网络开销。

使用Nacos注册中心，有丰富的监控管理、发布、下线接口、动态扩展等，对调用方来说是无感知、统一化的操作、协议私密，安全性较高

### 在RPC项目中使用短连接有什么问题?如何实现长连接?

由于采用TCP作为传输协议，所以若采用短连接，则每次传输都要建立连接和断开连接，有很多网络通信消耗，这和开发预期的高性能 RPC不符合。

将服务端注册入Nacos 服务器中，而客户端每次都从Nacos 服务器中获取服务端地址，连接后将返回的 ChannelFuture存进一个set里，每次发送请求都从set从取出ChannelFuture发送信息，并且发送消息完毕后并不关闭连接，这就保证了客户端和服务端的长连接。

### 项目是基于TCP还是HTTP来做的，为什么用TCP实现呢?

我是使用TCP的，因为TCP更灵活，我可以自定义消息格式、编码方式、传输方式，HTTP 有较为固定的请求/响应格式，限制了灵活性

而且TCP本身是双向的，允许客户端和服务器随时发送和接收数据，而 HTTP 是基于请求/响应的通信模式，服务器不能主动发送消息

在高并发场景下，使用 TCP 长连接可以更好地管理连接池、保持会话和重用连接，减少资源消耗和延迟。HTTP 的设计更多是为 web 场景优化，而在纯粹的 RPC 调用中，TCP 的性能表现通常更优。

### 同步阻塞调用性能瓶颈:有什么瓶颈?你是怎么解决的，怎么实现异步调用的?

同步阻塞的缺点是调用时线程会阻塞在请求中，cpu此时并不会切换到其他线程，若服务器不返回消息，则客户端会一直被阻塞，无法处理其他请求。

Netty是基于NIO开发的，采用了异步非阻塞通信方式，本项目采用了netty服务框架实现了客户端服务端开发。

### 为什么需要心跳检测,客户端发送请求服务端没回应会造成什么?心跳检测机制你是怎么实现的?

在系统空闲时，例如凌晨，往往没有业务消息。如果此时链路被防火墙Hang住，或者遭遇网络闪断、网络单通等，通信双方无法识别出这类链路异常。等到第二天业务高峰期到来时，瞬间的海量业务冲击会导致消息积压无法发送给对方，由于链路的重建需要时间，这期间业务会大量失败

为了解决这个问题，需要周期性的心跳对链路进行有效性检测，一旦发生问题，可以及时关闭链路，重建TCP连接。

当有业务消息时，无须心跳检测，可以由业务消息进行链路可用性检测。所以心跳消息往往是在链路空闲时发送的。使用ldleStateHandler实现心跳检测机制（netty自带）。

### 什么叫做序列化?Java原生序列化有什么问题？使用Kryo和JAVA序列化速度的差异?

序列化就是将**类对象编码成可在网络中传输的二进制数据**，

jdk原生序列化有下面三个问题：

1.Java序列化机制是Java内部的一种对象编解码技术，无法跨语言使用。例如对于异构系统之间的对接，Java序列化后的码流需要能够通过其他语言反序列化成原始对象(副本)，目前很难支持。

2.相比于其他开源的序列化框架，Java序列化后的码流太大，无论是网络传输还是持久化到磁盘，都会导致额外的资源占用。

3.序列化性能差，资源占用率高(主要是CPU资源占用高)。

**为什么选Kryo**

Kryo 是专门针对Java 语言序列化方式并且性能非常好，并且 Dubbo 官网的一篇文章中提到说推荐使用 Kryo 作为生产环境的序列化方式，Kryo 的序列化和反序列化速度非常快，序列化后的数据体积较小

使用Kryo需要注意

Kryo由于其变长存储特性并使用了字节码生成机制，拥有较高的运行速度和较小的字节码体积因为 Kryo 不是线程安全的。使用 ThreadLocal 来存储 Kryo 对象，一个线程一个 Kryo 实例。

### Kryo和其他序列化方式的比较

Hessian对java一些常见的对象类型不支持，如 Linked系列、Locale类等等

Protobuf是由谷歌公司开发的数据语言，支持java、C++、python等多平台，序列化后体积较小，转化性能较高。但需要预编译IDL

Protostuff不需要依赖IDL文件，可以直接对Java 领域对象进行反/序列化操作，在效率上跟 Protobuf差不多，生成的二进制格式和Protobuf是完全相同的，可以说是一个Java版本的 Protobuf序列化框架。但不支持null，也不支持单纯的 Map、List 集合对象,需要包在对象里面。

Kryo可以有效处理 Java 的复杂对象结构，包括嵌套对象、集合等，且对循环引用和深层次对象有很好的支持，而且Dubbo 官网的一篇文章中提到说推荐使用 Kryo 作为生产环境的序列化方式

### 你是怎么基于动态代理进行的请求处理?用的哪一种?为什么使用这种?

客户端通过 `getProxy(Class<T> clazz)` 方法，使用方法动态生成代理对象

当客户端调用代理对象的方法时，调用会被拦截并传递到 `invoke` 方法中

我使用的是JDK动态代理，我的`RpcClientProxy`类中，JDK 动态代理被用来拦截对 RPC 服务接口方法的调用，并将其转换为一个 RPC 请求，发送到远程服务器

因为我只需要对接口进行代理，接口方法的调用就足够满足需求，JDK比CGLIB 更简单

### 你的项目哪里遇到进程通信问题了?

当大量客户端同时向服务器发送请求时，服务器响应可能会产生并发问题，导致出现进程通信问题。

### 怎么解决 RPC项目中的进程通信问题?你在项目中又遇到什么难题？

使用Netty和`CompletableFuture`

`CompletableFuture` 支持非阻塞的异步编程，将多个异步操作组合在一起，并处理结果，简化了复杂的异步操作流程。

Netty 是一个高性能的网络通信框架，它本身已经实现了高效的并发处理机制，包括事件循环、线程池等

### 还有哪些解决进程通信问题的方式？

Synchronized:重量级锁；volatile乐观锁CAS机制，通过引入wait和notify机制，使用ReentryLock和 Condition进行控制。

### 是怎么基于BeanPostProcessor机制和ApplicationListener机制实现客户端的自启动与基于注解的服务调用？（rpc-注解调用）

首先客户端类 `NettyRpcClient` 的静态代码块会初始化一个netty客户端，因此在`BeanPostProcessor`中完成请求的包装后调用 `rpcClientProxy`的 `getProxy()`方法，代理里面会调用`NettyRpcClient`的`sendRpcRequest()`方法，就实现了客户端的初始化自启动。

而服务端则将初始化代码放在`NettyRpcServer`的`start()`方法里，启动服务器手动创建Spring 应用上下文并启动了 `NettyRpcServer`。

基于注解的服务调用方面，我将service的实现类标注了`@RpcService`注解，这是一个自定义注解，同时在Spring启动过程中会有一个`BeanPostProcessor`，对`@RpcService`注解标注的类进行处理，主要是将服务注册到服务提供者；而客户端则对 service的接口标注了`@RpcReference`，在`BeanPostProcessor`的实现类中对被 `@RpcReference`标注的接口完成方法与接口的映射，这样服务器的map和客户端的 map里关于方法和类名都是一致的，因比服务端可以知道该执行什么处理

### 注册中心你是怎么实现的?用的什么?

用的是Nacos，设置ip和端口号进行服务注册和发现

### 为什么RPC项目用Nacos作为注册中心?

服务端可以将服务实例（包括 IP 地址、端口号等）注册到 Nacos，客户端可以从 Nacos 查询可用的服务实例列表，从而进行负载均衡和请求路由

Nacos 支持健康检查功能，能够监控服务实例的健康状态，自带心跳检测

用户界面和管理工具，可以方便地查看和管理服务实例、配置项、健康状态等

### 那么当有新的服务上线或者旧服务下线的时候，你怎么保证得到最新结果?客户端怎么发现服务?怎么动态监听连接?给我讲一讲机制?

Nacos会主动通知客户端，这个机制基于 **订阅-发布** 模型，当服务实例的状态发生变化时，Nacos 会通知所有订阅了该服务的客户端

当服务实例的状态（如上线、下线、变更）发生变化时，Nacos 会向所有订阅该服务的客户端发送通知。通知内容包括服务实例的最新列表，这样客户端可以更新自己的服务实例缓存

### RPC怎么解决粘包问题？-----自定义消息头

我的解决办法是自定义消息头：

![image-20240813150328363](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240813150328363.png)

我的消息头包括：魔法数(4B)、版本(1B)、消息长度(4B)、消息类型(1B)、压缩类型(1B)、序列化类型(1B)、请求Id(4B)

魔法数主要是为了筛选来到服务端的数据包，有了这个魔数之后，服务端首先取出前面四个字节进行比对，能够在第一时间识别出这个数据包并非是遵循自定义协议的，也就是无效数据包，为了安全考虑可以直接关闭连接以节省资源



### rpc-负载均衡的实现（一致性哈希）

使用了一致性哈希算法，通过**哈希环**的概念将服务地址映射到一个环状的空间中。每个服务地址会通过哈希函数映射到环上的一个或多个点（这些点称为虚拟节点），当有一个请求到来时，也会被映射到环上的某个点，然后按照顺时针方向查找到最近的虚拟节点，对应的服务地址就是该请求应当路由到的地址。

这种方法的优点是，当服务节点增加或减少时，只有与该服务节点相邻的部分请求会被重新映射到其他节点，其他的请求仍然路由到原来的节点，从而实现了高效的负载均衡。

**哈希环的构建**：
哈希环使用 `TreeMap<Integer, String>` 来表示，`Integer` 是通过哈希函数计算出的哈希值，`String` 则是节点的标识。`TreeMap` 保证了节点按哈希值顺序排列，方便查找顺时针方向上最近的节点。

**虚拟节点的实现**：
每个物理节点对应多个虚拟节点。`addNode` 方法会为每个物理节点生成多个虚拟节点（通过在节点名称后附加索引值，如 `Node1-0`, `Node1-1`），并将它们映射到哈希环上。虚拟节点的数量由 `numberOfReplicas` 参数指定。

**数据到节点的映射**：
`getNode` 方法用于根据数据（或键）找到最近的节点。通过计算键的哈希值，并在 `TreeMap` 中顺时针查找最近的节点。如果找不到顺时针方向上的节点，则会返回哈希环上的第一个节点（模拟环的闭合）。

**哈希函数**：
使用 MD5 哈希算法来计算节点和数据的哈希值。虽然这里使用了 MD5，但在实际应用中，可以使用更适合分布式系统的哈希算法，例如 MurmurHash，这样能够更好地分布哈希值，避免哈希碰撞。

**节点的添加和删除**：
添加节点时会生成多个虚拟节点并将它们加入哈希环；删除节点时会移除对应的虚拟节点。这样做的目的是减少节点增删时的数据迁移量，并保持负载均衡

### Nacos链路

1. 服务注册

   **服务提供者** 启动时，注册信息包括服务名、实例的 IP 和端口，**Nacos Server** 接收到注册请求后，将服务实例信息存储到其内部数据结构中，并更新服务列表

2. 服务发现

   **服务消费者** 启动时，发起服务发现请求，**Nacos Server** 返回服务实例的列表给消费者。如果服务状态发生变化（例如新增、删除实例），Nacos 通过长轮询的方式将变更推送给消费者。

3. 服务更新

   **服务提供者** 更新服务实例信息（例如状态变更），并将更新请求发送到 Nacos Server，**Nacos Server** 更新其内部服务实例信息，并通知所有订阅该服务的消费者（通过长轮询）

### Nacos原理，与zk的区别，与nginx的区别，为什么不用nginx呢?Nacos/Zookeeper/Nginx

在网上了解到，zk在高并发，或者长时间维持一定量的请求，那么还是会导致zk的磁盘耗尽、io读写异常、导致zk不可用，从而导致整个集群的服务注册发型能力不可用；而且当 master 节点因为网络故障与其他节点失去联系时，剩余节点会重新进行 leader 选举。问题在于，选举 leader 的时间太长，30 ~ 120s, 且选举期间整个 zk 集群都是不可用的，这就导致在选举期间注册服务瘫痪，nacos中的一致性不是像zk通过节点数据进行维护，并不会出现服务无限重复注册的情况

**Nginx** 是一个高性能的 HTTP 和反向代理服务器，可以用于负载均衡、反向代理、静态资源服务等，但是并没有服务发现、服务注册等功能

### 注册中心除了Nacos还有哪些？

**Zookeeper**

服务提供者在 ZooKeeper 上注册自己的服务信息，包括服务名称、地址、端口等。注册的信息存储在 ZooKeeper 的 ZNode 中，ZNode 类似于一个目录，可以存储服务的各种元数据，服务消费者可以从 ZooKeeper 中查找服务。通过查询 ZooKeeper 上的注册信息，消费者能够获得可用的服务实例列表，从而实现服务发现

**Eureka**

服务启动时，会将自身的信息（例如服务名称、IP 地址、端口号等）注册到 Eureka 服务器上，服务在需要调用其他服务时，可以从 Eureka 服务器获取已注册的服务实例列表，然后调用，比较轻量级

### rpc项目的整体架构

这个 RPC 项目的整体架构可以分为以下几个主要部分：客户端、服务端、注册中心、负载均衡、通信层、序列化与反序列化等

**客户端**

客户端代理，负责创建服务接口的代理对象，并拦截方法调用，构建 RPC 请求，将其发送到服务器，并获取响应

Netty客户端，维护连接池，通过 `ChannelProvider` 管理和复用 `Channel`，避免频繁创建连接，使用异步 `CompletableFuture` 机制来处理 RPC 请求的响应。

**服务端**

服务端代理，负责根据请求调用本地服务的实际方法，处理完成后返回结果。使用反射机制 (`Method.invoke`) 调用目标服务的具体方法

Netty服务端，负责启动服务器并监听客户端请求，处理来自客户端的请求，解码请求、调用实际的服务实现，并返回结果

**注册中心**

负责将服务注册到 Nacos 注册中心以及从 Nacos 中发现服务

**负载均衡**

客户端在从注册中心获取多个服务实例后，通过负载均衡算法选择一个实例进行调用

**通信层**

使用 Netty 作为底层通信框架，实现高效的网络通信

自定义编码解码器，防止粘包

**序列化和反序列化**

负责将 Java 对象转换为字节流，以及将字节流还原为 Java 对象，我使用的是Kryo

### 别人要怎么使用你的rpc框架(部署rpc)

将RPC框架打包成一个JAR文件

上传到公司内部的Maven仓库

使用者只需在他们的项目中添加对你RPC框架的Maven依赖

需要在他们的服务端项目中使用你的RPC框架来暴露服务，也就是用`@RpcService`，暴露服务

客户端使用使用`@RpcReference`注解来注入RPC接口，然后调用远程方法

连接到一个Nacos服务器，并在配置中指定相应的Nacos地址

### 如果遇到网络拥塞、超时等异常情况该如何处理?

为每个 RPC 请求设置超时机制。超时后，立即返回错误信息，避免客户端一直等待。在发送请求时使用 `CompletableFuture` 的 `get(long timeout, TimeUnit unit)` 方法来实现超时控制

在客户端的服务发现或调用阶段实现负载均衡机制。避免将请求集中在少数服务器上，平衡请求压力

`CompletableFuture`提供了异步调用和回调，客户端不需要一直等待服务端的响应，可以在接收到响应后执行回调函数

### 你使用了 TCP 长连接池，但是TCP貌似是2h才断开，你在应用层有做什么操作吗?

看netty的长连接和心跳那个问题，【Netty 长连接、心跳机制了解么?】

### 你的RPC框架有重试机制吗？策略是什么？比如说你调用失败了，是通过配置、接口、或者类去重试吗？怎么实现的？

我的RPC框架没有设置重试机制，但是可以通过配置或者接口，或者代理、或者中间件层加入重试

在 `invoke` 方法中使用 `while` 循环实现重试机制。比如说可以用`retryCount` 用于记录当前重试的次数。当 `retryCount` 小于 `MAX_RETRIES` 时，如果发生异常

### 如果获取到新的服务地址还是调用失败呢？比如可能不是地址的问题而是接口的问题？

在异常处理中记录详细的错误信息并采取适当的措施，如告警或通知，对于某些非关键业务逻辑，如果接口调用失败，可以提供一个降级方案。例如，返回一个默认值或者使用本地缓存的数据，避免系统中断

如果一个服务地址调用失败，框架可以尝试调用同一服务的其他实例。可以通过服务发现机制获取可用的服务实例列表，然后进行轮询或随机选择进行重试，在重试多次后，如果所有实例都无法成功调用，系统可以触发服务降级逻辑

如果接口问题导致多次重试失败，框架可以自动回退到上一个版本的服务地址

### dubbo和你的rpc框架？你怎么评价他们的差异？

- Dubbo 是一个成熟的分布式服务框架，支持多种注册中心，如 Zookeeper、Nacos、Consul 等，并且可以选择不同的序列化机制。Dubbo 拥有一个庞大的社区和生态系统，有大量的开源插件和扩展支持
- 基本的 RPC 功能，包括服务注册、发现、序列化和反序列化、以及客户端与服务端的通信。虽然功能上较为基础，但它为你提供了更大的灵活性，可以根据具体需求进行深度定制。与 Dubbo 相比，你的框架可能更轻量化，但需要在功能和稳定性上进行更多的开发与测试。

### 你的RPC框架跟springboot有什么关系？

我在RPC框架中使用了Spring的一些核心功能，比如`@Component`注解来管理Bean的生命周期，`BeanPostProcessor`来处理自定义注解（如`@RpcService`和`@RpcReference`）。这些都依赖于Spring的IoC容器来实现。

Spring Boot通常用于构建微服务，而我的RPC框架可以为这些微服务之间的通信提供支持。结合使用时，Spring Boot负责应用的整体架构和管理，而我的RPC框架负责具体的服务调用逻辑。

### 有些RPC框架用的是http，你怎么看待?

使用HTTP作为RPC传输协议适合对性能要求不太高，或者需要兼容多种语言、框架的系统，特别是在需要与已有的REST API或Web服务集成时非常合适。然而，在需要高性能、低延迟的系统中，使用更轻量、更高效的协议（如TCP或自定义的二进制协议）可能会更合适。

**缺点**

使用HTTP协议通常意味着使用文本格式（如JSON、XML）进行数据传输，这比起二进制格式要更加冗长，导致传输数据量大，序列化和反序列化开销也较大

HTTP协议的设计是为Web服务而非高性能RPC服务而优化的

HTTP是无状态协议，如果需要在多个RPC调用间保持会话状态，必须额外处理会话管理（如通过Token、Cookie等），增加了开发的复杂性

### 如果有一台单点故障了，你轮询的时候怎么屏蔽?你怎么把故障服务器从列表淘汰掉?

通过主动健康检查（Nacos主动检查），结合负载均衡策略，可以有效地屏蔽单点故障，确保客户端始终能够调用到可用的服务实例。

服务器的话Nacos检测到他不健康就会标记为不可用

配合重试、服务降级等策略进一步提高系统的容错能力和稳定性

### 抛开RPC不谈，你去怎么理解降级这个事情的?给异常情况做else分支，做这个事情其实在不同场景有不同的解决方案，你了解到的哪类场景可以用哪类方案去做降级这个事?

降级在系统设计中指的是在系统压力过大、部分功能故障或外部依赖不可用时，通过限制或减少系统的部分功能，确保核心功能的稳定性和可用性。

降级的核心理念是优先保证系统的整体稳定性。通过主动放弃一些非核心或次要功能，确保核心功能仍能正常运行。

- 当系统遇到突发的流量激增时，如促销活动或热点新闻，服务器压力骤增。如果不采取措施，整个系统可能崩溃。此时，可以通过降级策略限制某些非核心功能，如暂停推荐系统、减少动态内容加载等，确保关键业务（如订单处理）能够正常运行。
- 系统中的一些功能可能依赖外部服务或第三方API，例如支付网关、物流查询等。如果这些外部依赖不可用，可以通过降级措施提供备用方案或简单的占位信息，而不是让整个功能失效。例如，支付系统可以降级为仅支持现金支付，或者提供稍后的支付提醒。
- 如果系统的某个内部模块出现故障，如数据库连接池满载或缓存服务失效，可以通过降级措施限制对该模块的访问，或启用备用方案。例如，当缓存服务失效时，可以直接从数据库读取数据，尽管性能会有所下降，但功能仍然可用。
- 在双十一等大促活动时，面对大量访问，平台可能会选择关闭部分非核心功能（如历史订单查询、优惠券展示等），以保证订单支付和商品浏览的顺畅。
- 在网络带宽不足或服务器负载过高时，平台可以选择降低视频的分辨率，甚至暂时关闭高清播放，来确保流媒体播放的连续性。

### 怎么用netty创建一个channel去发送数据？在java的代码是怎么写的？

创建一个 `NioEventLoopGroup` 实例，用于管理处理客户端的事件，使用 `Bootstrap` 类配置客户端。指定使用 `NioSocketChannel` 作为 Channel 类，并添加一个 `ChannelInitializer`，用于初始化新的 `SocketChannel` 实例，在 `ChannelInitializer` 中，向 `ChannelPipeline` 添加编解码器，调用 `bootstrap.connect` 方法连接到指定的服务器地址和端口。获取连接的 `Channel` 实例，并通过 `channel.writeAndFlush()` 方法将数据（例如消息字符串）发送到服务器。

### 讲讲RPC的机制，比如说怎么实现SPI机制的?（ServiceLoader）

在 RPC 框架中，SPI 机制通常用于：

- **序列化/反序列化**：动态选择不同的序列化方式，如 JSON、Protobuf、Kryo 等。
- **网络传输协议**：选择不同的网络传输协议，如 Netty、HTTP、Dubbo 等。
- **负载均衡策略**：选择不同的负载均衡策略。

但是我的rpc框架序列化只使用了Kryo，传输用了Netty，负载均衡使用了一致性哈希算法

SPI 是一种服务发现机制，允许框架或应用程序动态加载实现类，定义 SPI 接口，实现 SPI 接口，在 `META-INF/services` 目录下配置服务提供者，加载服务提供者

SPI使用了`ServiceLoader`来实现，这里涉及类加载的过程：

- **类加载器查找**：`ServiceLoader` 首先使用类加载器查找 `META-INF/services` 目录中的服务提供者配置文件。这个目录位于每个 JAR 包的根目录下。
- `ServiceLoader` 读取 `META-INF/services` 目录下的文件，文件名是服务接口的完全限定名，文件内容是服务实现的完全限定名列表。
- 对于每个服务实现类，`ServiceLoader` 使用服务实现类的默认构造函数进行实例化
- `ServiceLoader` 将所有加载的服务提供者存储在集合中，并允许客户端通过迭代器访问这些服务提供者

### netty底层为什么用direct buffer？

Netty 底层使用 `DirectByteBuffer`（直接缓冲区）主要是为了提高性能和效率

直接缓冲区是直接分配在操作系统的物理内存中的，而不是 JVM 堆内存。这样，Netty 可以减少在数据读取和写入过程中不必要的内存复制。

Netty 使用直接缓冲区来管理网络数据的内存，这样可以更精确地控制内存的分配和释放，减少垃圾回收（GC）的压力，由于直接缓冲区不受 JVM 堆内存限制，它们可以在长时间的连接中重复使用，避免频繁的 GC 开销。

直接缓冲区可以处理比 JVM 堆内存更大的数据量。这对于处理大量数据的网络通信（例如大文件传输或高并发连接）非常重要。

### RPC项目的长连接池是怎么实现的？

连接池通常包括以下功能：

- **连接创建**：如果 `ChannelProvider` 找不到可用的连接或现有连接不可用，则尝试重新创建连接，并将其添加到连接池中
- **连接复用**：在连接池中查找可用连接，如果找到则复用。
- **连接释放**：将连接返回到池中以供后续使用。
- **连接保持**：定期检查和保持连接活跃。
- **连接销毁**：在连接不再使用时关闭连接，并释放资源

### 可以做到自动扩容缩容吗?现在让你实现你如何实现?

可以的，依赖Nacos注册中心就可以实现

**扩容**：

- 启动新的服务实例，将其注册到服务注册中心。注册中心会自动更新服务实例列表，负载均衡器会自动包括新的实例

**缩容**：

- 停止现有的服务实例，并从服务注册中心注销。注册中心会更新服务实例列表，负载均衡器会自动排除已注销的实例

### 远程调用的时候，我直接传输对象不可以吗?

远程调用时，通常需要将对象从客户端传输到服务器，或者从服务器传输到客户端。这需要序列化（将对象转换为字节流）和反序列化（将字节流转换回对象）。对象的版本变化（字段增加、删除或类型变化）可能导致序列化和反序列化失败。大对象或复杂对象的序列化和反序列化可能会影响性能和网络带宽

### CompletableFuture+线程池是如何减少响应时间的?

- **异步处理**：在后台线程中处理耗时操作，不阻塞主线程。
- **线程池管理**：复用线程，减少线程创建和销毁的开销。
- **非阻塞操作**：使用回调处理任务结果，避免阻塞等待。

### 如果不用CompletableFuture怎么实现多个异步任务同步

使用 `Future` 和 `ExecutorService`：

`ExecutorService` 提供了管理线程池的功能，并可以提交任务。

`Future` 对象表示异步计算的结果，可以用来获取任务的结果或检查任务的完成状态。

### RPC如何对某些数据进行压缩？int、int数组、String数组

基于 GZIP 的压缩和解压，

**压缩**：

1. `int` 类型
   - 将 `int` 转换为字节数组。可以使用 `ByteBuffer` 或自定义转换方法。
   - 使用 `GzipCompress` 类中的 `compress` 方法压缩字节数组。
2. `int` 数组
   - 将 `int` 数组转换为字节数组。遍历数组，将每个 `int` 转换为 4 字节的字节数组，并拼接成一个完整的字节数组。
   - 使用 `GzipCompress` 类中的 `compress` 方法压缩完整的字节数组。
3. `String` 数组
   - 将 `String` 数组转换为字节数组。可以先将每个 `String` 转换为 UTF-8 编码的字节数组，然后将所有字节数组合并。
   - 使用 `GzipCompress` 类中的 `compress` 方法压缩完整的字节数组。

### 服务下线怎么实现的

代码调用`deregisterInstance()`，Nacos把服务注销掉

# 私厨上门

### 数据库

![image-20240912210950462](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240912210950462.png)

### 订单

在创建订单时，利用分布式锁，可以为每个用户的订单生成一个锁，使用 Redis 的 `SETNX` 命令

用一个订单表记录对应的订单，订单id不能使用数据库自增id，因为myslq单表数据量受限制，分表的话id又重复了，只能用Redis中的自增计数作为全局唯一id

- Redis 是基于内存的存储系统，读写速度极快。在高并发下，使用 Redis 生成唯一 ID 能够迅速响应，且不会受到磁盘 I/O 等瓶颈的影响
- Redis 是单线程模型，所有操作按顺序执行，因此使用 Redis 生成 ID 可以避免并发冲突
- 即使在分布式 Redis 集群环境中，通过 `INCR` 操作生成的 ID 仍然是全局唯一的。因为 Redis 集群可以通过主从复制、分片等机制，保证在集群环境中生成的 ID 唯一

后来发现使用redis有可能会出现一些并发问题，改成了使用redisson+分布式锁

主要发现的问题是虽然 Redis 的 `INCR` 操作能够保证唯一 ID 的生成，但当订单生成后，需要执行扣减库存、写入 MySQL，这可能会在多个节点或线程的同时请求下产生竞争。比如说：Redis 的 ID 生成完成后，多个节点可能会试图同时扣减库存，而库存可能不足，导致一些节点的扣减失败。在这种情况下，如果没有分布式锁，就可能导致一些**订单成功生成但库存扣减失败**，造成订单与库存的不一致。

我的订单逻辑设计是：**用户点击下单---->检查库存（加分布式锁）---->检查当前用户是否重复下单---->redission生成订单id(时间戳+随机数+序列号生成的订单号)---->扣减库存---->订单保存到MySQL（释放锁）---->提交订单消息到rabbitMQ---->MQ把通知推送到 `order.queue` ---->MQ推送日志记录推送到 `log.queue`---->用户支付（消费消息）**

我的这个逻辑优缺点是：确保下单的用户能够锁定库存，但是如果用户后续没有完成支付，可能会造成库存暂时锁定，影响其他用户的购买机会

但是因为我的处理支付其实是异步的使用rabbitMQ，起到一个削峰解耦的作用，所以如果采用支付完再扣库存，很容易发生超卖，因为如果多个用户几乎同时支付成功，但库存的扣减操作在不同的线程或服务中异步执行，可能会导致多个支付成功，而库存不足的情况。

还有一种策略是在生成id之前，直接扣减库存，这样更早的锁定库存，确保不会发生超卖，但是后续取消订单需要退回库存，更加复杂，只适用于一些秒杀，库存特别紧张的情况

### 订单取消的补偿机制

在用户下单时，将订单状态设置为“待支付”----->当支付处理完成后，将结果推送到 RabbitMQ，例如支付成功或支付失败消息，如果支付超时或用户取消支付，触发订单取消处理流程----->（获取分布式锁）---->锁定成功，处理订单取消----->回滚库存------>更新订单状态(释放锁)

### 具体的使用Redisson 分布式锁来解决超卖问题

每个商品的锁可以以商品 ID 作为唯一标识

```java
// 构建分布式锁，锁的 key 使用商品 ID 作为唯一标识 
RLock lock = redissonClient.getLock("lock:product:" + productId);
```

```java
// 尝试获取锁，等待时间 100ms，锁定时间 10s
if (lock.tryLock(100, 10, TimeUnit.SECONDS)
```

### 怎么设计一个秒杀系统

**前端优化**

**页面静态化** + CDN、请求频率限制

每次打开页面的时候，直接去请求 CDN 服务器，能极大地减少后端的请求流量，对于秒杀活动设计来说，我们可以将所有可以静态化的内容全部静态化，然后将其配置在 CDN 服务器上。这样既提高了用户打开页面的时间，又减少了后端服务器的压力。

请求频率限制，比如设定一个请求概率，只允许 30% 的概率向后端发送接口请求。或者设定一个请求频率，例如 10 秒钟只能请求 1 次，随后按钮置灰

**后端优化**

使用缓存（Redis）+缓存预热

因为这是一个典型的读多写少的场景，所以应该使用缓存，也就是Redis

但是高并发情况下，同一时间有大量请求

我使用提前预热配缓存击穿，同时使用分布式锁作为预备方案，在项目启动之前，先把缓存进行`预热`。即事先把所有的商品，同步到缓存中，这样商品基本都能直接从缓存中获取到，就不会出现缓存击穿的问题了，但如果缓存中设置的过期时间不对，缓存提前过期了，或者缓存被不小心删除了，就会启用分布式锁作为后备方案

我的项目是使用布隆过滤器来预防缓存穿透的，但是秒杀的场景布隆过滤器就不适合，因为如果缓存中数据有更新，则要及时同步到布隆过滤器中，所以布隆过滤器绝大部分使用在缓存数据更新很少的场景中，所以这里还是采用默认值的方案，如果查询不到，第一次就把这个数据写入缓存，并附给他默认值

然后就是还需要预防超卖，我用Redis分布式锁保证一人一单，具体实现就是使用`set`指令，指定lockKey(锁的标识)、requeatId(请求id)、NX(只有键不存在时，才对键进行设置操作)、expireTime(过期时间)

秒杀场景中，有三个核心流程：秒杀-下单-支付，把下单和支付功能从秒杀的主流程中拆分出来，使用mq异步处理

还有需要限制非法请求，比如用机器刷，一秒钟可以请求成千上万接口，可以加验证码，用户在请求之前，需要先输入验证码，比如移动滑块

还可以分散一下时间，比如12306刚开始的时候，全国人民都在同一时刻抢火车票，由于并发量太大，系统经常挂。后来，重构优化之后，将购买周期放长了，可以提前20天购买火车票，并且可以在9点、10、11点、12点等整点购买火车票。商城的话可以要求会员或者等级达到多少级才能参与活动

### 项目-预防缓存雪崩的策略

我是使用提前预热配合限流机制预防缓存雪崩，同时使用分布式锁作为预备方案

提前预热就是把一些热点数据，比如首页需要出现的数据，商品id，名称，图片，推荐商品，还有分类名称，购物车的数据

限流机制因为我是单体项目，我使用的是Google Guava 自带的限流工具类 `RateLimiter`，他是基于令牌桶的，（这里可能要说一下令牌桶算法）

![image-20240902174929026](https://cdn.jsdelivr.net/gh/1649200416/blogImage@main/img/image-20240902174929026.png)

请求在被处理之前需要拿到一个令牌，请求处理完毕之后将这个令牌丢弃（删除）。我们根据限流大小，按照一定的速率往桶里添加令牌。如果桶装满了，就不能继续往里面继续添加令牌了。

分布式锁作为后备方案，具体实现就是配置 Redisson 客户端，连接到 Redis 服务器，通过`getLock`来获取可重入锁，使用`tryLock`来设置等待时间，持有时间来尝试获取锁

redisson获取锁的底层就是setnx

这里说一下，`lock`是阻塞获取锁，如果锁被其他线程持有，那么当前线程会**阻塞**，直到成功获取锁为止

`tryLcok`是非阻塞，如果在指定的时间内未能获取到锁，方法会返回 `false`，并且不会阻塞线程。

### 基于 Token+Redis 实现用户认证和 RBAC 权限管理

用hash来存存储token，和用户信息

```java
stringRedisTemplate.opsForHash().putAll("login:token:" + token, userMap);
```

用string来存储验证码，并设置ttl

```java
stringRedisTemplate.opsForValue().set("login:token:" + phone, code, LOGIN_CODE_TTL, TimeUnit.MINUTES);
```

用set来存储收藏图文

```java
 Boolean isCollect = stringRedisTemplate.opsForSet().isMember("collect:chef:" + userId, chefId.toString());
```

用GEO来存储地理坐标：在 Redis 中，地理位置数据是以 key-value 的形式存储的，其中 key 是指定的键名，value 是地理位置数据。每个地理位置数据点都与一个 `member` 相关联，该成员在 `key` 下是唯一的

```java
stringRedisTemplate.opsForGeo().add("chef:geo", new Point(chef.getX(), chef.getY()), chef.getId().toString());
```

还使用了redisson分布式锁，

### 项目-Redisson布隆过滤器解决缓存穿透问题

引入Redisson依赖

获取布隆过滤器：`RedissonManager.getRedisson().getBloomFilter("bloomFilter:product");`

初始化布隆过滤器

`bloomFilter.tryInit(1000000, 0.03);`初始化容量为1000000，误差率为0.03

添加元素

`bloomFilter.add(element);`

检查元素是否存在

`bloomFilter.contains(element);`

**Key**: `"bloomFilter:product"`

**Value**: 位数组（bit array）

redisson布隆过滤器默认的哈希函数时**MurmurHash**，通过**不同的种子值（seed）**生成多个伪随机的哈希函数，无需手动指定哈希函数或种子，Redisson 会自动根据你设定的误差率和容量来配置。

布隆过滤器不支持删除操作，因为哈希碰撞，开发定时任务，每隔几个小时，自动创建一个新的布隆过滤器数组，替换老的。

布隆过滤器在初始化时通常会设定一个预期的数据容量和错误率。如果数据量超过预期容量，错误率会显著上升，标准布隆过滤器并没有动态扩展的功能，如果数据量超过预期，可能需要重建布隆过滤器。然而，扩展布隆过滤器（Scalable Bloom Filter）可以在需要时动态扩展。

### 项目-WebSocket技术

引入依赖

通过 `ConcurrentHashMap` 来存储每个用户的 WebSocket 会话

`onMessage`来接收通知通知，` sendText`来发送通知

### 缓存与数据库-分布式锁-RabbitMQ 消息队列异步处理

修改数据--->更新MySQL（更新前加分布式锁，更新完释放锁）--->把删除redis中数据的消息放进rabbitMQ---->redis删除数据

### MQ宕机怎么办？（MQ高可用）

消息到 MQ 的过程中搞丢，MQ 自己搞丢，MQ 到消费过程中搞丢

- 生产者到 RabbitMQ： Confirm 机制

   `channel.confirmSelect()` 启用确认机制

   `channel.waitForConfirms()` 等待 RabbitMQ 的确认

  也就是说，如果生产者发送消息给MQ的过程中消息丢失了，生产者是知道的，这个时候就去进一步排查MQ的问题，重启MQ，这个过程中进行降级处理，把这个消息存到数据库里。MQ好了之后重发消息

- RabbitMQ 自身：持久化

  RabbitMQ支持消息持久化，即消息被写入磁盘，即使发生网络波动、断连或服务重启，消息不会丢失。在网络恢复后，消息仍然可以被消费，确保删除缓存的操作能够最终执行。

- RabbitMQ 到消费者：确认机制

  RabbitMQ 提供了消息确认机制，确保消息在被消费端成功处理后才会从队列中移除。如果因为网络问题消息没有被消费成功，RabbitMQ 可以自动重试或者转发给其他消费者，避免消息丢失。

### 为什么Redis消息会丢失呢？

Redis 是基于**网络通信**的分布式缓存服务，网络延迟或瞬时的网络中断可能导致缓存删除操作失败。如果在删除操作时网络连接不稳定，操作无法及时传达给 Redis，导致删除失败。

更新完数据库redis宕机

### 使用MQ会不会增加更新数据的时延

是的，会增加一定的延迟，因为相比直接执行缓存删除操作，多了一个消息队列的中间环节。

首先，在将消息发送到 RabbitMQ 时，会涉及到一次网络请求，这比直接操作 Redis 多了一步网络交互。

消息在 RabbitMQ 中的排队、分发、消费都会带来额外的时间成本，特别是在高并发场景下，消息处理可能需要一定的时间

由于使用消息队列，缓存删除操作是异步执行的。这意味着数据库更新成功后，缓存删除操作不会立即执行，可能稍后才会触发。在某些场景下，这可能会导致缓存与数据库数据短暂不一致

但是呢，RabbitMQ 本身具有高性能和低延迟的特性，消息传递和处理的时延通常在毫秒级别，所以这个时延是可控的

而且相较于时延，数据的一致性更重要，而且消息队列允许系统组件之间的解耦，数据库更新和缓存删除操作可以异步独立进行，从而提高系统的容错性和可扩展性

### 使用RabbitMQ消息堆积、重复消费、消费性能（RabbitMQ 的问题）

**消息堆积**

出现了消息堆积，最直接的影响就是**新消息无法入队**

消息堆积即消息没及时被消费，是生产者生产消息速度快于消费者消费的速度导致的，增加消费者解决

**紧急处理消息堆积**

**增加消费者**：通过增加消费者的数量来提升消息的处理能力。增加消费者可以分担消息消费的负载，缓解消息队列的堆积问题。

**优化消费者的处理逻辑**：检查消费者的代码是否存在性能瓶颈或是复杂的处理逻辑。可以通过优化算法、减少消费过程的计算量或是提高代码的效率来减少消费者的 CPU 开销。

**将堆积的消息直接转移**：转移到数据库，后续在处理，也可以使用惰性队列

- 接收到消息后直接存入磁盘而非内存
- 消费者要消费消息时才会从磁盘中读取并加载到内存中
- 它支持百万级消息的存储

直接基于命令行修改将一个正在运行中的队列修改为惰性队列

`rabbitmqctl set_policy Lazy "^lazy-queue$" '{"queue-mode":"lazy"}' --apply-to queues  `

**调整 RabbitMQ 配置**：可以调整 RabbitMQ 的参数来适应系统的需求，如增加内存、调整消息堆积的阈值和策略，调整网络连接等配置。

**扩展硬件资源**：如果以上措施无法解决问题，可能需要考虑增加 RabbitMQ 的集群节点或者扩容服务器的硬件资源，以提升整个系统的处理能力。

之后立即排查异常问题，通过监控，日志等手段分析是什么问题导致消息堆积

**重复消费**

消息重复也就出现在**两个阶段**，**1**、生产者多发送了消息给MQ；**2**、MQ的一条消息被消费者消费了多次

一般就是第二种场景有问题

消费者消费消息成功后，在给MQ发送消息确认的时候出现了网络异常，MQ没有接收到确认，此时MQ不会将发送的消息删除

**1、消费者监听到消息后获取id，先去查询这个id是否存中**

**2、如果不存在，则正常消费消息，并把消息的id存入 数据库或者redis中（下面的编码示例使用redis）**

**3、如果存在则丢弃此消息**

**将id存入string中，一个队列，redis数据只有一条，每次消息过来都覆盖之前的消息**

```JAVA
        String messageRedisValue = redisUtil.get("queueName4","");
        if (messageRedisValue.equals(messageId)) {
            return;
        }
        System.out.println("消息："+msg+", id:"+messageId);
        redisUtil.set("queueName4",messageId);//以队列为key，id为value
```

### redis的具体运用

项目里面使用redis来作为缓存存储数据，比如说我使用到了String来存储验证码，包括设置ttl这些

```java
stringRedisTemplate.opsForValue().set("login:code:" + phone, code, LOGIN_CODE_TTL, TimeUnit.MINUTES);
```

使用hash来存储token，这里就相当于保存session了

```java
stringRedisTemplate.opsForHash().putAll("login:token:" + token, userMap);
```

使用set来保存用户点赞信息

```java
stringRedisTemplate.opsForSet().add("blog:liked:", userId.toString());
```

使用GEO来保存用户和商家的地址

在 Redis 中，地理位置数据是以 key-value 的形式存储的，其中 key 是指定的键名，value 是地理位置数据。每个地理位置数据点都与一个 `member` 相关联，该成员在 `key` 下是唯一的。

```java
stringRedisTemplate.opsForGeo().add("chef:geo", new Point(chef.getX(), chef.getY()), chef.getId().toString());
```





# RabbitMQ

### 消息队列？

消息队列就是一个使用队列来通信的组件

### RabbitMQ核心概念

RabbitMQ 整体上是一个生产者与消费者模型，主要负责接收、存储和转发消息。

消息：**消息头**和 **消息体**，生产者把消息交由 RabbitMQ 后，RabbitMQ 会根据消息头把消息发送给感兴趣的 消费者

**Exchange(交换器)**： 用来接收生产者发送的消息并将这些消息路由给服务器中的队列中

**RabbitMQ 的 Exchange(交换器) 有 4 种类型**：**direct(默认)**，**fanout**, **topic**, 和 **headers**

**Queue(消息队列)** 用来保存消息直到发送给消费者。多个消费者订阅同一个队列，队列中的消息会被平均分摊给多个消费者，不是广播

**Broker**（消息中间件的服务节点）：一个 RabbitMQ Broker 可以简单地看作一个 RabbitMQ 服务节点

### 交换机有哪些类型

4种：**fanout**、**direct**、**topic**、**headers**

**fanout：**会把所有发送到该 Exchange 的消息路由到所有与它绑定的 Queue 中，没有判断，通常用于广播

**direct：**把消息路由到那些 Bindingkey 与 RoutingKey 完全匹配的 Queue 中

**topic：**BindingKey 中可以存在两种特殊字符串“*”和“#”，用于做模糊匹配

**headers：**一般不用，他是对比键值对

### AMQP

RabbitMQ 就是 AMQP 协议的 `Erlang` 的实现，AMQP协议有三层：Module Layer(最高层，定义客户端调用命令)、Seesion Layer(中间层，客户端命令发送给服务器)、TransportLayer(最底层，主要传输二进制数据流)

AMQP模型的三大组件：交换机、队列、绑定

### RabbitMQ/Kafka(卡夫卡)（Partition）

RabbitMQ遵循AMQP协议，采用**传统消息队列**模式：消息通过 交换器 路由到 消息队列 中，消费者从 消息队列 读取消息

Kafka采用的是**自定义的 TCP 通信协议**，底层没有使用 AMQP 等标准协议，Producer 通过 **TCP 向 Kafka 发送消息**，消息被**分区（Partition）**后直接写入磁盘，每个 Partition 可以被多个消费者并行读取。Kafka 更注重消息的高吞吐量、水平扩展和数据持久化，消息的消费基于**offset**

RabbitMQ的消息存储机制以 消息队列 为核心，Queue 本质上是一个**先进先出**的队列结构，消息被消费者读取并确认（ACK）后，RabbitMQ 会将该消息删除。不适合需要消息回溯的场景，默认是存在内存里的

Kafka 将消息存储在 **topic（主题）** 这个逻辑层面，而相对应的队列逻辑只是 topic 实际存储文件中的位移标识，Kafka的所有的消息都会写入磁盘，通过**零拷贝**来提高写入效率，存储模型是**日志追加**，不会删除已经消费的信息，可以随时**回溯**之前的消息

RabbitMQ 采用**基于推送的模式**，即消息从 消息队列 中被推送到消费者

Kafka 采用**基于拉取的模式**，消费者主动从 分区 中拉取消息

RabbitMQ适用于**低延迟、可靠传递**的场景，如订单系统、支付系统、即时消息、任务分发等

Kafka适合**大数据流处理**、**日志聚合**、**实时数据分析**、**事件流**等场景，尤其适用于需要高吞吐量、数据持久化、消费者可回溯读取的场景，在分布式环境中很流行。天然支持**分布式架构**，每个 Topic 可以分为多个 分区，分区 可以分布在多个 Broker 上，形成一个高扩展的集群

### 死信队列？如何导致的？

当消息在一个队列中变成死信 (`dead message`) 之后，它能被重新发送到另一个交换器中，这个交换器就是 DLX，绑定 DLX 的队列就称之为死信队列。

原因：

- 消息被拒
- 消息 TTL 过期
- 队列满了，无法再添加

### 延迟队列？RabbitMQ怎么实现延迟队列？

延迟队列指的是存储对应的延迟消息，消息被发送以后，并不想让消费者立刻拿到消息，而是等待特定时间后，消费者才能拿到这个消息进行消费。

在一些优惠卷过期的情况可以使用，还有定时发布、一段时间后自动确认收货都可以用上

可以通过 TTL 和 DLX（死信队列） 模拟出延迟队列的功能

### RabbitMQ有哪些工作模式？

- 简单模式
- work 工作模式
- pub/sub 发布订阅模式
- Routing 路由模式
- Topic 主题模式

### RabbltMQ消息传输？

RabbitMQ 使用信道的方式来传输数据，信道是建立在 TCP 链接上的虚拟链接，一条 TCP 链接上建立成百上千个信道来达到多个线程处理

### RabbitMQ 消息的顺序性？

单个队列单个消费者

### RabbitMQ 高可用？镜像集群模式

**镜像集群模式**，每个 RabbitMQ 节点都有这个 queue 的一个完整镜像，包含 queue 的全部数据，但是性能开销大

### 解决消息队列的延时以及过期失效问题？

批量重导，流量高的适合直接丢弃，流量低的时候手动写程序把那 1000 个订单给查出来，手动发到 mq 里去再补一次

### rabbitMQ-消息的有序性

就一个`queue`但是对应一个`consumer`

**看业务更需要顺序性，还是更需要消费效率**

**1、发送消息：入队列**

消息发送时，需要业务来保证顺序性，就是保证生产者入队的顺序是有序的。

在分布式的场景下如果难以保证各个服务器的入队顺序，则可以**加分布式锁**的方式来解决。或者在业务生产方的**消息里带上消息递增 ID**

**2、队列中的消息**

在 RabbitMQ 的消息会保存在队列（Queue）中，在同一个队列里的消息是`先进先出（FIFO）`的，这个**由 RabbitMQ 来帮我们保证顺序**。

而不同队列中的消息，RabbitMQ 无法保证其顺序性，如果真需要保证只能一个`queue`对应一个`consumer`
